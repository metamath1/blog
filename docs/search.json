[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "이 블로그는 주로 최적화, 머신러닝 따위의 내용을 다루고 있다.\n개인적으로 공부한 내용을 주로 다루며 혹시나 다른 분들께 도움이 될까 싶어 가급적 문서는 친절하고 자세하게 적고 있다.\n글은 주로 jupyter notebook으로 작성되며 노트북 파일은 ml-simple-works에 공개 되어 있다.\n2022년까지 내용은 https://metamath1.github.io 에서 볼 수 있다.\n혹시나 내용의 오류 지적이나 내용에 대한 문의 사항은 metamath@gmail.com으로…\n저작권 관련 사항\n본 블로그에 내용은 얼마든지 인용하고 사용해도 좋으나 출처는 꼭 밝혀 주시면 감사하겠다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML simple works",
    "section": "",
    "text": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers\n\n\n\n\n\n\n\nai\n\n\nT5\n\n\ntransformers\n\n\nhugging face\n\n\n번역기\n\n\n한국어 번역기\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to Gradient Boosting\n\n\n\n\n\n\n\nai\n\n\nmachine learning\n\n\nboosting\n\n\ngradient boosting\n\n\nensemble\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2022\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\n  \n\n\n\n\nA Step by Step Introduction to EM Algorithm\n\n\n\n\n\n\n\nai\n\n\nmachine learning\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html",
    "href": "posts/gradientboost/gradient_boosting.html",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nimport graphviz\nfrom sklearn.tree import export_graphviz\nfrom imageio import imread\n\n# import plotly.graph_objects as go # colab\nfrom plotly import graph_objs as go # local, old version plotly\n\ncm2 = ListedColormap(['C1', 'C2'])\ncm3 = ListedColormap(['C1', 'C2', 'C3'])\ncm8 = ListedColormap(['#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087', '#f95d6a', \n                      '#ff7c43', '#ffa600'])\nimage_dpi=150"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#들어가며",
    "href": "posts/gradientboost/gradient_boosting.html#들어가며",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "들어가며",
    "text": "들어가며\n부스팅 알고리즘은 약한 학습기weak learner를 순차적으로 학습시키고 개별 학습기의 예측을 모두 더해 최종 예측을 만들어내는 앙상블 메소드의 한 종류입니다. 그 중 그래디언트 부스팅은 강력한 성능으로 가장 많이 애용되는 알고리즘입니다.\n인기가 많은 알고리즘이다 보니 많은 머신러닝 교재에서도 꼭 등장하는 편입니다. 주로 이전 단계 학습기가 만들어낸 잔차residual을 타겟으로 개별 학습기를 순차적으로 학습하는 방법을 보여주면서 직관적으로 알고리즘을 설명합니다. 하지만 이런 방식으로는 왜 ’그래디언트’라는 단어가 등장하는지 잘 설명되지 않습니다. 그리고 어떻게 어떻게 설명한다 하더라도 주로 회귀문제에 국한하여 설명하는 것이 대부분입니다. 이렇게 많은 설명이 피상적인 수준에 머무르는 이유는 이 알고리즘을 제대로 설명하기 위해서 좀 복잡한 최적화 방법을 이야기해야하기 때문입니다.\n하지만 결론적으로 이야기하면 회귀문제를 중심으로 간단하게 그래디언트 부스팅을 이해하면 실용적으로 알고리즘을 활용하기에 충분하다 할 수 있습니다. 왜냐하면 분류문제가 된다하더라도 회귀문제의 논리가 동일하게 적용되기 때문입니다. 실제로 회귀에 사용되는 오차제곱합 손실을 사용하는 방식을 그대로 분류 문제에 적용해도 잘 작동합니다. 굳이 분류문제를 예로 들지 않아도 크게 달라지는건 없습니다. 이런 이유로 우선 회귀문제를 중심으로 최대한 간단하게 그래디언트 부스팅을 이해한 다음 분류문제에 대한 구체적인 이야기를 이어가도록 하겠습니다.\n회귀문제를 설명하는 좋은 예제는 다음과 같습니다.1\n\n\n아래 예제는 핸즈온 머신러닝에 나온 예를 참고하였습니다.\n\n아래 링크를 클릭해서 코랩에서 직접 실행하면서 글을 읽을 수 있습니다.\n\n이 글은 국가수리과학연구소 초청세미나 발표 목적으로 작성되었으며 발표 슬라이드는 여기에서 볼 수 있다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#간단한-회귀문제-예",
    "href": "posts/gradientboost/gradient_boosting.html#간단한-회귀문제-예",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "간단한 회귀문제 예",
    "text": "간단한 회귀문제 예\n\n# 2차식으로 만든 데이터셋 + 잡음\nnp.random.seed(42)\nX_reg = np.random.rand(100, 1) - 0.5\ny_reg = 3*X_reg[:,0]**2 + 0.05 * np.random.randn(100)\n\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k')\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step1.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위와 같은 데이터에 결정회귀트리를 적합시키겠습니다. 처음에는 타겟의 평균만 출력하는 더미 학습기로 출발합니다.\n\n# 개별학습기 로드\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.dummy import DummyRegressor\n\n\n# 아주 약한 0번째 학습기를 한번 학습시킨다.\nh0 = DummyRegressor(strategy=\"mean\").fit(X_reg, y_reg)\n\n# 예측하고 잔차를 구한다.\ny_h0 = h0.predict(X_reg)\nr0 = y_reg - y_h0\n\n# 결과를 그림으로 확인\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True, dpi=100)\n\nax[0].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k')\nax[0].plot(x, h0.predict(x.reshape(-1,1)), color='C2')\nax[0].set_title('h0(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y')\nax[0].spines[\"right\"].set_visible(False)\nax[0].spines[\"top\"].set_visible(False)\n\nax[1].scatter(X_reg, r0, marker='o', color='C3', edgecolor='k')\nax[1].set_title('Residual')\nax[1].set_xlabel('x')\nax[1].set_ylabel('Residual_0')\nax[1].spines[\"right\"].set_visible(False)\nax[1].spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h0.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위 그래프에서 왼쪽 그림은 주어진 데이터와 초기 더미 학습기 \\(h_0(x)\\)의 출력을 나타내고 오른쪽 그림은 더미 학습기가 만든 출력으로부터 잔차residual을 계산하여 그린 것입니다. 두번째 학습기는 첫번째 학습기가 만든 이 오차를 타겟으로 학습하게 됩니다. 계속 이런식으로 반복합니다.\n\n# 아주 약한 1번째 학습기를 한번 학습시킨다.\n# X_reg에 대한 타겟을 이번에는 r0로 설정해서 학습한다.\n# 또 개별 학습기는 max_depth=2로 설정하여 그리 깊지 않은 트리가 생성되게 한다.\nh1 = DecisionTreeRegressor(max_depth=2).fit(X_reg, r0)\n\n# 예측하고 잔차를 구한다.\ny_h1 = h1.predict(X_reg)\nr1 = r0 - y_h1\n\n# 결과를 그림으로 확인\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True, dpi=100)\n\nax[0].scatter(X_reg, r0 , marker='o', color='C3', edgecolor='k')\nax[0].plot(x, h1.predict(x.reshape(-1,1)), color='C2')\nax[0].set_title('h1(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('Residual_0')\nax[0].spines[\"right\"].set_visible(False)\nax[0].spines[\"top\"].set_visible(False)\n\nax[1].scatter(X_reg, r1, marker='o', color='C4', edgecolor='k')\nax[1].set_title('Residual')\nax[1].set_xlabel('x')\nax[1].set_ylabel('Residual_1')\nax[1].spines[\"right\"].set_visible(False)\nax[1].spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위 그림에서 왼쪽 그림은 데이터로 이전 그림의 오른쪽에 그려진 잔차를 그리고 이에 대한 \\(h_1(x)\\)의 출력을 나타낸 것입니다. 오른쪽은 이렇게 \\(h_1(x)\\)의 출력으로 부터 다시 에러를 계산하여 그린 것입니다. 한번 더 반복하겠습니다.\n\n# 아주 약한 2번째 학습기를 한번 학습시킨다.\nh2 = DecisionTreeRegressor(max_depth=2).fit(X_reg, r1)\n\n# 예측하고 잔차를 구한다.\ny_h2 = h2.predict(X_reg)\nr2 = r1 - y_h2\n\n# 결과를 그림으로 확인\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True, dpi=100)\n\nax[0].scatter(X_reg, r1 , marker='o', color='C4', edgecolor='k')\nax[0].plot(x, h2.predict(x), color='C2')\nax[0].set_title('h2(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('Residual_1')\nax[0].spines[\"right\"].set_visible(False)\nax[0].spines[\"top\"].set_visible(False)\n\nax[1].scatter(X_reg, r2, marker='o', color='C5', edgecolor='k')\nax[1].set_title('Residual')\nax[1].set_xlabel('x')\nax[1].set_ylabel('Residual_2')\nax[1].spines[\"right\"].set_visible(False)\nax[1].spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h2.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n이제 세 학습기에 \\([-0.5,0.5]\\)를 입력하여 출력을 구해서 모두 더해 최종 출력을 만들어 냅니다. 이렇게 약한 학습기 세 개를 더한 최종 예측이 얼마나 잘 맞는지 확인해보겠습니다.\n\n# 학습기 0번, 1번, 2번을 다 더한다.\npred = ( h0.predict(x) + h1.predict(x) + h2.predict(x) )\n\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k')\nax.plot(x, pred, color='C2')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title(\"h0(x)+h1(x)+h2(x)\")\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h0h1h2.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n데이터의 전체 구조를 잘 따라가는 것을 확인할 수 있습니다.\n이런 방식은 0번 학습기가 틀린만큼 1번 학습기가 예측하여 더해주고 1번 학습기 조차 틀린 부분을 2번 학습기가 보완해주는 방식으로 진행합니다. 이런 방식으로 결과가 좋아진다는 것을 직관적으로 이해할 수 있는데 여기까지 설명으로는 그래디언트가 어느 장면에서 등장하는지 알 수 없습니다.\n많은 공개 강의나 블로그 글에서 이 후 그래디언트가 등장하는 장면을 간략하게 설명하는데 이 글에서는 조금 더 자세하게 이야기 해보도록 하겠습니다.\n참고로 앞으로 나오는 식번호에 (1) 모양으로 붙은 식번호는 그래디언트 부스팅 원논문의 식번호를 그대로 쓴 것이며 이 글에서 사용하는 식번호는 ([1])식으로 붙였습니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#매개변수-서치",
    "href": "posts/gradientboost/gradient_boosting.html#매개변수-서치",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "매개변수 서치",
    "text": "매개변수 서치\n벡터 변수 \\(\\mathbf{x}\\)와 그에 대응되는 타겟값 \\(y\\)가 있을 때 지도학습의 목표는 입력 \\(\\mathbf{x}\\)를 적절히 출력 \\(y\\)로 매핑하는 함수 \\(F(\\mathbf{x})\\)를 찾는 것입니다. 그렇게 찾아진 최적의 함수를 \\(F^{*}(\\mathbf{x})\\)라하면 이는 개념적으로 다음처럼 쓸 수 있습니다.\n\\[\nF^{*} = \\underset{F}{\\text{argmin}} \\, \\mathbb{E}_{y,\\mathbf{x}} \\left[ L(y, F(\\mathbf{x}) \\right] \\tag{1}\n\\]\n위 식에서 \\(L\\)은 \\(y\\)와 \\(F\\)의 차이를 설명하는 적절한 손실함수loss function입니다. 그러면 식(1)은 이 함수가 정의된 변수들의 존재하는 모든 경우에 대해 평균을 낸 것이 되고 이를 최소화하는 \\(F(\\mathbf{x})\\)를 찾는 것이 됩니다.\n보통의 경우 \\(F(\\mathbf{x})\\)를 특정한 형태의 함수로 제한하고 그 제한된 범위에서 함수를 결정하는 매개변수parmeter \\(\\mathbf{P}\\)를 찾게 됩니다. 다시말해 \\(F(\\mathbf{x};\\mathbf{P})\\)에서 \\(\\mathbf{P}\\)를 결정하는 문제를 다루게 됩니다. 예를들어 \\(F(\\mathbf{x};\\mathbf{P})\\)가 \\(p\\)차 다항식이라면 매개변수는 \\(p+1\\)개의 개수가 될것입니다. 또는 \\(F(\\mathbf{x};\\mathbf{P})\\)가 인공신경망이라면 신경망의 형태(네트워크 구조)를 고정하고 신경망의 매개변수를 결정하는 것이 될 것입니다.\n그래디언트 부스팅에서는 함수 \\(F(\\mathbf{x};\\mathbf{P})\\)를 다음처럼 매개변수 \\(\\mathbf{a}\\)에 의해 결정되는 \\(M\\)개의 일반적인 함수 \\(h\\)의 합으로 이뤄지는 함수라고 설정합니다.\n\\[\nF\\left(\\mathbf{x}; \\{\\beta_m, \\mathbf{a}_m\\}^M_{m=1}\\right) = \\sum_{m=1}^M \\beta_m h (\\mathbf{x}; \\mathbf{a}_m)\n\\]\n이렇게 정의된 문제에서 매개변수 공간을 서치하기 위해 일반적인 최적화 과정을 도입할 수 있습니다. 함수 \\(F(\\mathbf{x})\\)를 고정하고 매개변수를 최적화 할 것이기 때문에 매개 변수에 대한 손실함수를 다음처럼 적으면\n\\[\n\\Phi(\\mathbf{P}) = \\mathbb{E}_{y, \\mathbf{x}} \\left[ L(y, F(\\mathbf{x}; \\mathbf{P}) \\right]\n\\]\n최적의 매개변수는 다음처럼 찾아야 하고\n\\[\n\\mathbf{P}^* = \\underset{\\mathbf{P}}{\\text{argmin}} \\, \\Phi (\\mathbf{P}) \\tag{3}\n\\]\n이렇게 결정된 \\(\\mathbf{P}^*\\)를 이용해 다음처럼 \\(F^* (\\mathbf{x})\\)를 구할 수 있습니다.\n\\[\nF^{*} (\\mathbf{x}) = F(\\mathbf{x}; \\mathbf{P}^{*} )\n\\]\n위 식에서 \\(\\mathbf{P}^{*}\\)를 결정하기 위해 식(3)을 최적화 과정을 통해 풀어야 하는데 \\(M+1\\)번 단계를 거쳐 결정된 \\(\\mathbf{P}^{*}\\)를 다음처럼 쓸 수 있습니다.\n\\[\n\\mathbf{P}^{*} = \\sum_{m=0}^{M} \\mathbf{p}_m \\tag{4}\n\\]\n식(4)에서 \\(\\mathbf{p}_m\\)은 각 최적화 단계에서 결정되는 업데이트 벡터입니다. 그래서 \\(\\mathbf{p}_m\\)은 최적화 수법에 따라 정의될텐데 여기서는 가장 간단한 최속강하법steepest descent method를 고려합니다.\n\n최속강하법\n최속강하법은 최적화 알고리즘에가 가장 간단한 형태이나 최근 딥러닝에서 많이 사용하는 확률적 경사하강법과 거의 동일한 알고리즘입니다. 방법의 핵심은 주어진 목적함수(손실함수)를 설계변수(여기서는 매개변수)로 미분한 그래디언트의 반대방향으로 설계변수를 업데이트하는 것입니다.\n앞선 절에서 정의한 손실함수 \\(\\Phi(\\mathbf{P})\\)에 대한 매개변수 \\(\\mathbf{P}\\)에 대한 단계 \\(m\\)에서 그래디언트는 다음과 같습니다.\n\\[\n\\mathbf{g}_m = \\{g_{jm}\\} =\\left\\{ \\left[  \\frac{\\partial \\, \\Phi(\\mathbf{P})}{\\partial P_j} \\right]_{\\mathbf{P}=\\mathbf{P}_{m-1}} \\right\\}\n\\]\n위 식은 매개변수 공간에서 \\(\\mathbf{P}_{m-1}\\)의 위치에서 그래디언트를 나타냅니다. 그래디언트가 구체적인 벡터로 정의되려면 편도함수들에 현재 그래디언트가 정의되는 위치를 대입해야 한다는 사실을 상기합시다. 이를 나타내는 표현이 대괄호 아랫쪽에 \\(\\mathbf{P}=\\mathbf{P}_{m-1}\\)입니다. 그리고 \\(\\mathbf{P}_{m-1}\\)은 단계 \\(m-1\\)까지 \\(\\mathbf{p}_m\\)들이 누적된 벡터입니다.\n\\[\n\\mathbf{P}_{m-1} = \\sum_{i=0}^{m-1} \\mathbf{p}_i\n\\]\n각 단계에서 \\(\\mathbf{p}_m\\)은 다음처럼 그래디언트 \\(\\mathbf{g}_m\\)의 반대방향으로 \\(\\rho_m\\)만큼 이동한 벡터입니다.\n\\[\n\\mathbf{p}_m = -\\rho_m \\mathbf{g}_m\n\\]\n\\(\\rho_m\\)은 선탐색line search라는 과정으로 구할 수 있으며 다음과 같습니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}}\\, \\Phi(\\mathbf{P}_{m-1} - \\rho \\, \\mathbf{g}_m) \\tag{5}\n\\]\n이상의 내용은 조금 추상적으로 느껴질 수 있지만 기호가 좀 복잡할 뿐 보통 매개변수를 최적화하는 일반적인 내용에 지나지 않습니다. 그래디언트 부스팅이 복잡하게 느껴지는 이유는 이 같은 매개변수 최적화와 달리 함수공간에서 함수를 탐색하고자 하기 때문입니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#함수-서치",
    "href": "posts/gradientboost/gradient_boosting.html#함수-서치",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "함수 서치",
    "text": "함수 서치\n입력변수 \\(\\mathbf{x}\\)와 출력변수 \\(y\\)의 결합확률분포 상에서 손실함수 \\(L(y, F(\\mathbf{x}))\\)의 기대값은 \\(\\mathbb{E}_{y, \\mathbf{x}}\\left[ L(y, F(\\mathbf{x}))\\right]\\)이며 이를 최소화하는 ’매개변수’로써의 함수 \\(F(\\mathbf{x})\\)를 생각해볼 수 있습니다. 이를 좀 더 명시적으로 표시하기 위해 다음처럼 \\(F\\)를 변수로 하는 범함수functional로 쓰기로 합시다.\n\\[\n\\Phi(F) = \\mathbb{E}_{y, \\mathbf{x}}\\left[ L(y, F(\\mathbf{x}))\\right]\n\\]\n기댓값의 정의에 의해\n\\[\n\\begin{aligned}\n\\Phi(F) &= \\int_{\\mathbf{x}} \\int_{y} L(y, F(\\mathbf{x})) f_{XY}(\\mathbf{x}, y) \\, dy \\, d\\mathbf{x} \\\\[5pt]\n&= \\int_{\\mathbf{x}} \\int_{y} L(y, F(\\mathbf{x})) f_{Y \\mid X}(y \\mid \\mathbf{x}) \\, dy \\, f_X (\\mathbf{x}) \\, d \\mathbf{x} \\\\[5pt]\n&= \\mathbb{E}_{\\mathbf{x}} \\left[ \\,\\mathbb{E}_{y} [\\,L(y,F(\\mathbf{x}))\\mid \\mathbf{x}\\,] \\, \\right]\n\\end{aligned}\n\\]\n로 쓰고 위 식에서\n\\[\n\\phi(F(\\mathbf{x}))= \\mathbb{E}_{y} [\\,L(y,F(\\mathbf{x}))\\mid \\mathbf{x}\\,]\n\\]\n로 두면 \\(\\phi(F(\\mathbf{x}))\\)는 주어진 \\(\\mathbf{x}\\)에 대해서 함수 \\(F\\)를 바꿔치기 하면 함수 값이 달라지는 함수가 됩니다. \\(\\phi(F(\\mathbf{x}))\\)를 최소화하는 무수히 많은 후보 함수 \\(F\\)에서 최적의 함수를 찾기 위해 앞서 이야기한 최속강하법을 함수 공간에 적용하는 것을 생각해봅시다.\n식(4)에서 각 최적화 단계에서 구한 매개변수 벡터 \\(\\mathbf{p}_m\\)을 모두 더해 최적의 매개변수 벡터를 업데이트 했습니다. 이렇게 최적화 단계는 보통 유한한 길이를 가지는 벡터변수에 대해 그래디언트를 구하고 그것에 마이너스를 곱해 최적화 하고자 하는 변수를 업데이트 합니다. 하지만 지금은 \\(\\phi(F(\\mathbf{x}))\\)를 최소화하는 변수가 함수입니다. 따라서 최적화되는 변수는 요소 개수가 무한대인 벡터 변수라고 생각할 수 있습니다. 즉 함수 자체가 “매개변수”인 것입니다. 그러면 각 최적화 단계에서 찾아지는 벡터는 그 자체로 함수가 되며 그 함수들을 \\(f_m(\\mathbf{x})\\)라 하면 이를 모두 더한\n\\[\nF^* (\\mathbf{x}) = \\sum_{m=0}^M f_m (\\mathbf{x})\n\\]\n가 최적해가 되며 여기서 매 최적화 단계에서 \\(f_m(\\mathbf{x})\\)는\n\\[\nf_m(\\mathbf{x}) = - \\rho_m g_m(\\mathbf{x}) \\tag{6}\n\\]\n입니다. 식(6)은 최속강하법 매단계에서 구해지는 마이너스 그래디언트가 \\(\\mathbf{x}\\)의 함수인 \\(g_m(\\mathbf{x})\\)임을 의미합니다.\n\\(m\\)번째 단계에서 그래디언트 \\(g_m(\\mathbf{x})\\)은 \\(F(\\mathbf{x})\\)를 매개변수로 보고 다음처럼 구할 수 있습니다.\n\\[\ng_m(\\mathbf{x}) = \\left[ \\frac{\\partial \\, \\phi(F(\\mathbf{x}))}{\\partial \\, F(\\mathbf{x})} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})} = \\left[ \\frac{\\partial \\, \\mathbb{E}_{y} [\\,L(y,F(\\mathbf{x}))\\mid \\mathbf{x}\\,]   }{\\partial F(\\mathbf{x})} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})}\n\\]\n여기서 \\(F_{m-1}(\\mathbf{x})\\)은 다음처럼 개별 단계에서 찾아진 \\(f_i(\\mathbf{x})\\)를 0단계부터 \\(m-1\\)단계까지 더한\n\\[\nF_{m-1}(\\mathbf{x}) = \\sum_{i=0}^{m-1} f_i(\\mathbf{x})\n\\]\n입니다. 위 \\(g_m(\\mathbf{x})\\)을 구하는 과정을 보면 \\(F\\)로 미분하고 구체적인 그래디언트를 구하기 위해 \\(F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})\\)처럼 이전 단계에서 구해진 함수를 대입하는 것을 확인할 수 있습니다. 즉 그래디언트를 구하는 위치가 \\(F_{m-1}(\\mathbf{x})\\)인 것입니다. 그리고 적분과 미분의 순서를 바꾸면\n\\[\ng_m(\\mathbf{x}) = \\mathbb{E}_{y} \\left[ \\frac{\\partial \\, L(y,F(\\mathbf{x})) }{\\partial F(\\mathbf{x})} \\mid \\mathbf{x} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})} \\tag{7}\n\\]\n손실함수를 함수 형태인 매개변수로 편미분하고 주어진 \\(\\mathbf{x}\\)에 어떤 \\(y\\)가 매치 될지 모르니 \\(f_{Y \\mid X}(y \\mid \\mathbf{x})\\)상에서 \\(y\\)에 대해서 평균을 규해서 최종적인 함수를 구해내게 됩니다. 이 함수가 바로 매개변수 \\(F(\\mathbf{x})\\)에 대한 그래디언트가 됩니다. 그래디언트를 구했으므로 식(6)에 있는 그래디언트 \\(g_m(\\mathbf{x})\\) 방향으로 손실함수의 기댓값을 최소화하는 스탭사이즈 \\(\\rho_m\\)을 찾게 됩니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}} \\, \\mathbb{E}_{y, \\mathbf{x}}\\left[ L(\\, y, F_{m-1}(\\mathbf{x}) - \\rho \\, g_m(\\mathbf{x})\\, )\\right] \\tag{8}\n\\]\n\n유한개 데이터에 대해\n지금까지 이야기한 함수를 바로 찾는 방법은 유한개의 데이터 \\(N\\)개만 가진 현실적인 경우에 적용할 수 없습니다. \\(L(y, F)\\)을 \\(F\\)로 미분해서 \\(y\\)에 대해 적분해야지 함수로써의 그래디언트가 구해지게 되는데 이 적분을 수행할 수 가 없습니다. \\(f_{XY}\\)와 \\(f_{Y \\mid X}\\)를 모르고 그냥 \\(f_{XY}\\)에서 샘플링된 데이터 \\(N\\)개만 가지고 있으니 그래디언트를 함수 형태로 구할 수 없습니다.\n때문에 다음처럼 주어진 데이터 포인트에 대해서 그래디언트 함수의 함수값 \\(N\\)개만 구할 수 있습니다.\n데이터 \\(\\mathcal{D}=\\{\\mathbf{x}_i, y_i\\}_{i=1}^N\\)가 주어져 있을 때 식(7)로 주어지는 그래디언트의 마이너스곱한 값을 \\(\\mathbf{x}_i\\), \\(y_i\\)에 대해서 근사해보면\n\\[\n-g_m (\\mathbf{x}_i) = - \\left[ \\frac{\\partial \\, L(y_i, F(\\mathbf{x}_i)) }{\\partial F(\\mathbf{x}_i)} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})}\n\\]\n로 구할 수 있습니다. 이렇게 모두 \\(-g_m(\\mathbf{x}_i)\\)를 \\(N\\)개 구할 수 있는데 이 값들은 \\(\\mathbf{x}_i\\)에 대해서 주어진 \\(y_i\\)에 대해서만 구한 값들이라 식(7)에서 구해지는 함수 \\(g_m(\\mathbf{x})\\)에서의 함수값과 정확히 동일한 값이 아닙니다. 이런 이유로 \\(g_m (\\mathbf{x}_i)\\)를 식(7)에 대한 pseudo-response라 합니다.\n우리에게 필요한것은 모든 \\(\\mathbf{x}\\)에 대하서 값을 계산해주는 그래디언트 함수가 필요하므로 할 수 있는 최선은 구해진 pseudo-response를 피팅하여 함수 \\(g_m(\\mathbf{x})\\)를 재구성하는 것입니다.\n매개변수 \\(\\mathbf{a}\\)에 의해 정의되는 함수 \\(h(\\mathbf{x};\\mathbf{a})\\)을 준비해서 \\(-g_m(\\mathbf{x}_i)\\)들을 피팅합니다. 이 \\(h(\\mathbf{x};\\mathbf{a})\\)을 약한학습기weak learner 또는 기본학습기base leaner라고 합니다. 약한학습기는 다음과 같은 least square 손실을 사용해서 학습합니다.\n\\[\n\\mathbf{a}_m = \\underset{\\mathbf{a}}{\\text{argmin}} \\sum_{i=1}^N (-g_m(\\mathbf{x}_i) - h(\\mathbf{x}_i;\\mathbf{a}))^2 \\tag{11}\n\\]\n식(11)에서 \\(h(\\mathbf{x}_i;\\mathbf{a})\\)는 실제 구현에서 특정 머신러닝 모델이 되고 식(11) 자체는 이 약한학습기를 학습시키는 과정이 됩니다. 이렇게 학습한 매개변수 \\(\\mathbf{a}_m\\)을 사용해서 그래디언트 \\(h(x;\\mathbf{a}_m)\\)를 구성했으면 다음처럼 선탐색해서 스탭사이즈를 구합니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}} \\sum_{i=1}^N L(y_i, F_{m-1}(\\mathbf{x}_i) + \\rho h(\\mathbf{x}_i;\\mathbf{a}_m)) \\tag{12}\n\\]\n최종적으로 다음처럼 함수를 업데이트 할 수 있습니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m h(\\mathbf{x};\\mathbf{a}_m)\n\\]\n이상의 과정에서 중요한 점은 이 방법은 \\(h(\\mathbf{x}_i;\\mathbf{a})\\)이 어떤 모델이든 상관이 없다는 것입니다. 즉, 이 방법은 매우 일반적인 방법론입니다. 사이킷런 구현은 결정트리로 되어 있지만 기타 다른 모델에 대해서도 잘 작동하는 것을 이후 코딩 실험으로 확인해보도로고 하겠습니다.2\n결과적으로 그래디언트 \\(h(\\mathbf{x};\\mathbf{a}_m)\\)에 의해서 모델이 업데이트 되는 식이 되었고 그래서 이 알고리즘의 이름이 그래디언트 부스팅이 됩니다.\n\n\n분류문제에서도 이런 성질은 그대로 유지되지만 TreeBoost라는 이름으로 수정된 방식을 위해서는 꼭 결정트리를 사용해야 하는 것도 ’TreeBoost’절에서 확인해보겠습니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#그래디언트-부스팅-알고리즘",
    "href": "posts/gradientboost/gradient_boosting.html#그래디언트-부스팅-알고리즘",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "그래디언트 부스팅 알고리즘",
    "text": "그래디언트 부스팅 알고리즘\n이제 위에서 이야기한 내용을 구체적인 단계로 적용시켜 보겠습니다. 먼저 \\(F_0(\\mathbf{x})\\)를 정의 합니다.\n\n\n\n최초 학습기 \\(F_0(x)\\)를 다음을 만족하는 상수 \\(\\rho_0\\)로 설정합니다.\n\n\n\\[\nF_0(x) = \\underset{\\rho}{\\text{argmin}} \\sum_{i=1}^n L(y_i, \\rho) \\tag{step 1}\n\\]\n구체적으로 \\(\\rho_0\\)를 결정하는 과정은 이후 다시 알아보겠습니다. (step 1)에서 정한 학습기는 어떤 입력이 들어와도 무조건 상수 \\(\\rho_0\\)를 출력하는 더미 학습기입니다. 그래서 앞 예제에서 h_0로 DummyRegressor를 사용했습니다.\n - 2) 피팅할 pseudo-response를 다음처럼 계산합니다.\n\\[\n\\tilde{y}_i = - \\left[ \\frac{\\partial \\, L(y_i, F(\\mathbf{x}_i)) }{\\partial F(\\mathbf{x}_i)} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})} \\tag{step 2}\n\\]\n - 3) 앞서 기술한대로 어떤 손실함수를 쓰던지 상관없이 약한 학습기 \\(h(\\mathbf{x};\\mathbf{a}_m)\\)를 \\(\\{\\mathbf{x}_i, \\tilde{y}_i\\}_{i=1}^N\\)에 대해서 학습시킵니다. 이 단계에서 \\(h(\\mathbf{x};\\mathbf{a}_m)\\)는 어떤 모델이라도 상관없지만 주로 결정트리를 사용합니다. 아래 코딩 예제에서 결정트리 이와의 모델로도 직접 피팅해보도록 하겠습니다.\n\\[\n\\mathbf{a}_m = \\underset{\\mathbf{a}}{\\text{argmin}} \\sum_{i=1}^N (-g_m(\\mathbf{x}_i) - h(\\mathbf{x}_i;\\mathbf{a}))^2 \\tag{step 3}\n\\]\n - 4) step 3에을 구해진 그래디언트 방향으로 스탭사이즈 \\(\\rho_m\\)을 구합니다. 보통 선탐색하지 않고 고정된 학습률을 사용하기 때문에 이 단계는 적절한 고정 숫자 \\(\\rho_m\\)으로 대체될 수 있습니다. 이후 코딩 실습에서 실제 선탐색도 해보도록 하겠습니다. 스탭사이즈가 결정되었다면 step 2로 돌아가서 반복합니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}} \\sum_{i=1}^N L(y_i, F_{m-1}(\\mathbf{x}_i) + \\rho h(\\mathbf{x}_i;\\mathbf{a}_m)) \\tag{step 4}\n\\]\n - 5) step 3, 4에서 구한 그래디언트와 스탭사이즈로 다음처럼 업데이트 합니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m h(\\mathbf{x};\\mathbf{a}_m) \\tag{step 5}\n\\]\n\n이제 $ L(y, F())$를 다음처럼 구체적으로 squared loss로 두면\n\\[\nL(y, F(\\mathbf{x}))=\\frac{1}{2}  (y- F(\\mathbf{x}))^2\n\\]\n\\(F_{m-1}(\\mathbf{x})\\)에서 \\(F(\\mathbf{x})\\)에 대한 미분은 다음과 같이 됩니다.\n\\[\n\\left[ \\frac{\\partial }{\\partial F(\\mathbf{x})} \\left( \\frac{1}{2}  (y_i- F(\\mathbf{x}_i))^2 \\right) \\right]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} =  F_{m-1}(\\mathbf{x}_i) - y_i\n\\]\n마이너스 그래디언트를 만들기 위해 마이너스를 곱하면\n\\[\n\\tilde{y}_i =  y_i - F_{m-1}(\\mathbf{x}_i)\n\\]\n가 되는데 결국 그래디언트의 함수값이 이전 함수의 결과와 정답값의 차이가 되기 때문에 이를 pseudo-residual이라고도 하게 됩니다.\n이렇게 손실함수를 오차제곱합으로 정의했기 때문에 회귀문제에만 가능할 것 같지만 \\(y_i\\)를 0 또는 1인 타겟으로 두고 분류문제에 적용해도 큰 문제없이 적용 가능함을 잠시 후 코드로 확인해보겠습니다.\n하지만 분류문제에 오차제곱합 손실을 쓰는 것이 그렇게 바람직한 상황은 아닌 것은 분명합니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#직접-구현-1",
    "href": "posts/gradientboost/gradient_boosting.html#직접-구현-1",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "직접 구현 1",
    "text": "직접 구현 1\n지금끼지 이야기한 내용을 코드로 구현해서 작동을 확인해보겠습니다. 아래 코드는 최적화 과정중 선탐색을 하기 위해 황금분할 탐색법을 구현한 보조 코드입니다. 사이킷 런의 그래디언트 부스팅은 선탐색을 하지 않기 때문에 필수적인 코드는 아닙니다.\n\ndef gss(f_alpha, delta=1.0e-2, tol=1e-15):\n    '''\n    Line search function by golden section search\n    https://en.wikipedia.org/wiki/Golden-section_search and [arora]\n    \n    f_alpha: 1D objective function\n    delta  : Init. guess interval determining initial interval of uncertainty\n    tol    : stop criterion\n    '''\n    gr = (np.sqrt(5) + 1) / 2\n    \n    ########################################################################################\n    # ESTABLISH INITIAL DELTA\n    # 초기 delta를 잡는다.\n    # alpah = 0에서 값과 delta에서의 함수값을 계산하고 delta에서의 값이 크다면 delta를 줄인다.\n    ########################################################################################\n    AL = 0.\n    FL = f_alpha(AL)\n    AA = delta\n    FA = f_alpha(AA)\n    while  FL &lt; FA :\n        delta = 0.1*delta\n        AA = delta\n        FA = f_alpha(AA)\n    ########################################################################################\n    \n    ########################################################################################\n    # ESTABLISH INITIAL INTERVAL OF UNCERTAINTY\n    # delta를 사용하여 초기 불확정 구간을 설정한다.\n    # 결정된 구간을 [AL, AU] 로 두고 황금분할 탐색을 시작한다.\n    ########################################################################################\n    j = 1\n    AU = AA + delta * (gr**j)\n    FU = f_alpha(AU)\n    while FA &gt; FU :\n        AL = AA\n        AA = AU\n        FL = FA\n        FA = FU\n        \n        j += 1\n        AU = AA + delta * (gr**j)\n        FU = f_alpha(AU)\n\n    AB = AL + (AU - AL) / gr\n\n    FB = f_alpha(AB)\n    \n    while abs(AA - AB) &gt; tol:\n        if f_alpha(AA) &lt; f_alpha(AB):\n            AU = AB\n        else:\n            AL = AA\n\n        # we recompute both c and d here to avoid loss of precision \n        # which may lead to incorrect results or infinite loop\n        AA = AU - (AU - AL) / gr\n        AB = AL + (AU - AL) / gr\n\n    return (AU + AL) / 2\n\n구현은 두가지 함수로 이뤄져있습니다. train_gradient_boost()는 데이터와 약한학습기 h와 약한학습기 수 M과 학습률 lr을 넘겨 받아 이전 절에서 설명한 내용을 그대로 따라갑니다. 자세한 설명은 코드에 달았습니다.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.base import clone\n\n# 그래디언트부스팅 직접 만들기[+]\ndef train_gradient_boost(X, y, h, M=10, lr=0.1, loss=None):\n    \"\"\"\n    X: train samples (N,D), N: # of samples, D: the dimension of a sample\n    y: target\n    h: weak learner instance\n    M: # of week learner (except for the first dummy learner)\n    lr: float: learning_rate, 'linesearch': linesearch\n    loss: loss function whose args are predictions and ground truth\n    \"\"\"\n    loss_values = []\n\n    # 위 알고리즘에서처럼 상수로 첫번째 예측기를 정한다,  m=0\n    # step 1, F0 = argmin_{rho} sum_i L(y_i, rho)\n    # 타겟의 평균으로 \n    H = [ y.mean() ] \n    Fm_1 = np.ones(X.shape[0])*H[0]\n    steps = [None, ]\n\n    do_linesearch = True if type(lr) is str else False\n\n    # 학습\n    for m in range(1, M+1):# 최초 더미 예측기 제외 M개 트리 \n        # 각 단계의 학습기에서 만들어낸 로스 저장\n        # 여기서는 로스값을 저장할 목적으로 loss를 사용할 뿐\n        # 실제 계산을 위해 사용하는 것은 아님\n        # 이 코드에서 실제 loss의 미분은 squared loss로 하드코딩\n        if loss is not None:\n            loss_values.append( loss(y, Fm_1) )\n\n        # step 2 그래디언트의 값 구함 pseudo-residual\n        r_im = y - Fm_1 \n\n        # step 3 구해진 잔차 r_im에 대해서 학습기 hm을 학습\n        h_ = clone(h)\n        H.append( h_.fit(X, r_im) ) \n\n        # step 4 & 5\n        if do_linesearch:\n            def f_rho(rho):\n                return loss(y, Fm_1 + rho * H[-1].predict(X))\n\n            rho = gss(f_rho)\n            steps.append(rho)\n            \n            Fm_1 += steps[-1]*H[-1].predict(X)\n        else:\n            Fm_1 += lr*H[-1].predict(X)\n\n    if do_linesearch:\n        return {'learners':H, 'learning_rate':steps, 'loss_values':loss_values}\n    else:\n        return {'learners':H, 'learning_rate':lr, 'loss_values':loss_values}\n    \n\n# 예측하기[+]\ndef predict(X, gradient_boost):\n    \"\"\"\n    X: input, (N,D)\n    gradient_boost: model dict. that has been trained \n    \"\"\"\n    H = gradient_boost['learners']\n    lr = gradient_boost['learning_rate']\n\n    # 0번째 약한학습기는 모든 입력에 대해서 상수 H[0] 출력\n    F = np.ones(X.shape[0])*H[0]\n\n    # 1번째 약한 학습기부터 M번째 약한 학습기까지 결과 더하기\n    for m in range(1, len(H)):\n        # lr이 리스트로 구성되었으면 각 단계에서 선탐색을 한것이므로\n        # 각 단계마다 결정된 스탭사이즈를 사용\n        if hasattr(lr, '__iter__'):\n            F += lr[m]*H[m].predict(X)\n        # 그렇지 않으면 고정 러닝레이트를 사용    \n        else:\n            F += lr*H[m].predict(X)\n\n    return F"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#회귀",
    "href": "posts/gradientboost/gradient_boosting.html#회귀",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "회귀",
    "text": "회귀\n이제 직접 구현한 코드를 회귀 문제에 적용해보도록 하겠습니다.\n\nKNeighborsRegressor 이용\n당연한 이야기지만 단계별 학습기로 꼭 결정 트리를 사용할 필요없습니다. 여기서는 KNeighborsRegressor으로 먼저 시도해보겠습니다. 먼저 학습과정에서 줄어드는 손실을 계산하기 위해 간단한 squared loss 함수를 정의합니다.\n\nsquared_loss = lambda y, pred:  np.sum((y - pred)**2)\n\n\n# 개별 estimator는 꼭 결정트리가 아니어도 상관없음 \nfrom sklearn.neighbors import KNeighborsRegressor\n\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nh = KNeighborsRegressor(n_neighbors=20)\n\n# 선탐색하는 경우와 하지 않는 경우를 비교해 봅니다.\n# learning_rate = 'linesearch'\nlearning_rate = 0.1\ngb_reg_knn_mse = train_gradient_boost(X_reg, y_reg, h, M=50, \n                                  lr=learning_rate, loss=squared_loss)\n# 예측결과의 저장은 pred_{datsset}_{weak learner}_{loss}로 함\npred_X_reg_knn_mse = predict(X_reg, gb_reg_knn_mse)\npred_x_reg_knn_mse = predict(x, gb_reg_knn_mse)\n\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True)\n\nax[0].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[0].scatter(X_reg, pred_X_reg_knn_mse, marker='o', color='C2', edgecolor='k', label=\"pred\")\nax[0].legend()\n\nax[1].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[1].plot(x, pred_x_reg_knn_mse, color='C2', lw=3, label=\"pred\")\nax[1].legend()\n\nplt.show()\n\n# fig.savefig(\"reg_knn_mse_lrate.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n결과를 보면 그럴듯 하게 회귀된 것을 확인할 수 있습니다. 여기서 각 학습단계에 대한 손실도 그려보면\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_reg_knn_mse['loss_values'], '.-', color='C1')\nax.set_xlabel('iterations')\nax.set_ylabel('loss')\nplt.show()\n\n# fig.savefig(\"reg_knn_mse_lrate_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n단계별 손실함수 값도 안정적으로 줄어들고 있는 것을 확인할 수 있습니다.\n보통 개별 학습기로 결정 트리를 사용하고 사이킷-런도 내부 예측기가 결정트리로 구현되어 있으므로 결정트리로 다시 시도해보겠습니다.\n\n\nDecisionTreeRegressor 이용\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\n# learning_rate = 'linesearch'\nlearning_rate = 0.1\ngb_reg_tree_mse = train_gradient_boost(X_reg, y_reg, h, M=50, \n                                   lr=learning_rate, loss=squared_loss)\npred_X_reg_tree_mse = predict(X_reg, gb_reg_tree_mse)\npred_x_reg_tree_mse = predict(x, gb_reg_tree_mse)\n\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True)\n\nax[0].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[0].scatter(X_reg, pred_X_reg_tree_mse, marker='o', color='C2', edgecolor='k', label=\"pred\")\nax[0].legend()\n\nax[1].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[1].plot(x, pred_x_reg_tree_mse, color='C2', lw=3, label=\"pred\")\nax[1].legend()\n\nplt.show()\n\n# fig.savefig(\"reg_tree_mse_lrate.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_reg_tree_mse['loss_values'], '.-', color='C1')\nax.set_xlabel('iterations')\nax.set_ylabel('loss')\nplt.show()\n\n# fig.savefig(\"reg_tree_mse_lrate_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n회귀 결과와 손실함수값을 확인해보면 문제없이 잘 작동하는 것 같습니다.\n이렇게 간단한 실험을 통해 약한 학습기는 knn, 트리 모델 모두 상관없이 잘 작동한다는 약한 학습기의 일반성을 확인해봤습니다.\n\n\nsklearn 사용\n지금까지 직접 코딩한 버전을 바닐라 버전이라 칭하고 사이킷런을 사용해서 동일한 문제를 풀어보겠습니다.\n사이킷런에서 제공하는 GradientBoostingRegressor를 사용합니다. 여기서 각 개별 약한 학습기를 따로 지정할 수 없고 무조건 결정트리가 사용되는데 트리 분기 기준으로 criterion='squared_error를 사용하여 트리의 작동방식을 검증하겠습니다.\n\n# 모델 로드\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# 동일 조건으로 모델 생성과 fit\nlearning_rate = 0.1\ngb_reg_sk = GradientBoostingRegressor(criterion='squared_error', \n                                 max_depth=2, n_estimators=50, \n                                 learning_rate=learning_rate)\ngb_reg_sk.fit(X_reg,y_reg)\n\nGradientBoostingRegressor(criterion='squared_error', max_depth=2,\n                          n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(criterion='squared_error', max_depth=2,\n                          n_estimators=50)\n\n\n\n# 예측\npred_x_reg_sk = gb_reg_sk.predict(x)\npred_x_reg_sk.shape\n\n(1000,)\n\n\n\n# 그림 확인\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.plot(X_reg, y_reg , '.', color='C1')\nax.plot(x, pred_x_reg_sk, color='C3', lw=3, label='sklearn')\nax.plot(x, pred_x_reg_tree_mse, '--', color='C2', label='vanilla gradient boost')\n\nax.legend()\nplt.show()\n\n# fig.savefig(\"reg_tree_mse_sklearn.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n사이킷런 예측과 직접 만든 예측을 비교해보면 동일한 것을 알 수 있습니다. 정확히 확인하게 위해 예측을 서로 비교해보면\n\nnp.sum((pred_x_reg_sk - pred_x_reg_tree_mse)**2)\n\n0.0\n\n\n0이 되어 두 예측 결과는 완전히 동일함을 알 수 있습니다.\n\n사이킷런 결과 분석\n이제 사이킷런이 만들어낸 첫번째 학습기와 두번째 학습기를 직접 확인하면서 내부적으로 어떻게 구현되었는지 확인해보도록 하겠습니다. 이미 확인한 결과를 통해 예상해보면 직접 만든 구현과 크게 다르지 않을 것 같습니다.\n사이킷런 그래디언트 부스팅 객체에는 gb_reg_sk.init_에 무조건 평균을 예측하는 DummyRegressor() 학습기 있고 gb_reg_sk.estimators_에 (M,1)인 어레이로 학습기가 순서대로 들어 있습니다. 그러니까 더미 포함 총 M+1개가 있는 것입니다.\n\n# 첫번째 트리의 예측\n\ny_reg_mean = y_reg.mean()\nprint(f\"초기 더미 예측:{np.unique(gb_reg_sk.init_.predict(X_reg))[0]:.6f}, \\\n학습세트 타겟평균:{y_reg_mean:.6f}\")\n\n# 더미 예측에 학습률만큼 곱한 첫번째 예측기의 예측을 더함\nh_1 = gb_reg_sk.estimators_[0,0]\npred_1 = gb_reg_sk.init_.predict(x) + learning_rate*h_1.predict(x)\n\n# 그림으로 확인\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.plot(X_reg, y_reg , '.', color='C1')\nax.plot(x, pred_1, color='C3', lw=3, label=r'$h_0 + \\eta h_1$')\n\nfor thr in h_1.tree_.threshold:\n    if X_reg.min() &lt; thr &lt; X_reg.max():\n        ax.vlines(thr, ymin=y_reg.min(), ymax=y_reg.max(), \n                  ls='--', color='C4', lw=2, alpha=0.5)\n\nax.legend()\nplt.show()\n\n# fig.savefig(\"gb_reg_sk_h0h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n초기 더미 예측:0.265458, 학습세트 타겟평균:0.265458\n\n\n\n\n\n위 그림은 사이킷런이 학습한 모델 gb_reg_sk의 더미 학습기와 첫번째 학습기가 만들어낸 예측 \\(h_0(x) + \\eta h_1(x)\\)을 그린 것입니다. 여기서 스탭사이즈는 고정 학습률을 사용했기 때문에 \\(\\eta\\)라 표현했습니다. 첫번째 트리 \\(h_1(x)\\)는 세번 분기한것을 알 수 있습니다. 이 트리를 직접 그려보면 다음과 같습니다.\n\n# h_1을 시각화\nfig = plt.figure(figsize=(10,5), dpi=100)\nax = plt.axes()\n\n# 트리 그림 그리기\ndot_data = export_graphviz(h_1, out_file=None, max_depth=2)\ngraph = graphviz.Source(dot_data, format=\"png\")\ntree_img = imread(graph.render())\nax.imshow(tree_img)\nax.axis('off')\n\nplt.show()\n\n# fig.savefig(\"gb_reg_sk_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n/tmp/ipykernel_127927/1229487347.py:8: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  tree_img = imread(graph.render())\n\n\n\n\n\n그림으로 표현된 첫번때 트리 h_1을 보고 각 노드에 출력값 squared_error와 value를 계산해보겠습니다.\n\n# 첫번째 트리의 타겟은 더미 예측기의 출력인 y_reg_mean과의 차이가 됨\nresidual_1  = y_reg - y_reg_mean\nprint(f\"root value={residual_1.mean():.3f}\")\n\n# 첫번째 분류에서 벨류값이 그냥 평균이냐?\ndepth1_first_node_idx = X_reg[:,0]&lt;=-0.36\ndepth1_first_node_target = residual_1[depth1_first_node_idx]\n\n# squared_error은 평균과 각 타겟 사이의 mse\nprint(\"첫번째 트리의 depth1 first node 결과\")\nprint(f\"squared_error={np.mean((depth1_first_node_target - depth1_first_node_target.mean())**2):.3f}\") \nprint(f\"value={depth1_first_node_target.mean():.3f}\") # 출력값은 그냥 평균\n\nroot value=0.000\n첫번째 트리의 depth1 first node 결과\nsquared_error=0.012\nvalue=0.309\n\n\n루트노드에 모인 샘플의 타겟은 더미 학습기 예측과 최초 타겟값의 차이, 잔차(residual)이며 이를 평균한 값이 루트노드의 value로 출력되었음을 알 수 있습니다. 깊이1에서 첫번째 노드에 나타난 value=0.309는 루트노드에 있는 샘플이 X[0]&lt;=-0.36이라는 조건을 통해 이 노드에 모인 샘플들의 타겟값 평균임을 확인할 수 있습니다. 깊이1에서 첫번째 노드에 나타난 squared_error=0.012는 앞서 구한 value값과 노드에 모인 샘플들의 타겟값 사이에서 계산되는 오차제곱합임을 알 수 있습니다.\n이제 h_1트리의 실제 출력을 담당하는 리프노드에서 값도 확인해보겠습니다.\n\ndepth2_first_node_idx = X_reg[:,0] &lt;= -0.43\n\ndepth2_first_node_target = residual_1[depth1_first_node_idx & depth2_first_node_idx]\n\nprint(\"첫번째 트리의 depth2 first node 결과\")\nprint(f\"squared_error: {np.mean((depth2_first_node_target - depth2_first_node_target.mean())**2):.3f}\") \nprint(f\"value: {depth2_first_node_target.mean():.3f}\") # 출력값은 그냥 평균\n\n첫번째 트리의 depth2 first node 결과\nsquared_error: 0.004\nvalue: 0.395\n\n\n깊이1의 첫번째 노드처럼 그냥 노드 샘플의 평균을 value로 출력하는 일반적인 회귀 트리임을 확인할 수 있습니다. 이렇게 그래디언트 부스팅이 회귀 문제에 적용되면 예측과 타겟의 잔차를 다시 타겟으로 하는 회귀 트리의 연속적인 모임이라는 것을 알 수 있습니다.\n앞서 복잡한 식을 통해 유도했던 과정이 그대로 적용됨을 실제 사이킷런 학습 결과를 분석하면서 확인할 수 있었습니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#분류",
    "href": "posts/gradientboost/gradient_boosting.html#분류",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "분류",
    "text": "분류\n이제 위에서 만든 함수를 수정없이 그대로 이진분류 문제에 적용해보겠습니다. 이진분류 문제에 적용하게 되면 타겟은 0 또는 1이 되며 잔차는 0 또는 1과 예측값의 차이가 될 것입니다. 원칙적으로 이 문제에 대한 학습기는 0보다 작은값 또는 1보다 큰값을 출력하지 않아야 합니다. 현재 작성된 함수에 그런 제약조건을 고려하지 않아서 1보다 큰값, 0보다 작은 값이 출력될 수 있지만 그것과 별개로 손실을 줄이도록 만들어졌으므로 전반적으로는 잘 작동할 것으로 예상됩니다.\n두가지 예제를 통해 확인해보겠습니다.\n\n첫번째 예제\n첫번째 예제는 사이킷런의 make_gaussian_quantiles 함수를 통해 생성합니다.\n\n# https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\n\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(\n    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1\n)\nX2, y2 = make_gaussian_quantiles(\n    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1\n)\nX_clf1 = np.concatenate((X1, X2))\ny_clf1 = np.concatenate((y1, -y2 + 1))\n\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X_clf1[y_clf1==0][:, 0], X_clf1[y_clf1==0][:, 1], 'o', color='C1')\nax.plot(X_clf1[y_clf1==1][:, 0], X_clf1[y_clf1==1][:, 1], '^', color='C2')\nax.axis('tight')\nplt.show()\n\n\n\n\n앞서 만들어둔 함수 train_gradient_boost는 오차제곱합 손실함수에 대한 그래디언트를 계산하므로 분류 문제에서도 타겟 0, 1이 회귀해야할 실제 값입니다. 따라서 개별 학습기도 여전히 Regressor로 지정해야 합니다. 이런 이유로 그래디언트 부스팅 알고리즘의 다른 이름이 그래디언트 부스팅 회귀트리gradient boosting regression tree가 되는 것입니다.\n\nDecisionTreeRegressor 이용\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 0.2\ngb_clf1_tree_mse = train_gradient_boost(X_clf1, y_clf1, h, M=50, \n                                        lr=learning_rate, loss=squared_loss)\n\n\n실행 결과를 그림으로 확인해보자.\n\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_x_clf1_tree_mse = predict(X_grid, gb_clf1_tree_mse)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf1_tree_mse &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\n\nax.axis('tight')\n\nplt.show()\n\npred_x_clf1_tree_mse = Z.copy()\n\n# fig.savefig(\"gb_clf1_tree_mse.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n생각보다 나쁘지 않게 잘되는 것을 확인할 수 있습니다. 학습시키는 함수로부터 각 단계에서 발생한 로스값을 돌려받았으므로 그림으로 그려보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf1_tree_mse['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n# fig.savefig(\"gb_clf1_tree_mse_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n손실도 꾸준히 잘 감소하는 것을 확인할 수 있습니다. 재미삼아 kNN을 개별 예측기로 사용해서 실험 해보겠습니다.\n\n\nKNeighborsRegressor 이용\n\nh = KNeighborsRegressor(n_neighbors=10)\n\nlearning_rate = 0.1\ngb_clf1_knn_mse = train_gradient_boost(X_clf1, y_clf1, h, M=25, \n                                       lr=learning_rate, loss=squared_loss)\n\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_x_clf1_knn_mse = predict(X_grid, gb_clf1_knn_mse)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf1_knn_mse &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\n\nax.axis('tight')\n\nplt.show()\n\n# fig.savefig(\"gb_clf1_knn_mse.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\nkNN 결과도 나쁘지 않습니다. 손실도 그려보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf1_knn_mse['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n# fig.savefig(\"gb_clf1_knn_mse_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\nk값, 학습률, 개별 학습기 수 M을 적당히 잘 지정하면 나쁘지 않은 결과를 얻을 수 있습니다. 분류된 결과는 결정 트리를 기반으로 하는 경우와는 사뭇 다른 모습인것도 재미있는 부분입니다.\n이제 사이킷런으로 같은 데이터에 대해서 실행해보겠습니다.\n\n\nsklearn 사용\n사이킷런의 GradientBoostingClassifier를 사용합니다. 단 여기서는 어떤 이유로 인해 학습률을 1로 지정하도록 하겠습니다. 1로 두는 이유는 이 글 마지막에 이야기하도록 하겠습니다.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nlearning_rate = 1.0\n# 옵션 criterion 은 약한 학습기\ngb_clf1_sk = GradientBoostingClassifier(criterion='squared_error', \n                                        learning_rate=learning_rate, \n                                        n_estimators=50, max_depth=2)\n\n\ngb_clf1_sk.fit(X_clf1, y_clf1)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)\n\n\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nZ = gb_clf1_sk.predict(X_grid)\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\n\nax.axis('tight')\n\nplt.show()\n\npred_x_clf1_sk = Z.copy()\n\n# fig.savefig(\"gb_clf1_sk.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n결정트리를 개별 학습로 사용한 결과와 크게 다르지 않은 결과를 확인할 수 있습니다. 하지만 회귀의 경우처럼 완벽하게 동일하지는 않기 때문에 사이킷런 학습 과정은 앞서 설명한 그래디언트 부스팅의 과정과 약간 다를 것이란 것을 짐작할 수 있습니다.\n조금 더 간단한 두 번째 예제를 하나 더 실행하고 두 번째 예제를 가지고 분석을 해보도록 하겠습니다.\n\n\n\n두번째 예제\n\nfrom sklearn.datasets import make_moons\n\nX_clf2, y_clf2 = make_moons(n_samples=100, noise=0.25, random_state=3)\n\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X_clf2[y_clf2==0][:, 0], X_clf2[y_clf2==0][:, 1], 'o', color='C1')\nax.plot(X_clf2[y_clf2==1][:, 0], X_clf2[y_clf2==1][:, 1], '^', color='C2')\nax.axis('tight')\nplt.show()\n\n\n\n\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 0.2\ngb_clf2_tree_mse = train_gradient_boost(X_clf2, y_clf2, h, M=50, \n                                        lr=learning_rate, loss=squared_loss)\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_x_clf2_tree_mse = predict(X_grid, gb_clf2_tree_mse)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf2_tree_mse &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.axis('tight')\n\nplt.show()\n\npred_x_clf2_tree_mse = Z.copy()\n\n\n# fig.savefig(\"gb_clf2_tree_mse.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf2_tree_mse['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n\n# fig.savefig(\"gb_clf2_tree_mse_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n이번 예제도 예측 결과나 손실이 줄어드는 과정이 썩 나쁘지 않습니다. 사이킷런으로 실행해서 결과를 비교해봅시다.\n\nsklearn 사용\n이전과 동일하게 실행하며 이번에도 학습률을 1로 두겠습니다.\n\nlearning_rate = 1.0\ngb_clf2_sk = GradientBoostingClassifier(criterion='squared_error', \n                                 learning_rate=learning_rate, \n                                  n_estimators=50, max_depth=2)\n\n\ngb_clf2_sk.fit(X_clf2, y_clf2)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)\n\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nZ = gb_clf2_sk.predict(X_grid)\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.axis('tight')\n\nplt.show()\n\npred_x_clf2_sk = Z.copy()\n\n# fig.savefig(\"gb_clf2_sk.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n역시 비슷한 결과를 얻을 수 있지만 완전히 동일하지 않은 것을 확인할 수 있습니다. 무엇이 다른지 확인하기 위해 회귀때 처럼 첫번째 트리를 분석해보기로 하겠습니다.\n\n\n사이킷런 결과 분석\n\n# 첫번째 트리의 예측\ny_mean = y.mean()\nprint(f\"초기 더미 예측:{np.unique(gb_clf2_sk.init_.predict(X))[0]:.6f}, 학습세트 타겟평균:{y_mean:.6f}\")\n\n초기 더미 예측:0.000000, 학습세트 타겟평균:0.500000\n\n\n초기 타겟의 평균은 0.5인데 더미 예측기가 타겟의 평균을 출력하지 않는것을 확인할 수 있습니다. 초기 더미 예측기가 0을 출력한 것은 타겟 평균 0.5를 확률값으로 보고 이에 대한 로짓값을 출력하기 때문입니다.\n\n# 위에 더미 예측기의 출력값이 0 이 나오는 이유는? \n# 예측기는 확률값을 출력하지 않고 로짓을 출력한다.\np = y_mean\nnp.log(p / (1-p))\n\n0.0\n\n\n이 사실로부터 분류문제에서 각 학습기는 로짓을 출력한다고 가정한 것을 알 수 있습니다. 마치 로지스틱 회귀의 선형함수 부분과 같은 역할을 한다고 생각할 수 있습니다. 로지스틱회귀에서도 선형식을 이용해서 [0,1]로 바운드되지 않는 로짓값을 출력하고 이를 로지스틱 시그모이드 함수에 입력하여 최종 출력을 분류문제에 적합한 확률로 바꾸는데 그래디언트 부스팅도 유사하게 작동하는 것입니다.\n개별 학습기가 확률을 바로 출력한다고 하면 [0,1]로 바운드된 값을 출력해야 하는데 이렇게 하기 위해서는 제약조건을 걸어야 합니다. 그것보다 출력을 로짓으로 가정하고 \\((-\\infty, \\infty)\\)로 언바운드된 값을 출력하게 하는 편이 더 간편합니다.\n이제 \\(F_m(\\mathbf{x})\\)가 로짓을 출력하는 함수이므로 로짓을 입력으로 받는 목적함수를 정의 해야 합니다.\n\n\n\nDeviance loss\n이진분류 문제에서 로그 가능도는 다음과 같습니다.\n\\[\n\\ell = y \\log(p) + (1-y) \\log(1 - p)\n\\]\n위 식에서 이 식에 입력되는 값이 확률임을 분명히 하기 위해 \\(p\\)로 표기 했습니다. 적당히 전개를 합니다.\n\\[\n\\begin{aligned}\n\\ell &= y \\log(p) + (1-y) \\log(1 - p) \\\\[5pt]\n&= y \\log(p) + \\log(1 - p) - y \\log(1 - p) \\\\[5pt]\n&= \\log(1 - p) + y ( \\log(p) - \\log(1 - p)) \\\\[5pt]\n&= \\log(1 - p) + y \\log \\left( \\frac{p}{1-p} \\right)\n\\end{aligned} \\tag{[1]}\n\\]\n한편 \\(p\\)와 로짓 \\(z\\)의 관계는 다음과 같으므로 (확률은 더해서 1이 된다는 것을 이용)\n\\[\np = \\frac{e^{z}}{1+e^{z}} \\implies 1-p = \\frac{1}{1+e^{z}}\n\\]\n두번째 결과에 로그를 취하면\n\\[\n\\log(1 - p) = \\log \\left( \\frac{1}{1 + e^{z}} \\right) =  -\\log (1+ e^{z}) \\tag{[2]}\n\\]\n이 결과를 [1]에 대입하면\n\\[\n\\ell =  -\\log (1+ e^{z}) + y \\log \\left( \\frac{p}{1 - p} \\right)  \n\\]\n여기서 \\(\\log \\left( \\frac{p}{1 - p} \\right) = z\\)이므로\n\\[\n\\ell =  y  z- \\log (1+ e^{z})\n\\]\n손실로 만들기위애 가능도에 마이너스를 곱하면\n\\[\nL(y, z) = -y  z + \\log (1+ e^{z})\n\\]\n그리고 가정에 의해\n\\[\nz = F(\\mathbf{x})\n\\]\n이므로 최종적으로 손실함수는 다음처럼 주어집니다.\n\\[\nL(y, F(\\mathbf{x})) = -y F(\\mathbf{x}) + \\log \\left( 1+ e^{F(\\mathbf{x})} \\right) \\tag{[3]}\n\\]\n이제 [3]를 \\(F(\\mathbf{x})\\)에 대해 미분해보면\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\, F(\\mathbf{x})} L(y, F(\\mathbf{x}))  &=  -y F(\\mathbf{x}) + \\log \\left( 1+ e^{F(\\mathbf{x})} \\right) \\\\\n&= -y + \\frac{e^{F(\\mathbf{x})}}{1+ e^{F(\\mathbf{x})}}\n\\end{aligned}\n\\]\n그래디언트를 구하기 위해 그래디언트가 정의되는 포인트를 \\(F_{m-1}(\\mathbf{x})\\)로 설정하면\n\\[\n\\left[ \\frac{\\partial}{\\partial \\, F(\\mathbf{x})} L(y, F(\\mathbf{x})) \\right]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} = -y + \\frac{e^{F_{m-1}(\\mathbf{x})}}{1+ e^{F_{m-1}(\\mathbf{x})}} = -y + p_{m-1}(\\mathbf{x})\n\\]\n가 됩니다.\n강하방향으로 만들기 위해 마이너스를 곱하면\n\\[\n-\\left[ \\frac{\\partial}{\\partial \\, F(\\mathbf{x})} L(y, F(\\mathbf{x})) \\right]_{F(\\mathbf{x})= F_{m-1}(\\mathbf{x}) } = y - p_{m-1}(\\mathbf{x})\n\\]\n따라서 pseudo-response는 다음과 같습니다.\n\\[\n-g_m(\\mathbf{x}_i) = y_i - p_{m-1}(\\mathbf{x}_i) \\tag{[4]}\n\\]\n이 식에서 \\(p_{m-1}\\)은 \\(F_{m-1}(\\mathbf{x})\\)가 만들어낸 출력 로짓을 확률로 변환한 값을 의미합니다. 그러고 보면 이번에도 역시 그래디언트의 유사값analogue이 확률의 잔차가 됨을 알 수 있습니다. 그래서 역시 이번에도 pseudo-residual이 됩니다.\n\\[\n\\left[ \\frac{\\partial}{\\partial \\, F(\\mathbf{x})} \\left( -y_i F(\\mathbf{x}_i) + \\log \\left(1+e^{F(\\mathbf{x}_i)} \\right) \\right) \\right]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} =  p_{m-1}(\\mathbf{x}_i) - y_i\n\\]\n\n\n직접구현 2\n위에서 유도한 새로운 손실함수에 대해서 코드를 작성했습니다. 뭔가 복잡하게 유도된 듯 하지만 결론은 식[4]이고 놀랍게도 식[4]는 기존 방식에서 잔차를 구하는 것과 동일합니다. 다른 점은 잔차를 계산하기전에 모델의 출력을 로짓에서 확률로 바꾸는 것 밖에 없습니다.\n\nlogit = lambda p: np.log(p / (1-p))\nproba = lambda z: 1 / (1+np.exp(-z))\n\n# 그래디언트부스팅 직접 만들기[+]\ndef train_gradient_boost_clf(X, y, h, M=10, lr=0.1, loss=None):\n    \"\"\"\n    X: train samples (N,D), N: # of samples, D: the dimension of a sample\n    y: target\n    h: weak learner instance\n    M: # of week learner (except for the first dummy learner)\n    lr: float: learning_rate, 'linesearch': linesearch\n    loss: loss function whose args are predictions and ground truth\n    \"\"\"\n    loss_values = []\n\n    # step 1\n    # 알고리즘에서처럼 상수로 첫번째 예측기를 정한다,  m=0\n    # 단 이번에는 평균에 대한 로짓을 함수값으로 가진다.\n    H = [ logit(y.mean()) ] \n    Fm_1 = np.ones(X.shape[0])*H[0]\n    steps = [None, ]\n\n    do_linesearch = True if type(lr) is str else False\n\n    # 학습\n    for m in range(1, M+1):# 최초 더미 예측기 제외 M개\n        # 각 단계의 학습기에서 만들어낸 로스 저장\n        # 여기서는 로스값을 저장할 목적으로 loss를 사용할 뿐\n        # 실제 계산을 위해 사용하는 것은 아님\n        # 이 코드에서 실제 loss의 미분은 squared loss로 하드코딩\n        if loss is not None:\n            loss_values.append( loss(y, Fm_1) )\n\n        # step 2 그래디언트의 값 N를 구함 pseudo-residual\n        # 위 유도 결과에 따라 잔차는 다시 확률로 바꿔서 계산\n        r_im = y - proba(Fm_1)\n        \n        # step 3 구해진 잔차 r_im에 대해서 학습기 hm을 학습\n        h_ = clone(h)\n        H.append( h_.fit(X, r_im) )\n\n        # step 4 & 5\n        if do_linesearch:\n            def f_rho(rho) :\n                return loss(y, Fm_1 + rho * H[-1].predict(X))\n\n            rho = gss(f_rho)\n            steps.append(rho)\n\n            Fm_1 += steps[-1]*H[-1].predict(X)\n        else:\n            Fm_1 += lr*H[-1].predict(X)\n    \n    if do_linesearch:\n        return {'learners':H, 'learning_rate':steps, 'loss_values':loss_values}\n    else:\n        return {'learners':H, 'learning_rate':lr, 'loss_values':loss_values}\n\n# 예측하기[+]\ndef predict_clf(X, gradient_boost):\n    \"\"\"\n    X: input, (N,D)\n    gradient_boost: model dict. that has been trained \n    \"\"\"\n    \n    H = gradient_boost['learners']\n    lr = gradient_boost['learning_rate']\n\n    # 0번째 약한학습기는 모든 입력에 대해서 상수 H[0] 출력\n    F = np.ones(X.shape[0])*H[0]\n\n    # 여기서 계산되는 함수값은 로짓이다.\n    for m in range(1, len(H)):\n        # lr이 리스트로 구성되었으면 각 단계에서 선탐색을 한것이므로\n        # 각 단계마다 결정된 스탭사이즈를 사용\n        if hasattr(lr, '__iter__'):\n            F += lr[m]*H[m].predict(X)\n        # 그렇지 않으면 고정 러닝레이트를 사용    \n        else:\n            F += lr*H[m].predict(X)\n\n    # 리턴하기 전에 확률값으로 바꾼다.\n    pred = proba(F)\n\n    return pred\n\n다시 코딩하긴 했지만 달라진 부분은 결국 손실함수가 바뀐 부분밖에 없습니다. 이전처럼 부스팅 과정에서 손실함수 값을 가져오기 위해 앞서 유도한 deviance loss를 준비합니다.\n\ndeviance_loss = lambda y, pred:  np.sum( -y*pred + np.logaddexp(0, pred) )\n\n그런데 방금 코딩한 손실함수를 사이킷런 구현에서 살펴보면 다음처럼 되어 있습니다.\n\nhttps://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/ensemble/_gb_losses.py#L633\n\n-2*np.mean( y*pred - np.logaddexp(0, pred) ) \n사이킥런에서는 sum() 대신 mean()을 쓰고 앞에 2가 곱해져 있습니다. mean()을 쓴것은 별로 문제가 안되는데 앞에 2가 곱해진 것은 좀 이해하기 어렵습니다. 2가 곱해진 이유는 로그가능도가 점근적으로 카이제곱분포를 따르도록 만들기 위함이라고 합니다. 물론 여기서 이야기하고 있는 내용은 통계학에서 가설검정과는 상관없는 내용이므로 2가 곱해전 것은 손실이 2배가 된다는 것 말고는 아무 의미도 없습니다.\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\n# learning_rate= 'linesearch'\ngb_clf1_tree_dev = train_gradient_boost_clf(X_clf1, y_clf1, h, M=50, \n                                      lr=learning_rate, loss=deviance_loss)\n\npred_X_clf1_tree_dev = predict_clf(X_clf1, gb_clf1_tree_dev)\n\n만들어진 모델을 학습데이터에 대해서 예측하여 몇개나 틀리는지 확인해보겠습니다.\n아래 셀을 실행하면 결과가 나오는데 위 셀에서 선탐색을 한 경우와 안한 경우를 비교해보면 선탐색을 한 경우 모든 데이터를 다 맞추는 것을 확인할 수 있습니다. 선탐색을 하지 않으면 42개는 틀리게 됩니다.\n\nnp.sum( (pred_X_clf1_tree_dev &gt;= 0.5).astype(int) != y_clf1 )\n\n42\n\n\n이제 이전 결과와 이번 결과를 비교 해봅시다.\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nfig, ax = plt.subplots(figsize=(15,4), nrows=1, ncols=3)\n\n# squared loss로 한것\n# Z = np.zeros(X_grid.shape[0])\n# Z[pred_x_clf1_tree_mse &gt;= 0.5] = 1.\n# Z = Z.reshape(X1.shape)\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf1_tree_mse, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('ours, squared loss(lr=0.2)')\n\n#sklearn\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, pred_x_clf1_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('sklearn, deviance loss(lr=1.0)')\n\n# deviance loss\npred_x_clf1_tree_dev = predict_clf(X_grid, gb_clf1_tree_dev)\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf1_tree_dev &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\nax[2].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[2].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[2].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[2].set_xlim(x_min, x_max)\nax[2].set_ylim(y_min, y_max)\nax[2].set_title(f'ours, deviance loss(lr={learning_rate})')\n\nplt.show()\n\npred_x_clf1_tree_dev = Z.copy()\n\n# fig.savefig(\"dv_clf1_test.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위 그림은 분류 1번 데이터에 대해서 squared loss와 deviance loss를 적용한 것을 비교한 그림입니다. 가운데 그림은 사이킷런으로 실행한 결과입니다. 세번째 그림이 첫번째 그림보다 가운데 그림과 조금 더 비슷하게 보입니다. 하지만 손실을 deviance loss로 바꾸고 학습률도 동일하게 두었는데도 사이킷런 결과와 완전히 일치하지 않습니다. 학습 과정중에 손실이 어떻게 줄어드는 지도 확인해보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf1_tree_dev['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n\n\n\n손실도 안정적으로 줄어들고 있습니다. 사이킷런 구현에 다른 뭔가가 있는 듯 해 보입니다. 계속해서 두 번째 예제에 대해서도 실험해봅시다.\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\n# learning_rate = 'linesearch'\nlearning_rate = 1.0\ngb_clf2_tree_dev = train_gradient_boost_clf(X_clf2, y_clf2, h, M=50, \n                                      lr=learning_rate, loss=deviance_loss)\npred_X_clf2_tree_dev = predict_clf(X_clf2, gb_clf2_tree_dev)\n\n\nnp.sum( (pred_X_clf2_tree_dev &gt;= 0.5).astype(int) != y_clf2 )\n\n0\n\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nfig, ax = plt.subplots(figsize=(15,4), nrows=1, ncols=3)\n\n# squared loss로 한것\n# Z = Z.reshape(X1.shape)\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf2_tree_mse, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('ours, squared loss(lr=0.2)')\n\n#sklearn\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, pred_x_clf2_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('sklearn, deviance loss(lr=1.0)')\n\n# deviance loss\npred_x_clf2_tree_dev = predict_clf(X_grid, gb_clf2_tree_dev)\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf2_tree_dev &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\nax[2].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[2].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[2].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[2].set_xlim(x_min, x_max)\nax[2].set_ylim(y_min, y_max)\nax[2].set_title(f'ours, deviance loss(lr={learning_rate})')\n\nplt.show()\n\n# fig.savefig(\"dv_clf2_test.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n여기서도 예제1과 마찬가지로 deviance loss를 쓴 경우 사이킷런과 좀 더 닮은 더 안정적인 결정 영역을 만드는 것처럼 보입니다. 하지만 이번 예도 아직 sklearn의 결과와는 약간 차이가 있습니다. 손실이 안정적으로 떨어지지 않아서 그런지 단계별 손실값도 다시 한번 그려보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf2_tree_dev['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n\n\n\n손실도 안정적으로 줄어들고 있습니다. 그렇다면 이제 남은 것은 무엇일까요? 무엇이 문제인지 알아보기 위해 여기서 사이킷런의 \\(h_1(x)\\)를 그려보도록 합시다.\n\n사이킷런 결과 분석(계속)\n\n# h_1을 시각화\nh_1 = gb_clf2_sk.estimators_[0,0]\n\nfig = plt.figure(figsize=(10,5), dpi=100)\nax = plt.axes()\n# 트리 그림 그리기\ndot_data = export_graphviz(h_1, out_file=None, \n                           max_depth=3, precision=3)\ngraph = graphviz.Source(dot_data, format=\"png\")\ntree_img = imread(graph.render())\nax.imshow(tree_img)\nax.axis('off')\nplt.show()\n\n# fig.savefig(\"gb_clf_sk_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n/tmp/ipykernel_127927/2577970660.py:10: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  tree_img = imread(graph.render())\n\n\n\n\n\n위처럼 그려지는 \\(h_1(x)\\) 대해서 이전에 했던것처럼 각 노드별로 결정되는 숫자값을 조사해보도록 합시다. 루트 노드와 깊이 1에서 첫번째 노드에 대한 squared_error, value를 계산해보겠습니다.\n\n# 첫번째 회귀트리 h_1(x)이 타겟으로 삼을 확률의 잔차\n# 잔차는 타겟 y와 h_0(x)가 출력한 값을 확률료 바꾼 값과의 차이\n# 첫단계에서 그값은 y의 평균이 된다.\nresidual_1 = y - y_mean\n\n# 루트 노드 squared_error, value\n# value는 노드에 모인 샘플들의 평균\nroot_mean = residual_1.mean()\n\nroot_squared_error = np.mean((residual_1 - root_mean)**2)\nprint(f\"root squared_error={root_squared_error:.3f}\")\nprint(f\"root value={root_mean:.3f}\")\n\n# 깊이1 첫번째 노드\ndepth1_first_node_idx = X_clf2[:,1]&lt;=0.06\ndepth1_first_node_target = residual_1[depth1_first_node_idx]\n\n# squared_error은 평균과 각 타겟 사이의 mse\nprint(\"첫번째 트리의 depth1 first node 결과\")\nprint(f\"squared_error: {np.mean((depth1_first_node_target - depth1_first_node_target.mean())**2):.3f}\") \nprint(f\"value: {depth1_first_node_target.mean():.3f}\") \n\nroot squared_error=0.250\nroot value=0.000\n첫번째 트리의 depth1 first node 결과\nsquared_error: 0.055\nvalue: 0.441\n\n\n결과를 보면 value는 해당 노드에 모인 샘플의 타겟 평균, squared_error은 타겟 평균과의 제곱오차 평균임을 알 수 있습니다.\n이제 두번째 단계로 리프노드를 조사해봅시다. 이전과 조사방식은 똑같습니다.\n\n# 깊이2 첫번째 노드\ndepth2_first_node_idx = X_clf2[:,0]&lt;=-0.418\n\ndepth2_first_node_target = residual_1[depth1_first_node_idx & depth2_first_node_idx]\ndepth2_first_node_sample = X_clf2[depth1_first_node_idx & depth2_first_node_idx]\n\n# squared_error은 평균과 각 타겟 사이의 mse\nprint(\"첫번째 트리의 depth2 first node 결과\")\nprint(f\"squared_error: {np.mean((depth2_first_node_target - depth2_first_node_target.mean())**2):.3f}\") \nprint(f\"value: {depth2_first_node_target.mean():.3f}\") \n\n첫번째 트리의 depth2 first node 결과\nsquared_error: 0.000\nvalue: -0.500\n\n\n앗! 이번에는 squared_error은 사이킷런과 동일하게 계산되지만 value는 동일하지 않습니다.\n이것으로 분류 문제에서 사이킷런이 사용하는 개별 학습기 회귀트리에서 출력하는 값은 샘플 타겟의 평균이 아니란 것을 알 수 있습니다.\n\nmy_h_1 = gb_clf2_tree_dev['learners'][1]\n\n# h_1을 시각화\nfig = plt.figure(figsize=(10,5), dpi=100)\nax = plt.axes()\n# 트리 그림 그리기\ndot_data = export_graphviz(my_h_1, out_file=None, \n                           max_depth=3, precision=3)\ngraph = graphviz.Source(dot_data, format=\"png\")\ntree_img = imread(graph.render())\nax.imshow(tree_img)\nax.axis('off')\nplt.show()\n\n# fig.savefig(\"gb_clf_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n/tmp/ipykernel_127927/3643183575.py:10: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  tree_img = imread(graph.render())\n\n\n\n\n\n\n\n\nTreeBoost\n앞서 knn과 결정트리를 사용해서 약한 학습기가 특정 모델로 제한될 필요가 없다는 특징을 알아봤습니다. 그런데 결정트리를 사용한다면 매 반복에서 구해지는 약한 학습기들은 출력값으로 트리의 리프노드 숫자만큼의 출력값만 가지게 됩니다. 더 쉽게 이해하기 위해 다음 그림을 봅시다.\n\n어떤 반복단계 \\(m\\)에서 결정트리로 만들어진 약한 학습기 \\(h_m(\\mathbf{x})\\)가 위 그림과 같다고 할때 이 함수는 출력값을 영역 네 개로 나누게 됩니다. 이 영역을 \\(R_{jm}\\)이라고 표시합시다. \\(m\\)번째 트리의 \\(j\\)번째 영역이란 의미입니다. 전체 영역 개수는 \\(J_m\\)으로 표시합니다. 그럼 이 함수는 어떤 입력이 들어와도 출력값은 각 영역에서 계산되는 출력값 \\(b_{jm}\\)만 출력하게 됩니다. 이를 식으로 표시하면\n\\[\nh\\left(\\mathbf{x}; \\{b_m, R_j\\}_{j=1}^{J_m} \\right) = \\sum_{j=1}^{J_m} b_{jm} 1(\\mathbf{x} \\in R_{jm}) \\tag{15}\n\\]\n처럼 쓸 수 있습니다. 위 식에서 \\(1()\\)은 입력되는 \\(\\mathbf{x}\\)가 \\(\\mathbf{x} \\in R_{jm}\\)을 만족하면 1 아니면 0인 identity 함수입니다.\n이 표현법으로 아래 모델의 업데이트 식\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m h(\\mathbf{x};\\mathbf{a}_m)\n\\]\n을 다음처럼 바꿔 쓸 수 있습니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m \\sum_{j=1}^{J_m} b_{jm} 1(\\mathbf{x} \\in R_{jm}) \\tag{16}\n\\]\n식(16)에서 \\(b_{jm}\\)은 다음처럼 리프노드에 모인 샘플들의 타겟값 평균이 됩니다.\n\\[\nb_{jm} = ave_{\\mathbf{x} \\in R_{jm}} \\tilde{y}_i\n\\]\n\\(m\\)번째 약한 학습기가 피팅하는 타겟은 pseudo-residual \\(\\tilde{y}_i\\)이라는 것을 유념해야 합니다.\n약한 학습기의 리프노트 출력이 해당 노드에 모인 샘플들의 타겟 평균값이 되는, 즉 위 식에서 \\(b_{jm}\\)을 출력하는 트리는 현재 우리가 직접 코딩한 그래디언트 부스팅이 사용하고 있는 트리입니다. 그런데 사이킷런에서 사용하는 개별 트리는 이 \\(b_{jm}\\) 값을 출력하지 않았음을 상기합시다. 그래서 지금 이 이야기를 하고 있는 것입니다. 우리가 계산한 \\(h_1(\\mathbf{x})\\)의 첫번째 리프노드 출력값은 -0.5였는데 실제 사이킷런에서 출력한 값은 -2.0이였습니다.\n식(16)을 더 이해하기 편하게 그림으로 그려봅시다. 첫번째 약학 학습기가 만들어낸 잔차를 학습하는 두번째 학습기를 실제로 그려보겠습니다.\n먼저 첫번째 약한 학습기로 부터 잔차를 구합니다.\n\n# r_im = y - proba(F)\nresidual_for_h1 = y_clf2 - proba(gb_clf2_tree_dev['learners'][0])\nresidual_for_h1\n\narray([ 0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5, -0.5, -0.5,  0.5,\n       -0.5,  0.5,  0.5, -0.5,  0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5,\n       -0.5, -0.5,  0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5, -0.5, -0.5,\n       -0.5,  0.5, -0.5,  0.5,  0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n        0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5,\n        0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5, -0.5,  0.5,\n       -0.5, -0.5, -0.5, -0.5, -0.5,  0.5,  0.5, -0.5,  0.5,  0.5, -0.5,\n        0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n       -0.5,  0.5, -0.5, -0.5,  0.5, -0.5, -0.5, -0.5,  0.5,  0.5, -0.5,\n       -0.5])\n\n\n이렇게 구한 잔차를 데이터로 하고 그 위에 두번째 약한 학습기를 실제로 그려보면\n\n# colab 노트북을 로컬런타임에 연결했다면 이 셀을 실행 \n# 호스팅 런타임이면 실행안해도 됨\n# https://plotly.com/python/renderers/#setting-the-default-renderer\nimport plotly.io\n\n# local에서 그냥 실행하는 상황이면 notebook, jupyterlab 으로\nplotly.io.renderers.default = \"colab\" \n\n\n# https://plotly.com/python/sliders/\n# https://stackoverflow.com/questions/62397485/plotly-relabelling-animation-tick-marks-on-the-slider-bar\n\nX = X_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 20)\nyy = np.linspace(y_min, y_max, 20)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred = my_h_1.predict(X_grid)\npred = pred.reshape(X1.shape)\n\nlayout = go.Layout(\n    title='Gradient H_1(x)',\n    width=600, height=600,\n    margin=dict(l=0, r=0, b=0, t=25),\n    scene = dict(\n        xaxis = dict(title='x1', range=[x_min, x_max],),\n        yaxis = dict(title='x2', range=[y_min, y_max],),\n        zaxis = dict(title='residual', range=[-2,2],),\n        aspectratio=dict(x=1, y=1, z=1)\n    )\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\nfig.add_trace(\n    go.Scatter3d(\n        x=X[:,0], y=X[:,1], z=residual_for_h1, \n        mode='markers',\n        marker=dict(\n            symbol='circle', size=3, color='#E64A45',\n            line=dict(color='#000000', width=0.5),\n            opacity=1.0\n        ),\n        name='Data', visible=True\n    )\n)\n\n# Add traces, one for each slider step\nrho_m = np.linspace(0, 3, 52)\nfor rho in rho_m:\n    fig.add_trace(\n        go.Surface(\n            x=X_grid[:,0].reshape(X1.shape), y=X_grid[:,1].reshape(X1.shape), \n            z=pred * rho, \n            showscale=False,  opacity=1.,\n            contours=dict(\n                x=dict(show=True, highlight=True),\n                y=dict(show=True,  highlight=True),\n                z=dict(show=True,  highlight=True),\n            ),visible=False\n        )\n    )\n\nfig.data[25].visible = True\n\n# Create and add slider\nsteps = []\n\nfor i in range(1, len(fig.data)):\n    step = dict(\n        method=\"update\",\n        args=[\n            {\"visible\": [True] + [False] * (len(fig.data)-1)},\n            # {\"title\": \"rho: \" + f\"{rho_m[i-1]:.2f}\"}\n        ],  # layout attribute\n        label=f\"{rho_m[i-1]:.2f}\"\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [\n    dict(\n        active=17,\n        currentvalue={\"prefix\": \"rho: \"},\n        pad={\"l\":10, \"t\": 50, \"r\":10, \"b\":10},\n        steps=steps\n    )\n]\n\nfig.update_layout(sliders=sliders)\n\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n[노트] 위 셀의 실행 결과가 블로그에서 표시되지 않는 문제가 있는데 아래 링크를 클릭해서 코랩에서 직접 실행하면 약한 학습기의 실제 모습을 입체로 확인할 수 있습니다. \n\n위 처럼 그러집니다. 위 그림은 식(16)에서 \\(\\sum_{j=1}^{J_m} b_{jm} 1(\\mathbf{x} \\in R_{jm})\\)에 해당하는 부분입니다. 이 그래디언트로 선탐색을 해서 \\(\\rho_m\\)을 구할텐데 위 그래프에서 rho_m 슬라이드 바를 움직면 스탭사이즈 rho_m에 따른 변화가 그려집니다.\n위 그래프에서 볼 수 있는것 처럼 두번째 약한 학습기는 전체 영역을 네개로 나누고 각 영역에서 동일한 값을 출력합니다. 이 출력값을 \\(\\gamma_{jm}\\)이라 두면 식(16)은 다음처럼 쓸 수 있습니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) +  \\sum_{j=1}^{J_m} \\gamma_{jm} 1(\\mathbf{x} \\in R_{jm}) \\tag{17}\n\\]\n여기서 \\(\\gamma_{jm} = \\rho_{m} b_{jm}\\)입니다.\n두번째 약한 학습기가 출력값을 식(17)처럼 출력한다는 말은 각 리프노트에 모인 샘플의 타겟값들을 평균한 값에 선탐색으로 찾아낸 스탭사이즈 \\(\\rho_m\\)을 일괄적으로 곱해서 출력한다는 것입니다. 그런데 각 출력값 \\(\\gamma_{jm}\\)은 완전히 분리된 값이므로 이를 각각 분리된 네개의 개별적인 기저함수separate basis function에 의한 출력값을 더한다고 생각할 수 있습니다. 예를 들어 \\(b_{1m}\\)이 변하면 \\(b_{2m}\\)이 그 영향을 받아서 같이 변하지 않는다는 의미입니다. 그 개별 기저함수의 출력값은 결국 다음처럼 손실함수 \\(L\\)을 최소화 시키는 값이 \\(J\\)개가 될 것입니다.\n\\[\n\\{\\gamma_{jm}\\}_{j=1}^J = \\underset{\\{\\gamma_j\\}_{i=1}^J}{\\text{argmin}} \\sum_{i=1}^N L \\left( y_i, F_{m-1}(\\mathbf{x}_i) + \\sum_{j=1}^J \\gamma_j 1(\\mathbf{x} \\in R_{jm})\\right)\n\\]\n그런데 각 영역에서 함수값은 완전히 분리disjoint되어 있기 때문에 각 영역별로 최적화를 따로 수행해도 됩니다.\n\\[\n\\gamma_{jm} = \\underset{\\gamma}{\\text{argmin}} \\sum_{\\mathbf{x}_i \\in R_{jm}} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma) \\tag{18}\n\\]\n\nNewton’s Method\n위 식(18)을 풀기 위해 최적화 수법 중 2계법인 뉴턴메소드를 사용합니다. 테일러 시리즈 2차 근사를 하고 근사된 식을 \\(\\gamma\\)로 미분하여 0으로 두고 방정식을 풉니다.\n\\[\n\\sum_{\\mathbf{x}_i \\in R_{jm}} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma) = \\sum_{\\mathbf{x}_i \\in R_{jm}} \\left(  L(y_i, F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial L}{\\partial F} +  \\frac{1}{2} \\gamma^2 \\frac{\\partial^2 L}{\\partial F^2} + O(\\gamma^3)\\right)\n\\]\n이제 \\(\\gamma\\)에 대해 미분하고 \\(\\gamma\\)의 2차항까지 남기면\n\\[\n\\begin{aligned}\n\\sum_{\\mathbf{x}_i \\in R_{jm}} \\frac{d}{d \\gamma} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma) &=  \\sum_{\\mathbf{x}_i \\in R_{jm}} \\frac{d}{d \\gamma} \\left(  L(y_i, F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial L}{\\partial F} +  \\frac{1}{2} \\gamma^2 \\frac{\\partial^2 L}{\\partial F^2} + O(\\gamma^3)\\right) \\\\\n&\\approx \\sum_{\\mathbf{x}_i \\in R_{jm}} \\left( \\frac{\\partial }{\\partial F} L(y_i + F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial^2}{\\partial^2 F} L(y_i + F_{m-1}(\\mathbf{x}_i)) \\right)\n\\end{aligned}\n\\]\n결과를 0으로 놓고 정리합니다.\n\\[\n\\sum_{\\mathbf{x}_i \\in R_{jm}} \\left( \\frac{\\partial }{\\partial F} L(y_i + F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial^2}{\\partial^2 F} L(y_i + F_{m-1}(\\mathbf{x}_i)) \\right) = 0\n\\]\n시그마 기호를 풀고 이항하여 정리하면\n\\[\n\\gamma = \\frac{\\sum_{\\mathbf{x}_i \\in R_{jm}} -\\frac{\\partial}{\\partial F} L(\\cdot) }{\\sum_{\\mathbf{x}_i \\in R_{jm}} \\frac{\\partial^2}{\\partial F^2} L(\\cdot) }\n\\]\n위 식에서 식([4])에 의해 분자는 다음과 같습니다.\n\\[\n-\\frac{\\partial }{\\partial F} L(y_i, F_{m-1}(\\mathbf{x}_i)) = y_i - p_{m-1}(\\mathbf{x}_i)\n\\]\n위 식에서 \\(p_{m-1}(\\mathbf{x}_i)\\)는 \\(F_{m-1}(\\mathbf{x}_i)\\)가 출력한 로짓을 확률로 바꾼 값입니다.\n이 결과를 이용해서 분모를 다음처럼 정리할 수 있습니다.\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial F^2} L(y_i, F_{m-1}(\\mathbf{x}_i)) &= \\frac{\\partial}{\\partial F} -y_i + p_{m-1}(\\mathbf{x}_i) \\\\\n&= \\frac{\\partial}{\\partial F} \\left( -y_i + \\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\right) \\\\\n&=  \\frac{\\partial}{\\partial F} \\left( -y_i + \\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^{-1} e^{F_{m-1}(\\mathbf{x}_i)} \\right) \\\\\n&= -\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^{-2} e^{2F_{m-1}(\\mathbf{x}_i)} + \\left( 1+e^{F_{m-1}(\\mathbf{x}_i)} \\right)^{-1} e^{F_{m-1}(\\mathbf{x}_i)} \\\\\n&= \\frac{-e^{2F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2} + \\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\\\\n&= \\frac{-e^{2F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2} + \\frac{e^{F_{m-1}(\\mathbf{x}_i)}+e^{2F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2} \\\\\n&= \\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2}=\\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\times \\frac{1}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\\\[5pt]\n&= p_{m-1}(\\mathbf{x}_i) \\left( 1- p_{m-1}(\\mathbf{x}_i) \\right)\n\\end{aligned}\n\\]\n정리한 분자 분모를 원래 식에 대입하면 다음과 같은 결과를 얻을 수 있습니다.\n\\[\n\\gamma_{jm} = \\frac{\\sum_{\\mathbf{x}_i\\in R_{jm}} y_i - p_{m-1}(\\mathbf{x}_i) }{\\sum_{\\mathbf{x}_i\\in R_{jm}} p_{m-1}(\\mathbf{x}_i)(1-p_{m-1}(\\mathbf{x}_i))} \\tag{[5]}\n\\]\n위 식의 의미는 \\(m\\)번째 결정 트리 \\(h_m(\\mathbf{x})\\)는 \\(J_m\\)개의 출력을 출력하도록 학습되는데 각 출력값은 최종 리프 노드에 속하는 샘플 \\(\\mathbf{x}_i \\in R_{jm}\\)들을 사용해서 계산될 수 있다는 것입니다.\n아래 그림은 각 영역별로 최적화를 거쳐 바로 출력값을 구한 두번째 약한학습기의 모습입니다.\n\nX = X_clf2\n\npred = h_1.predict(X_grid)\npred = pred.reshape(X1.shape)\n\ndata = [\n    go.Scatter3d(x=X[:,0], y=X[:,1], z=residual_for_h1, mode='markers',\n                marker=dict(\n                    symbol='circle', size=3, color='#E64A45',\n                    line=dict(color='#000000', width=0.5),\n                    opacity=1.0\n                ),\n                name='Data'\n    ),\n    go.Surface(x=X_grid[:,0].reshape(X1.shape), y=X_grid[:,1].reshape(X1.shape), \n                z=pred, \n                showscale=False,  opacity=1.,\n                contours=dict(\n                x=dict(show=True, highlight=True),\n                y=dict(show=True,  highlight=True),\n                z=dict(show=True,  highlight=True),\n            ),\n    ),\n]\n\nlayout = go.Layout(\n    title='Gamma_r1',\n    width=500, height=500,\n    margin=dict(l=0, r=0, b=0, t=25),\n    scene = dict(\n        xaxis = dict(title='x1', range=[x_min, x_max],),\n        yaxis = dict(title='x2', range=[y_min, y_max],),\n        zaxis = dict(title='residual', range=[-2,2],),\n        aspectratio=dict(x=1, y=1, z=1)\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n[노트] 위 셀의 실행 결과가 블로그에서 표시되지 않는 문제가 있는데 아래 링크를 클릭해서 코랩에서 직접 실행하면 약한 학습기의 실제 모습을 입체로 확인할 수 있습니다. \n\n이렇게 약한 학습기로 결정트리를 사용할 때 트리의 리프노드 출력값을 조절하는 방식을 TreeBoost라고 합니다.\nTreeBoost 방식을 사용하면 약한 학습기를 학습시키고 난후 모델을 업데이트 할 때 선탐색을 하지 않기 때문에 훨씬 빠르게 모델을 적합시킬 수 있습니다.\n지금까지 이야기한 TreeBoost를 구현하기 위해서는 보통의 DecisionTreeRegressor로 부터 최종 출력값을 TreeBoost에 맞도록 계산하는 추가 과정이 필요합니다. \\(h_m(\\mathbf{x})\\)의 출력을 적절히 만들어내는 과정은 다음 절차로 코딩할 수 있습니다.\n\n학습된 \\(h_m(\\mathbf{x})\\)을 이용하여 입력된 샘플들이 몇번 리프로 가는지 알아낸다. 즉 다음을 판단한다 \\(\\mathbf{x}_i \\in R_{jm}\\)\n1에서 얻어진 샘플들이 도착하는 리프노드 인덱스를 유일하게 만들어 리프노드 인덱스 집합을 얻는다.\n2에서 만든 리프노드 인덱스를 순회하면서 각 리프노드에 모이는 샘플들의 타겟(확률의 잔차)를 구하고 위 식([5])로 리프노드의 출력값을 계산한다.\n\n위 과정을 \\(h_1(\\mathbf{x})\\)에 대해서 시험적으로 코딩해보면 아래와 같습니다.\n\n# 첫번째 회귀트리 h_1(x)이 타겟으로 삼을 확률의 잔차\n# 잔차는 h_0(x)가 출력한 타겟의 평균과의 차이가 된다.\nresidual_1 = y - y_mean\nprint(residual_1.shape)\n# 첫번째 회귀트리 h_1(x)이 타겟으로 삼을 확률의 잔차를 \n# 만들어낸 이전 트리 h_0(x)의 예측 확률\nprev_proba = y_mean\n\n####################################################\n# Friedman의 \"TreeBoost\"에서 hm 출력 만들어 내기\n####################################################\n# 1. 각 샘플이 몇번 리프로 가는지 알아낸다.\nleaf_idx_X = h_1.apply(X_clf2)\nprint(\"샘플들이 몇번 노드로 가는지 알아냄\")\nprint(leaf_idx_X)\n\n# 2. 얻어진 샘플에 대한 리프노드 인덱스를 유일하게 만들어 \n#    리프노드 인덱스 집합을 얻는다.\nRm = sorted(set(leaf_idx_X))\nprint(\"\\n샘플들이 도착하는 리프노드 인덱스\")\nprint(Rm)\n\n# 3. 2에서 만든 리프노드 인덱스를 순회하면서 \n#    각 리프노드의 출력값을 계산한다.\n# 트리의 전체 노드 수만큼 자리를 만든다.\nhm_output = np.zeros(max(Rm)+1)\nfor j in Rm:\n    residual = residual_1[leaf_idx_X==j]\n    # 첫번째 트리의 예측확률은 값이 한개밖에 없으니니까 분모 sum()은 그냥\n    # 해당 샘플 개수를 곱한다. eq([5])\n    hm_output[j] =  residual.sum() / (residual.shape[0]*(prev_proba*(1-prev_proba)))\n\nwith np.printoptions(precision=3):\n    print(\"\\n계산된 리프노드의 출력\")\n    print(hm_output)\n\n    # 4. 1에서 얻은 결과 인덱스를 3에서 얻은 리프노드\n    #    결과에서 조회하여 최종 출력을 만든다.\n    print(\"\\n샘플들에 대한 출력\")\n    print(hm_output[leaf_idx_X])\n\n(100,)\n샘플들이 몇번 노드로 가는지 알아냄\n[6 3 5 5 6 6 6 5 5 5 3 5 3 6 5 3 3 5 5 2 3 5 5 5 3 3 5 3 5 5 5 5 5 5 6 5 3\n 3 5 5 5 5 5 5 6 3 3 3 3 5 3 3 3 3 5 3 3 6 5 5 5 5 3 3 2 3 5 5 5 5 5 5 6 5\n 3 5 5 3 3 5 3 5 3 5 5 5 5 5 5 6 5 5 3 5 5 5 3 3 6 5]\n\n샘플들이 도착하는 리프노드 인덱스\n[2, 3, 5, 6]\n\n계산된 리프노드의 출력\n[ 0.     0.    -2.     2.     0.    -1.418  1.636]\n\n샘플들에 대한 출력\n[ 1.636  2.    -1.418 -1.418  1.636  1.636  1.636 -1.418 -1.418 -1.418\n  2.    -1.418  2.     1.636 -1.418  2.     2.    -1.418 -1.418 -2.\n  2.    -1.418 -1.418 -1.418  2.     2.    -1.418  2.    -1.418 -1.418\n -1.418 -1.418 -1.418 -1.418  1.636 -1.418  2.     2.    -1.418 -1.418\n -1.418 -1.418 -1.418 -1.418  1.636  2.     2.     2.     2.    -1.418\n  2.     2.     2.     2.    -1.418  2.     2.     1.636 -1.418 -1.418\n -1.418 -1.418  2.     2.    -2.     2.    -1.418 -1.418 -1.418 -1.418\n -1.418 -1.418  1.636 -1.418  2.    -1.418 -1.418  2.     2.    -1.418\n  2.    -1.418  2.    -1.418 -1.418 -1.418 -1.418 -1.418 -1.418  1.636\n -1.418 -1.418  2.    -1.418 -1.418 -1.418  2.     2.     1.636 -1.418]\n\n\n첫번째 학습기 \\(h_1(x)\\)에 대해서 샘플 100개에 대한 출력값을 성공적으로 구할 수 있었습니다. 마지막에 출력된 값 100개와 위에서 그린 \\(h_1(x)\\)의 리프 노드 출력값을 비교해보세요. 값 100개가 그림에 나타난 value값들로 구성되었음을 알 수 있을 것입니다.\n이제 모든 조각이 완성되었습니다! 이제 우리는 분류 문제에서 각 개별 트리 학습기가 어떤 값을 출력하면 선탐색하지 않고 효율적으로 경사하강을 할 수 있는지 알았습니다.\n이 결과를 앞서 “직접구현 2”에서 작성한 코드에 추가해봅시다.\n\nlogit = lambda p: np.log(p / (1-p))\nproba = lambda z: 1 / (1+np.exp(-z))\n\n# 그래디언트부스팅 직접 만들기\ndef train_tree_boost(X, y, h, M=10, lr=0.1, loss=None):\n    \"\"\"\n    X: train samples (N,D), N: # of samples, D: the dimension of a sample\n    y: target\n    h: weak learner instance, must be a DeicisionTreeRegressor\n    M: # of week learner (except for the first dummy learner)\n    lr: learning rate\n    loss: loss function whose args are predictions and ground truth\n    \"\"\"\n    loss_values = [] # 학습 중 로스를 저장\n    probas = [] # 개별 트리 학습기라 로짓을 출력하면 확률로 바꿔서 여기에 저장\n    r_im_s = [] # 개별 트리 학습기가 만들어낸 확률의 잔차(그래디언트)를 저장\n    Ho = [None, ] # 개별 트리 학습기가 출력할 출력을 저장\n\n    # 위 알고리즘 (1)에서처럼 상수로 첫번째 예측기를 정한다,  m=0\n    # 단 이번에는 평균에 대한 로짓을 함수값으로 가진다.\n    H = [ logit(y.mean()) ] \n    Fm_1 = np.ones(X.shape[0])*H[0]\n\n    # 학습\n    for m in range(1, M+1):# 최초 더미 예측기 제외 M개 트리 \n        # 각 단계의 학습기에서 만들어낸 로스 저장\n        if loss is not None:\n            loss_values.append( loss(y, Fm_1) )\n\n        # 잔차는 다시 확률로 바꿔서 계산\n        probas.append(proba(Fm_1))\n        r_im = y - probas[-1]\n        r_im_s.append(r_im)\n\n        # 구해진 잔차 r_im에 대해서 학습기 hm을 학습\n        h_ = clone(h)\n        \n        H.append( h_.fit(X, r_im) )\n        \n        ################################################################\n        # Freidman TreeBoost\n        # H[m] 학습이 끝났으면 리프노드에서 출력할 출력 구하기\n        # 1. 각 샘플이 몇번 리프로 가는지 알아낸다.\n        leaf_idx_X = H[m].apply(X)\n\n        # 2. 얻어진 샘플에 대한 리프노드 인덱스를 유일하게 만들어 \n        #    리프노드 인덱스 집합을 얻는다.\n        Rm = sorted(set(leaf_idx_X))\n\n        # 3. 2에서 만든 리프노드 인덱스를 순회하면서 \n        #    각 리프노드의 출력값을 계산한다.\n        ho = np.zeros(max(Rm)+1)\n\n        for j in Rm:\n            residual = r_im_s[-1][leaf_idx_X == j]\n            prev_proba = probas[-1][leaf_idx_X == j]\n            ho[j] = residual.sum() / np.sum(prev_proba*(1-prev_proba)) \n\n        # 4. H[m]의 출력을 저장한다.\n        # 이제부터 H[m]의 출력은 H[m].predict(X)로 얻는 것이 아니라\n        # Ho[m][ H[m].apply(X) ] 로 얻으면 된다.\n        Ho.append(ho)\n        ################################################################\n\n        # update\n        Fm_1 += lr*Ho[-1][H[-1].apply(X)]\n\n    # print('return')    \n    return {'learners':H, 'learners_out':Ho, \n            'learning_rate':lr, 'loss_values':loss_values}\n\n\n# 예측하기\ndef predict_tree_boost(X, tree_boost):\n    H = tree_boost['learners']\n    Ho = tree_boost['learners_out']\n    lr = tree_boost['learning_rate']\n\n    F = np.ones(X.shape[0])*H[0]\n\n    # 여기서 계산되는 함수값을 로짓이다.\n    for m in range(1, len(H)):\n        F += lr*Ho[m][H[m].apply(X)]\n\n    # 리턴하기 전에 확률값으로 바꾼다.\n    pred = proba(F)\n\n    return pred\n\n이제 만든 함수를 테스트 해봅시다. 사이킷런과 결과를 비교하기 위해 GradientBoostingClassifier를 실행했을 때와 동일한 조건으로 실행합시다.\nlearning_rate = 1.0\ngbrt = GradientBoostingClassifier(criterion='squared_error', \n                                 learning_rate=learning_rate, \n                                  n_estimators=50, max_depth=2)\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\ntb_clf2_tree_dev = train_tree_boost(X_clf2, y_clf2, h, M=50, \n                              lr=learning_rate, loss=deviance_loss)\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_tb_x_clf2_tree_dev = predict_tree_boost(X_grid, tb_clf2_tree_dev)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_tb_x_clf2_tree_dev &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, dpi=100)\n\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf2_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('sklearn')\n\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('tree boost')\n\nplt.show()\n\npred_tb_x_clf2_tree_dev = Z.copy()\n#\n# fig.savefig(\"clf2_sk_and_tb.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n\nnp.sum( pred_tb_x_clf2_tree_dev != pred_x_clf2_sk )\n\n0\n\n\n사이킷런 결과와 완벽하게 일치합니다!\n이렇게 사이킷런에서 그래디언트 부스팅을 구현한 방식을 완전히 재현했습니다. 몇가지 다른 예를 테스트해보겠습니다.\n\n\n추가 예제\n\ngaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(\n    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=10\n)\nX2, y2 = make_gaussian_quantiles(\n    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=10\n)\n\nX_clf3 = np.concatenate((X1, X2))\ny_clf3 = np.concatenate((y1, -y2 + 1))\n\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.plot(X_clf3[y_clf3==0][:, 0], X_clf3[y_clf3==0][:, 1], 'o', color='C1')\nax.plot(X_clf3[y_clf3==1][:, 0], X_clf3[y_clf3==1][:, 1], '^', color='C2')\n\nplt.show()\n\n\n\n\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\ntb_clf3_tree_dev = train_tree_boost(X_clf3, y_clf3, h, M=10, \n                                   lr=learning_rate, loss=deviance_loss)\n\n\nlearning_rate = 1.0\ngb_clf3_sk = GradientBoostingClassifier(criterion='squared_error', \n                                        learning_rate=learning_rate, \n                                        n_estimators=10, max_depth=2)\ngb_clf3_sk.fit(X_clf3, y_clf3)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)\n\n\n\n# 그림으로 확인\nX = X_clf3\ny = y_clf3\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred = predict_tree_boost(X_grid, tb_clf3_tree_dev)\npred_tb_x_clf3_tree_dev = np.zeros(X_grid.shape[0])\npred_tb_x_clf3_tree_dev[pred &gt;= 0.5] = 1.\npred_tb_x_clf3_tree_dev = pred_tb_x_clf3_tree_dev.reshape(X1.shape)\n\npred_x_clf3_sk = gb_clf3_sk.predict(X_grid)\npred_x_clf3_sk = pred_x_clf3_sk.reshape(X1.shape)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, dpi=100)\n\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf3_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('sklearn')\n\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, pred_tb_x_clf3_tree_dev, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('tree boost')\n\nplt.show()\n\n\n\n\n\n\nnp.sum( pred_tb_x_clf3_tree_dev != pred_x_clf3_sk )\n\n0\n\n\n\n\nIris dataset\n\n# 아이리스에 대해서\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\nX_iris = iris['data']\n# y_iris = iris['target']\ny_iris = (iris[\"target\"] == 2).astype(int)\n\n\n# y_iris[130] = 0\n# np.log( y_iris.mean() / (1-y_iris.mean()) )\n\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\ntb_iris = train_tree_boost(X_iris, y_iris, h, M=10, \n                              lr=learning_rate, loss=deviance_loss)\n\ngb_iris_sk = GradientBoostingClassifier(criterion='squared_error', \n                                 learning_rate=learning_rate, \n                                  n_estimators=10, max_depth=2)\ngb_iris_sk.fit(X_iris, y_iris)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)\n\n\n\npred_tb_iris_proba = predict_tree_boost(X_iris, tb_iris)\npred_iris_sk_proba = gb_iris_sk.predict_proba(X_iris)\n\n\n# 소수 6자리에서 반올림해서 비교\nnp.sum( \n    (\n        np.round(pred_tb_iris_proba, 6) != np.round(pred_iris_sk_proba[:,1],6)\n    ).astype(int)  \n)\n\n0\n\n\n\n\n\n규제, shrinkage\n마지막으로 shrinkage라고 하는 규제 방법에 대해서 이야기하고 글을 마무리 하겠습니다. 앞서 개별 트리 학습기의 리프 노드 출력값을 테일러 시리즈 2차 근사를 통해 다음처럼 직접 구하는 방법을 알아봤습니다.\n\\[\nF_m(x) = F_{m-1}(\\mathbf{x}) + \\sum_{j=1}^{J_m} \\gamma_{jm} \\mathbb{1}(\\mathbf{x} \\in R_{jm}), \\qquad \\gamma_{jm} = \\underset{\\gamma}{\\text{argmin}} \\sum_{\\mathbf{x}_i \\in R_{jm}} L \\left(y_i, F_{m-1}(\\mathbf{x}_i)+\\gamma \\right)\n\\]\n이렇게 구해진 값은 2차 근사 형태로 선탐색까지 마친 결과이므로 더 이상 스탭사이즈를 곱하지 않고 그 값을 바로 \\(F_{m-1}(\\mathbf{x})\\)에 더하게 됩니다. 이런 이유로 앞선 실험에서 학습률을 1로 두었던 것입니다. (그래디언트에 곱하는 숫자를 1로 두어 아무것도 곱하지 않는것과 같다는 의미)\n하지만 최종 구현에서는 \\(F_{m-1}(\\mathbf{x})\\)에 더해지는 \\(\\gamma_{jm}\\)의 기여 정도를 조정하기 위해 마치 학습률처럼 1보다 작은 상수를 곱하게 됩니다. 따라서 위 업데이트 룰은 다음처럼 됩니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\nu \\sum_{j=1}^{J_m} \\gamma_{jm} \\mathbb{1}(\\mathbf{x} \\in R_{jm}), \\qquad 0 &lt; \\nu \\le 1\n\\]\n이렇게 개별 학습기의 출력값을 적당히 축소시키면 \\(\\nu=1\\)인 경우 보다 모델의 일반화 성능이 크게 증가함을 확인할 수 있습니다.\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 0.1\ntb_clf2_tree_dev_nu = train_tree_boost(X_clf2, y_clf2, h, M=50, \n                              lr=learning_rate, loss=deviance_loss)\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred = predict_tree_boost(X_grid, tb_clf2_tree_dev_nu)\nZ = np.zeros(X_grid.shape[0])\nZ[pred &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig, ax = plt.subplots(figsize=(10,4), nrows=1, ncols=2, dpi=100)\n\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_tb_x_clf2_tree_dev, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_title(r\"$\\nu=1.0$\")\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\n\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_title(r\"$\\nu=$\"+f\"{learning_rate}\")\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\n\nplt.show()\n\n# fig.savefig(\"shrinkage.png\", dpi=image_dpi, bbox_inches='tight')"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#마무리",
    "href": "posts/gradientboost/gradient_boosting.html#마무리",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "마무리",
    "text": "마무리\n이렇게 좀 길었지만 그래디언트 부스팅에 대해서 자세히 알아봤습니다. 이상의 내용을 잘 이해하고 있으면 최근 가장 각광받고 있는 XGBoost라는 알고리즘을 이해하는 큰 도움이 됩니다. 왜냐하면 XGBoost도 기본적인 논리의 전개는 지금까지 알아본 내용과 모두 같기 때문입니다. 세부적으로 여러 다른 점이 있기는 하지만 핵심적인 차이점은 약한 학습기를 학습시키는 방식으로 목적함수에 규제항을 적용하고 이를 테일러 시리즈 2차 근사하여 뉴턴 메소드를 사용하는 부분입니다.*) 그런데 이 방식도 사실 우리 글에서 알아본 TreeBoost에서 리프노드 값을 결정하는 것과 동일한 방식입니다.\n제 블로그의 글이 늘 그렇지만 라이브러리를 가져다 사용하는 입장에서는 큰 도움이 되지 않을지 모르겠습니다. 하지만 원리를 파악하고 싶어하는 분들께는 꼭 도움이 되디라 믿으며 글을 마치도록 하겠습니다.\n\n*) 물론 가중 분위수 스케치weighted quantile sketch나 하드웨어 특성을 활용해서 효율을 높이는 부분은 너무 세부적이라 이해하기 힘든 내용들이긴 합니다. 하지만 개별 트리를 학습하는데 쓰이는 손실이나 분기점을 찾기 위한 랜덤서치 방식은 이 글을 읽고 나면 충분히 이해할 수 있습니다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "트랜스포머가 딥러닝 세상을 지배한 지금 과거 CNN, RNN을 모르고 딥러닝을 안다고 할 수 없듯이 이제는 트랜스포머를 이해하지 못하고 딥러닝을 공부한다고 말할 수 없는 시대가 되었다.\n트랜스포머가 초기 타겟한 작업이 번역이기 때문에 트랜스포머를 설명하는 글에서 단골로 등장하는 예제가 번역기 예제다. 주로 영어-독일어, 영어-스페인어 예제가 많다. 하지만 쉽게 접할 수 있는 알파벳권 언어 사이의 번역기 예제에 비해 영어-한국어 데이터를 사용해서 번역기를 학습시키는 예제는 이상하리만큼 찾아보기 힘들었다. 왜 그런지 이유는 잘 모르겠지만 어쨌든 거의 없다. 그래서 영어-한국어 문장쌍이 들어있는 원시데이터를 사용해 T5 모델로 영어-한국어 번역기를 데모 수준 정도로 학습하는 예제를 만들어 블로그에 포스팅하면 많은 사람들에게 도움이 되지 않을까 해서 이 글을 적게 되었다.\n이 글에서는 트랜스포머에 대한 기초적인 내용은 다루지 않고 오직 데이터를 어떻게 준비하고, 어떻게 데이터를 모델에 입력하여 학습을 시키고, 마지막으로 어떻게 영어로 부터 한국어 번역 문장을 출력시키는가 하는 것에만 초점을 맞추었다. 그리고 코드를 복잡하게 만드는 그 어떤 테크닉도 사용하지 않는다. 오로지 가장 간단하게 한국어 번역기를 구축하는데만 집중할 것이다. 사실 이 글의 대부분 내용은 허깅페이스 도움말에 있는 것을 정리한것에 지나지 않는다. 하지만 입문자나 이제 막 트랜스포머를 이용해서 한국어 번역기를 만들고자 하는 사람들은 허깅페이스 도움말을 보고 이 내용을 모두 정리하기 쉽지 않은 것이 사실이어서 이글이 꽤 도움이 되리라 생각한다.\n\n\n\n\n시작하기전에 필요한 라이브러리를 설치한다. 본인 컴퓨터에 이미 관련 라이브러리가 설치되어 있다면 설치하지 않아도 된다.\n먼저 허깅페이스의 트랜스포머스 라이브러리를 설치한다.\n\n!pip install transformers\n\n그 다음은 데이터 셋을 다운받는기 위해 다음 명령을 실행해서 허깅페이스 Datasets 라이브러리를 설치한다.\n\n!pip install datasets\n\n그리고 T5 모델의 토크나이저가 sentencepiece를 사용하므로 다음을 실행해서 설치한다.\n\n!pip install sentencepiece\n\n또 모델 평가를 위해 허깅페이스 evaluate 라이브러리와 BLEU 점수를 계산하기위해 sacrebleu를 설치한다.\n\n!pip install evaluate\n\n\n!pip install sacrebleu\n\n\n\n\n모두 설치가 완료되었다면 데이터 셋을 다운받아야 한다. 먼저 허깅페이스 사이트에 접속해서 상단 메뉴에 Datasets를 클릭하고 아래처럼 검색 조건을 맞추면\n\n좌측 작은 메뉴에서 Languages를 선택한다.\nLanguages 하단에 보이는 여러 언어중에 Korean을 선택한다.\n다시 우측 검색 필터 창에 en을 적는다.\n\n데이터 셋 네 개가 보이는데 이 중에서 bongsoo/news_talk_en_ko를 사용하도록 하겠다.\nbongsoo/news_talk_en_ko를 클릭해서 나오는 화면에서 Files and Versions를 클릭하면 tsv 파일이 보이는데 이 파일에는 영어 문장과 한국어 문장이 한줄에 탭 문자로 구분되어 적혀있다. 로컬 디스크이 이 파일을 다운받고 파일을 읽어보면 다음처럼 확인된다.\n\n[노트] 로컬 또는 코랩 런타임에 파일을 다운 받지 않았다면 굳이 다운받을 필요없고 이 셀은 스킵하자. 그냥 데이터 파일 한줄에 영어 문장과 짝이 되는 한국어 문장이 탭문자로 구분되어 있다는 것만 알면 된다. 실제 데이터를 가져올 때는 허깅페이스를 통해 다운 받게 된다.)\n\n\n!head -5 news_talk_en_ko_train_130000.tsv\n\nSkinner's reward is mostly eye-watering.    스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\nEven some problems can be predicted.    심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\nOnly God will exactly know why. 오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\nBusinesses should not overlook China's dispute. 중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\nSlow-beating songs often float over time.   박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n데이터 파일은 아주 단순한 형태인 것을 알 수 있다. 직접 tsv파일을 다운받아서 사용해도 되나 허깅페이스 허브로 부터 바로 다운받아 사용하는 편이 더 편하다. 다음 명령으로 다운받을 수 있다.\n\n# 데이터 셋을 다운받을 함수를 임포트 한다.\nfrom datasets import load_dataset\n\n\n# 좀 전에 알아본 체크포인트를 사용해서 데이터를 받아온다.\nen_ko = load_dataset(\"bongsoo/news_talk_en_ko\")\n\nUsing custom data configuration bongsoo--news_talk_en_ko-e7f00bc8f76f18d5\nFound cached dataset csv (/home/metamath/.cache/huggingface/datasets/bongsoo___csv/bongsoo--news_talk_en_ko-e7f00bc8f76f18d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n\n\n\n\n\n이제 데이터 객체를 확인해보면 DatasetDict라는 것을 알 수 있고 안에 train 키만 있는 것이 확인된다.\n\nen_ko\n\nDatasetDict({\n    train: Dataset({\n        features: [\"Skinner's reward is mostly eye-watering.\", '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.'],\n        num_rows: 1299999\n    })\n})\n\n\ntrain키에 Dataset 객체가 하나 있는데 features가 첫번째 데이터로 되어있고 행수는 1299999개인 것을 보아 데이터 파일에 컬럼명이 적혀있는 헤더라인이 없어서 첫줄을 헤더로 읽은것 같다. 첫줄을 데이터로 다시 집어 넣고 컬럼명은 en, ko로 설정하기 위해 데이터 셋을 pandas로 읽어드린다.\n\nimport pandas as pd\n\n\n# 허깅페이스 데이터셋을 판다스 포맷으로 세팅\nen_ko.set_format(type=\"pandas\")\n\n\n# 'train'키의 모든 행을 DataFrame df에 할당\ndf = en_ko[\"train\"][:]\n\n# 잘 담겼는지 확인한다.\ndf.head()\n\n\n\n\n\n\n\n\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n\n\n0\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n1\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n2\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n3\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n4\nI can't even consider uninsured treatments.\n보험 처리가 안 되는 비급여 시술은 엄두도 못 낸다.\n\n\n\n\n\n\n\n예상처럼 첫 줄이 헤더가 되었으니 이를 수정한 DataFrame을 만든다.\n\nexample_0 = list(df.columns)\nexample_0\n\n[\"Skinner's reward is mostly eye-watering.\",\n '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.']\n\n\n적당히 조작해서 컬럼명이 en, ko가 되게 하고 example_0가 첫 행이 되도록 만든다.\n\nexample_0_df = pd.DataFrame({col: [value] for col, value in zip(('en', 'ko'), example_0)})\n\n\ndf.columns = ('en', 'ko')\n\n\nen_ko_df = pd.concat([example_0_df, df],).reset_index(drop=True)\nen_ko_df.head()\n\n\n\n\n\n\n\n\nen\nko\n\n\n\n\n0\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n1\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n2\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n3\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n4\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n\n\n\n\n\n이렇게 데이터 셋을 DataFrame으로 만들었다. 이제 이 en_ko_df로 부터 다시 허깅페이스 데이터 셋을 생성하자.\n\nfrom datasets import Dataset\n\n\ndataset = Dataset.from_pandas(en_ko_df)\n\n\ndataset\n\nDataset({\n    features: ['en', 'ko'],\n    num_rows: 1300000\n})\n\n\n다시 데이터 셋을 확인해보면 features가 제대로 표시되고 샘플 수도 1300000개 인것을 확인할 수 있다.\n이렇게 만들어진 DataFrame으로 부터 데이터 셋이 잘 초기화되는 것을 확인했으니 en_ko_df를 세조각으로 쪼개서 tsv파일로 저장하자.\n\n# 각 데이터 셋의 샘플수를 정한다.\nnum_train = 1200000\nnum_valid = 90000\nnum_test = 10000\n\n설정된 크기만큼 DataFrame을 자른다.\n\nen_ko_df_train = en_ko_df.iloc[:num_train]\n\n\nen_ko_df_valid = en_ko_df.iloc[num_train:num_train+num_valid]\n\n\nen_ko_df_test = en_ko_df.iloc[-num_test:]\n\n다시 tsv파일로 저장한다.\n\nen_ko_df_train.to_csv(\"train.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_valid.to_csv(\"valid.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_test.to_csv(\"test.tsv\", sep='\\t', index=False)\n\n이렇게 tsv파일 세개로 데이터를 정리했다. 이제 필요할때 이 파일을 읽어 허깅페이스 데이터셋을 만들 수 있다.\n아래처럼 스플릿을 정의한 사전을 load_dataset에 넘기면 된다. 이때 delimiter를 탭 문자로 지정해야 한다.\n\ndata_files = {\"train\": \"train.tsv\", \"valid\": \"valid.tsv\", \"test\": \"test.tsv\"}\n\n\ndataset =  load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n\nUsing custom data configuration default-02a3611b1810efcd\n\n\nDownloading and preparing dataset csv/default to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\nDataset csv downloaded and prepared to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n제대로 로딩되었는지 dataset을 확인해보자.\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 10000\n    })\n})\n\n\nDatasetDict에 train, valid, test 키로 120만 문장, 9만 문장, 1만 문장이 저장된 것을 확인할 수 있다.\n이 데이터 셋에서 개별 샘플에 대한 접근은 [split][feature][row num] 형태로 가능하다.\n\n# train 스플릿에서 영어 3개와 한국어 3개 샘플을 가져온다.\nprint(dataset['train']['en'][:3], dataset['train']['ko'][:3])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n그런데 feature와 row num은 순서를 바꿔서 사용할 수 도 있다.\n\nprint(dataset['train'][:3]['en'], dataset['train'][:3]['ko'])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n데이터를 어떻게 조회하는지는 데이터 구성 방식에 따라 조금씩 다르므로 데이터 셋을 보고 몇번 해보면 금방 접근법을 알 수 있다.\n\n\n\n데이터 셋 준비를 마쳤으니 학습할 차례이다. 허깅페이스에서 제공하는 필요 클래스를 임포트 한다.\n먼저 선학습 모델을 사용하기 위한 클래스를 임포트 한다. AutoTokenizer는 선학습된 모델이 사용한 토크나이저를 읽기 위해 필요하며 AutoModelForSeq2SeqLM은 시퀀스 투 스퀀스 방식으로 작동하는 선학습된 모델을 불러 올 때 마지막에 분류기 헤드를 붙여서 모델을 로딩하기 위해 사용한다.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n2023-03-01 16:05:02.191320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-01 16:05:02.266647: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-03-01 16:05:02.281905: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-01 16:05:02.592853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n\n다음은 데이터 콜레이터를 임포트한다. 시쿼스 투 시퀀스 학습 과정은 인코더 입력 시퀀스, 디코더 입력 시퀀스, 디코더 출력 시퀀스를 필요로 하는데 미니배치로 부터 이를 적절히 정리해서 모델에 입력하는 작업이 필요하다. 예를 들면 미니 배치 내에 있는 인코더 입력 시퀀스의 길이를 맞춘다든지 디코더 입력시퀀스를 오른쪽으로 한칸 쉬프트시켜 디코더 출력 시퀀스를 만드는 작업등이 콜레이터에서 일어나는 작업인데 이런 작업을 DataCollatorForSeq2Seq가 자동으로 처리하게 된다.\n\nfrom transformers import DataCollatorForSeq2Seq\n\n그리고 학습에 필요한 클래스를 임포트 한다. 학습에 필요한 설정을 Seq2SeqTrainingArguments에 정의하고 실제 학습은 Seq2SeqTrainer로 하게 된다. Seq2SeqTrainer는 generate()함수를 제공한다.\n\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n허깅페이스 라이브러리로는 마지막으로 데이터 셋을 로딩하는 함수와 번역 결과를 측정할 함수를 로딩한다.\n\nfrom datasets import load_dataset, load_metric\n\n그외 필요한 각종 라이브러리를 임포트 한다.\n\nimport numpy as np\nimport torch\nimport multiprocessing\n\n허깅페이스에서 파이토치 기반 구현을 사용하므로 gpu가 있다면 device를 세팅한다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\n미리 학습된 모델의 체크포인트를 세팅한다. 여기서 사용할 모델은 한국어와 영어에 미리 학습된 KE-T5모델을 사용한다. T5모델은 트랜스포머의 인코더, 디코더 구조를 모두 사용하는 모델로 번역기를 만들 때 사용할 수 있는 모델이다. 아래처럼 모델 체크 포인트와 T5 모델에 입력될 최대 토큰 길이를 설정한다.\n\nmodel_ckpt = \"KETI-AIR/ke-t5-base\"\nmax_token_length = 64\n\n\n\n\n먼저 모델 체크 포인트를 사용하여 KE-T5 모델이 학습할때 함께 사용한 토크나이저를 불러온다. 허깅페이스 트랜스포머스 라이브러리를 사용할 때 가장 핵심이 되며 익숙해지기 쉽지 않은 부분이 이 토크나이저라고 개인적으로 생각한다.\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n\n토크나이저를 로딩할때 sentencepiece가 없다고 에러가 나면 위 제시한 라이브러리가 설치 안된 것이므로 설치하고 다시 시도한다.\n토크나이저를 불러왔으니 현재 사용하는 데이터셋에서 샘플을 가져와 토크나이징해보는 것이 좋을 것이다. 학습 세트에서 10번 샘플을 가지고 실험해보자. 먼저 10번 샘플을 뿌려보고\n\ndataset['train'][10]['en'], dataset['train'][10]['ko']\n\n('Any academic achievement requires constant repetition.',\n '어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.')\n\n\n토크나이저에 각 문장을 입력하고 토큰화된 상태로 돌려 받는다.\n\ntokenized_sample_en = tokenizer(dataset['train'][10]['en'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_en\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenized_sample_ko = tokenizer(dataset['train'][10]['ko'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_ko\n\n{'input_ids': [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n문장에 토큰으로 쪼개지고 각 토큰이 숫자로 변환된 것을 볼 수 있다. 이렇게 숫자화된 토큰을 input_ids로 반환하고 추가로 트랜스포머 인코더, 디코더에 쓰일 패딩 마스크도 함께 attention_mask로 돌려준다. 마스크가 모두 1인 이유는 샘플이 하나밖에 없어서 이다. 샘플 몇개를 더 실험해보면\n\ntokenizer(dataset['train'][:3]['en'], \n          max_length=max_token_length, \n          padding=True, truncation=True)\n\n{'input_ids': [[388, 6809, 2952, 17, 8, 32204, 43, 8023, 6687, 28, 9495, 91, 3, 1], [4014, 322, 3170, 147, 67, 23274, 3, 1, 0, 0, 0, 0, 0, 0], [11783, 4412, 96, 6556, 709, 1632, 3, 1, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}\n\n\n미니배치에 있는 샘플의 최대길이메 맞춰서 패딩되는 모습을 확인할 수 있다. 실제로 어떻게 토큰화 되었는지 확인해보자.\n\npd.DataFrame(\n    [\n        tokenized_sample_en['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_en['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nids\n13941\n10114\n25542\n9361\n20526\n742\n32268\n12520\n3\n1\n\n\ntokens\n▁Any\n▁academic\n▁achievement\n▁requires\n▁constant\n▁re\npet\nition\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\n\npd.DataFrame(\n    [\n        tokenized_sample_ko['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_ko['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nids\n404\n12663\n15\n10775\n2334\n6\n15757\n21\n29819\n1736\n26778\n4342\n15\n1701\n3\n1\n\n\ntokens\n▁어떤\n▁학문\n이\n든지\n▁일정\n의\n▁성취\n를\n▁이루기\n▁위해서는\n▁끊임없는\n▁반복\n이\n▁필요하다\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\nKE-T5를 학습할때 학습된 규칙대로 토큰화가 진행된다. 영어에서 repetition은 re, pet, ition으로 쪼개진 것을 볼 수 있고, 한국어에서 성취를은 성취, 를로 쪼개지고 학문이든지는 학문, 이, 든지로 쪼개진것을 볼 수 있다. 토큰 앞에 _표시는 이 토큰 앞에는 공백이 있어야 한다는 의미다. 그리고 마지막에 엔드 토큰인 &lt;/s&gt;가 항상 붙게 되는 것도 확인할 수 있다.\n이제 앞서 tsv파일로 부터 로딩한 dataset내의 문장을 모두 토크나이저를 사용해서 숫자로 바꾸는 작업을 해야 한다. 즉 문자로된 문장을 숫자로 바꿔 특성화 해야 한다. dataset.map()함수에 각 샘플을 토큰화 하는 함수를 만들어 전달하면 map()이 모든 샘플에 대해 전달받은 함수를 적용하게 되는데 함수는 이렇게 작성하면 된다.\n\ndef convert_examples_to_features(examples):\n    ###########################################################################\n    # with 쓰는 옛날 방식\n    # input_encodings = tokenizer(examples['en'], \n    #                             max_length=max_token_length, truncation=True)\n    \n    # Setup the tokenizer for targets\n    # with tokenizer.as_target_tokenizer():\n    # target_encodings = tokenizer(text_target=examples['ko'], \n    #                             max_length=max_token_length, truncation=True)\n    #\n    #\n    # return {\n    #     \"input_ids\": input_encodings[\"input_ids\"],\n    #     \"attention_mask\": input_encodings[\"attention_mask\"],\n    #     \"labels\": target_encodings[\"input_ids\"]\n    # }\n    \n    # 그런데 이렇게 하면 인풋하고 한번에 처리 가능함.\n    model_inputs = tokenizer(examples['en'],\n                             text_target=examples['ko'], \n                             max_length=max_token_length, truncation=True)\n    \n    return model_inputs\n\nconvert_examples_to_features()가 하고 싶은 일은 dataset에 있는 “어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.”라는 샘플 문장을 [404,12663,15,10775,2334,6,15757,21,29819,1736,26778,4342,15,1701,3,1]라는 정수로 바꾸는 것이다. convert_examples_to_features()가 dataset에 적용될 때 넘겨 받는 examples는 다음과 같이 넘어 온다.\nexamples= {'en':['sent1', 'sent2', ... , 'sent1000'], # 이건 문장 1000개짜리 리스트\n           'ko':['sent1', 'sent2', ... , 'sent1000']}\n기본으로 미니 배치 사이즈는 1000으로 세팅되어 있다.(함수 기본인자는 여기서 확인 가능)\n미니 배치로 넘어온 문장 샘플을 영어 문장과 한국어 문장을 각각 인풋과 타겟으로 토큰화하고 이로 부터 input_ids, attention_mask, labels로 묶어 리턴하는 방식이 예전에 쓰던 방식으로 함수 위쪽에 주석처리 되어 있다. 타겟 문장을 토큰화 할 때 타겟에서 필요로 하는 특수 토큰을 추가하는 경우 이를 처리하기위해 타겟 토큰 토큰화 때는 with tokenizer.as_target_tokenizer():라는 컨텍스트 매니저를 사용했는데 최근 업데이트에서는 그냥 tokenizer에 text_target인자에 타겟 문장을 넣어서 한번에 다 처리할 수 있다. 이렇게 model_inputs을 반환하면 dataset에 있던 각 레코드 마다 en, ko 특성에 추가로 input_ids, attention_mask, labels 특성이 더 추가 되게 된다. 사실 en, ko 특성은 더이상 필요없기 때문에 convert_examples_to_features()를 적용할 때 없애라는 인자를 세팅한다.\n바로 dataset에 함수를 적용해보자. 그냥 해도되나 좀 더 빠르게 하기 위해 num_proc 인자에 스레드 개수를 지정한다.\n\nNUM_CPU = multiprocessing.cpu_count() \nNUM_CPU\n\n20\n\n\n그리고 remove_columns 인자에 기존 특성 이름인 en, ko를 전달해서 기존 특성은 제거하게 한다. 이 특성이 있으면 이후 콜레이터가 샘플들을 미니 배치로 묶을 때 패딩처리를 못하게 된다.\n\ntokenized_datasets = dataset.map(convert_examples_to_features, \n                                 batched=True, \n                                 # 이걸 쓰지 않으면 원 데이터 'en', 'ko'가 남아서\n                                 # 아래서 콜레이터가 패딩을 못해서 에러남\n                                 remove_columns=dataset[\"train\"].column_names,\n                                 num_proc=NUM_CPU) \n\n\n[노트] dataset.map()이 실행되면서 출력되는 출력은 생략됨\n\nconvert_examples_to_features()이 dataset의 모든 샘플에 다 적용되고 나면 tokenized_datasets는 다음처럼 된다.\n\ntokenized_datasets\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 10000\n    })\n})\n\n\n기존에 있던 특성 en, ko는 사라졌고 en은 input_ids와 attention_mask로 ko는 labels로 바뀐것을 확인할 수 있다. 예를 들어 학습 세트에 10번 데이터를 보면 다음처럼 다 숫자라 바뀌게 된것이다.\n\ntokenized_datasets['train'][10]\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'labels': [404,\n  12663,\n  15,\n  10775,\n  2334,\n  6,\n  15757,\n  21,\n  29819,\n  1736,\n  26778,\n  4342,\n  15,\n  1701,\n  3,\n  1]}\n\n\n토크나이저를 써서 숫자로 부터 토큰화 해보면 다음과 같다.\n\nprint( '원 데이터    :', dataset['train'][10]['en'] )\nprint( '처리 후 데이터:', tokenized_datasets['train'][10]['input_ids'] )\nprint( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['input_ids']) )\n\nprint('\\n')\nprint( '원 데이터    :', dataset['train'][10]['ko'] )\nprint( '처리 후 데이터:', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['labels']) )\nprint( '토큰화       :', tokenized_datasets['train'][10]['labels'] )\n\n원 데이터    : Any academic achievement requires constant repetition.\n처리 후 데이터: [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1]\n토큰화       : ['▁Any', '▁academic', '▁achievement', '▁requires', '▁constant', '▁re', 'pet', 'ition', '.', '&lt;/s&gt;']\n\n\n원 데이터    : 어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.\n처리 후 데이터: ['▁어떤', '▁학문', '이', '든지', '▁일정', '의', '▁성취', '를', '▁이루기', '▁위해서는', '▁끊임없는', '▁반복', '이', '▁필요하다', '.', '&lt;/s&gt;']\n토큰화       : [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1]\n\n\n데이터 특성화를 모두 마쳤으므로 이제 모델을 로딩하자. AutoModelForSeq2SeqLM를 사용해서 선학습 모델을 불러오면 선학습된 T5모델 마지막에 파인튜닝할 수 있는 분류 헤드를 붙인 모델을 반환한다.\n\n\n\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n위처럼 모델을 로딩하고 모델 출력 시켜보면 T5 모델 레이어가 매우 길게 출력되는데 제일 마지막 부분에 다음과 같이 분류 헤드가 붙어 있는 것을 확인할 수 있다. 헤드를 보면 모델에서 출력하는 벡터는 768차원이고 이를 단어장 사이즈인 64128로 변환시키고 있는 것을 알 수 있다.\n(lm_head): Linear(in_features=768, out_features=64128, bias=False)\n이렇게 생성된 model은 인코더-디코더 구조를 가지는 트랜스포머이므로 이 모델을 포워딩 하려면 인코더 인풋과 디코더 인풋을 넣어줘야 한다. 모델을 만들고 가장 먼저해야되는 작업은 포워딩 테스트라고 개인적으로 생각한다. 임의의 입력을 넣고 출력이 의도대로 나오는지 확인하는 것이다. 이런 작업은 직접 만든 모델이 아닐 수록 중요한데 이렇게 해야지 모델이 제대로 작동하는지 또 어떤 구조로 되어 있는지 쉽게 이해할 수 있기 때문이다. 포워드 테스트를 하기위해 간단한 영어문장으로 예제를 준비한다.\n\nencoder_inputs = tokenizer(\n    [\"Studies have been shown that owning a dog is good for you\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\ndecoder_targets = tokenizer(\n    [\"개를 키우는 것이 건강에 좋다는 연구 결과가 있습니다.\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\n영어 문장은 인코더의 입력이 되고 한국어 문장은 디코더의 타겟이 된다. 아래처럼 모두 숫자로 변환되어 있다.\n\nprint( encoder_inputs )\nprint( decoder_targets )\n\ntensor([[24611,    84,   166,  8135,    38,   847,    91,    16,  8146,    43,\n           667,    40,   106,     1]], device='cuda:0')\ntensor([[15833, 12236,   179, 16120, 28117,  1007,  3883,   327,     3,     1]],\n       device='cuda:0')\n\n\n이제 디코더 입력을 만들기위해 model._shift_right를 사용해 디코더 출력을 오른쪽으로 쉬프트 시킨다.\n\ndecoder_inputs = model._shift_right(decoder_targets)\n\ndecoder_inputs와 decoder_targets이 어떻게 다른지 비교해보면\n\npd.DataFrame(\n    [\n        tokenizer.convert_ids_to_tokens(decoder_targets[0]),\n        tokenizer.convert_ids_to_tokens(decoder_inputs[0])\n    ],\n    index=('decoder target', 'decoder input')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\ndecoder target\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n&lt;/s&gt;\n\n\ndecoder input\n&lt;pad&gt;\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n\n\n\n\n\n\n\n위처럼 오른쪽으로 쉬프트된 디코더 입력은 &lt;pad&gt; 토큰이 추가되었다. 이렇게 출력으로 쓰이는 문장을 오른쪽으로 쉬프트시켜 티처포싱Teacher forcing을 진행하게 된다. 다음처럼 model에 인코더 입력, 디코더 입력, 디코더 타겟을 입력하고 포워드 시킨다.\n\n# forward pass\noutputs = model(input_ids=encoder_inputs, \n                decoder_input_ids=decoder_inputs, \n                labels=decoder_targets)\n\nmodel의 outputs에는 다음과 같은 키가 있다.\n\noutputs.keys()\n\nodict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])\n\n\n손실함수 값을 다음처럼 확인할 수 있고 grad_fn이 있기 때문에 output.loss를 백워드 시킬 수 있다.\n\noutputs.loss\n\ntensor(87.8185, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n\n\n인코더의 마지막 상태는 (1, 14, 768)이다. 각 숫자는 순서대로 샘플 수, 스탭 수, 모델 사이즈를 나타낸다. 즉 인코더로 들어가는 14개 토큰이 각각 768차원 벡터로 인코딩되었다.\n\noutputs['encoder_last_hidden_state'].shape\n\ntorch.Size([1, 14, 768])\n\n\nlogit은 디코더 입력 토큰 10개에 대한 그 다음 토큰 예측 10개를 담고있다. 샘플 한개에 대해서 10개 토큰에 대해서 64128개 단어에 대한 확률값이 들어 있다.\n\noutputs['logits'].shape\n\ntorch.Size([1, 10, 64128])\n\n\nlogit에 argmax를 씌워서 토큰화시켜보면 다음과 같다.\n\ntokenizer.convert_ids_to_tokens( torch.argmax(outputs['logits'][0], axis=1).cpu().numpy() )\n\n['큐브', '큐브', '▁비일비재', '▁비일비재', '▁베네', '▁비일비재', '▁베네', '▁베네', '큐브', '큐브']\n\n\n마지막 헤더가 학습이 되지 않았기 때문에 적절한 아웃풋이 나오지 않지만 입력과 출력의 텐서 모양을 보면 포워드 패스가 제대로 작동한다는 것을 알 수 있다.\n지금까지 데이터 셋, 토크나이저, 모델에 대해서 알아봤다. 이제 학습을 위해 두 단계가 남았는데 하나는 데이터를 미니배치 형태로 모아 주는 콜레이터collator와 나머지 하나는 모델을 평가할 매트릭이다\n\n\n\n파이토치에서 모델을 학습시키기 위해서 DataLoader를 사용하게 되는데 이 데이터 로더의 역할은 for 루프를 돌면서 데이터 셋으로 부터 샘플을 미니 배치 수만큼 가져오는 것이다. 이때 샘플을 미니 배치 수만큼 무작위로 가져와 어떤 식으로든 각 샘플을 짝맞춤해서 반환해야하는데 크기가 통일된 간단한 이미지 데이터인 경우 특별히 할것이 없지만 서로 크기가 다른 샘플들을 다루는 경우는 반환전 크기 또는 길이를 맞춘다든지 패딩을 한다든지 하는 추가 작업이 필요하게 된다. 이런 작업이 일어나는 곳이 collate_fn으로 지정되는 함수이다.\n시퀀스 투 시퀀스 모델을 학습시킬때 이런 콜레이터 함수가 하는 전형적인 역할은 입력 또는 출력 문자열을 패딩하고 조금 전 모델에서 알아봤듯이 디코더 타겟을 오른쪽으로 한칸 쉬프트 시켜서 디코더 입력으로 만드는 일이다. 앞서 이런 과정을 간단히기 직접 코딩해서 확인했지만 이런 작업을 자동으로 처리해주는 클래스가 DataCollatorForSeq2Seq이다.\n우선 콜레이터를 만들기 위해서는 토크나이저와 모델을 넘겨야 한다.\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n앞서 만들어논 tokenized_datasets에서 샘플 두개를 조회하면 다음처럼 전체 결과는 사전으로 리턴되며 사전의 각 키 아래에 여러 샘플들의 값이 리스트로 들어있게 된다.\n\n# 각 항목아래 샘플들이 리스트 형태로 묶여 반환된다.\ntokenized_datasets[\"train\"][1:3]\n\n{'input_ids': [[4014, 322, 3170, 147, 67, 23274, 3, 1],\n  [11783, 4412, 96, 6556, 709, 1632, 3, 1]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n 'labels': [[6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n  [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]]}\n\n\n콜레이터에 샘플을 넘길 때는 개별 샘플이 사전으로 묶이는 형태가 되어야 되므로 아래처럼 한번 가공하게 된다.\n\n# 콜레이터에는 샘플을 개별 {}로 넘겨야 됨\n[tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n\n[{'input_ids': [4014, 322, 3170, 147, 67, 23274, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1]},\n {'input_ids': [11783, 4412, 96, 6556, 709, 1632, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]}]\n\n\n위에 반환된 결과를 보면 각 샘플이 사전 {}으로 묶이고 샘플 하나에는 input_ids, attention_mask, labels이 존재한다. 각 샘플을 리스트로 묶어서 콜레이터에게 전달하고 반환되는 값을 확인해보자.\n\n# 콜레이터를 돌리면 알아서 패딩하고 쉬프트 시킨다.\nbatch = data_collator(\n    [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n)\n\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n반환된 batch의 키를 확인해보면 decoder_input_ids가 생긴것을 확인할 수 있다.\n\nbatch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\nbatch의 각 키에 어떤 값들이 들어있는지 확인해보자.\n\nbatch\n\n{'input_ids': tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n        [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,    15,\n          1587,     3,     1],\n        [ 9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,  2255,\n             3,     1,  -100]]), 'decoder_input_ids': tensor([[    0,  6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,\n            15,  1587,     3],\n        [    0,  9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,\n          2255,     3,     1]])}\n\n\n출력된 batch를 정리하면 아래처럼 된다.\n{\n    'input_ids': \n        tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n                [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), \n    'attention_mask': \n        tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1, 1, 1, 1]]), \n    'labels': \n        tensor(\n            [[ 6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n             [ 9881,18590,3837,70,4341,1086,677,35,426,2255,3,1,-100]]), \n    'decoder_input_ids': \n        tensor(\n            [[ 0,6842,404,951,5767,15387,    27,   831, 800,  4378, 15,  1587,     3],\n             [ 0,9881,18590,3837,70,4341,1086,677, 35,   426,2255,     3,     1]])\n}\n새로 생긴 decoder_input_ids는 앞에 0()이 붙어 있는 것이 보이고 label에서 끝에 1이 사라져 label이 오른쪽으로 쉬프트된 것임을 알 수 있다. 그리고 또 두번째 샘플 labels에서 마지막에 -100 이 보인다. 이 값은 label이 패딩된 것을 나타내며 손실 함수값을 계산할 때 -100이 있는 위치는 손실을 계산하지 않게 된다. 이렇게 시퀀스 투 시퀀스 모델을 학습하기 위해 필요한 자잘한 작업을 콜레이터가 알아서 자동으로 처리한다.\n\n\n\n마지막으로 학습한 모델을 측정할 매트릭을 준비해야 한다. 번역 모델에서는 주로 BLEU 점수를 사용한다. BLEU 점수는 번역기가 생성한 문장이 레퍼런스(정답이라는 표현을 사용하지 않는 이유는 제대로 된 번역 문장이 오직 하나가 아니기 때문)문장과 얼마나 비슷한지 측정하는 점수라고 생각하면 된다. 단 같은 단어가 반복된다든지 레퍼런스 문장보다 너무 짧은 문장을 생성한다든지 하면 패널티를 부여 한다. 그렇기 때문에 레퍼런스 문장과 길이가 최대한 비슷하고 다양한 단어를 사용하면서 생성된 문장의 단어가 레퍼런스 단어에 많이 보여야 높은 점수를 얻게 된다.\nBLEU를 계산하기 위해 허깅페이스 evaluate 라이브러리와 sacrebleu라이브러리를 제일 처음에 설치했었다.\nsacrebleu 라이브러리는 BLEU 구현체에서 사실상 표준 라이브러리이며 각 모델이 다른 토크나이저를 쓰는 경우 이를 BPE로 통일 시켜 BLEU 점수를 계산한다고 한다. 참고링크\nevaluate라이브러리로 이 sacrebleu를 불러온다.\n\nimport evaluate\n\nmetric = evaluate.load(\"sacrebleu\")\n\n아래와 같은 예제가 있을 때 두 영어 문장을 번역기가 predictions처럼 번역했고 데이터 셋에 두 문장의 레퍼런스 번역이 references처럼 두개씩 있을 때 bleu점수를 계산해보면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 100.00000000000004,\n 'counts': [21, 19, 17, 15],\n 'totals': [21, 19, 17, 15],\n 'precisions': [100.0, 100.0, 100.0, 100.0],\n 'bp': 1.0,\n 'sys_len': 21,\n 'ref_len': 21}\n\n\n첫 예에서는 predictions가 references의 두 문장 중 하나와 완전히 일치하므로 score가 100점이 나왔다. 하지만 약간 다른 식으로 번역을 한다면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"딥러닝 프레임워크가 잘 개발되었기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 25.28116160010779,\n 'counts': [14, 7, 4, 1],\n 'totals': [19, 17, 15, 13],\n 'precisions': [73.6842105263158,\n  41.1764705882353,\n  26.666666666666668,\n  7.6923076923076925],\n 'bp': 0.9000876262522591,\n 'sys_len': 19,\n 'ref_len': 21}\n\n\n점수가 떨어지는 것을 확인할 수 있다. 아래 함수는 모델의 예측과 레이블을 가지고 bleu를 계산하는 헬퍼 함수로 트랜스포머 학습 코스 번역기 매트릭에서 제공하는 코드를 그대로 복사 한 것이다.\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Some simple post-processing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n    \n    return result\n\n\n\n\n학습을 간단히 하기위해 허깅페이스에서 제공하는 Seq2SeqTrainer클래스를 사용한다. 학습 세부 조건은 Seq2SeqTrainingArguments를 사용하여 설정한다. 다음 코드로 학습에 필요한 세부 사항을 설정할 수 있다.\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"chkpt\",\n    learning_rate=0.0005,\n    weight_decay=0.01,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=128,\n    num_train_epochs=1,\n    save_steps=500,\n    save_total_limit=2,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"no\",\n    predict_with_generate=True,\n    fp16=False,\n    gradient_accumulation_steps=2,\n    report_to=\"none\" # Wandb 로그 끄기\n)\n\n이런 저런 자잘한 세팅을 해서 training_args를 만들고 trainer를 생성한다. 지금까지 준비한 model, training_args, tokenized_datasets, data_collator, tokenizer, compute_metrics를 넘기면 된다.\n\ntrainer = Seq2SeqTrainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"valid\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n이제 아래 코드로 드디어 학습을 할 수 있다!\n\n[주의] 코랩에서 실행한다면 per_device_train_batch_size를 12정도로 줄여서 학습해야 하는데 학습 시간만 10시간이 넘게 걸린다.\n\n\ntrainer.train()\n\n\n[노트] 학습 과정에서 출력되는 로그 문장들이 너무 길어서 여기선 생략 되었음\n\n학습이 끝났으면 다음 셀을 실행해서 결과를 저장한다.\n\ntrainer.save_model(\"./results\")\n\n\n\n\n학습과 저장을 성공적으로 마쳤으면 다음 명령으로 모델을 불러올 수 있다.\n\nmodel_dir = \"./results\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\nmodel.cpu();\n\nloading file spiece.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading configuration file ./results/config.json\nModel config T5Config {\n  \"_name_or_path\": \"./results\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"gelu_new\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"gated-gelu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": true,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.24.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64128\n}\n\nloading weights file ./results/pytorch_model.bin\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./results.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n\n\n로딩된 모델을 테스트하기 위해 다음 두 문장을 준비한다.\n\ninput_text = [\n    \"Because deep learning frameworks are well developed, in these days, machine translation system can be built without anyone's help.\",\n    \"This system was made by using HuggingFace's T5 model for a one day\"\n]\n\n모델이 입력하기위해 토크나이저로 토큰화 시킨다.\n\ninputs = tokenizer(input_text, return_tensors=\"pt\", \n                   padding=True, max_length=max_token_length)\n\n/home/metamath/miniconda3/envs/torchflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n\n\ninputs를 확인해보면 input_ids와 attention_mask로 토큰화 된것을 알 수 있다. 첫번째 문장이 더 길기 때문에 두번째 문장의 마스크는 마지막에 0으로 패딩된 것도 확인할 수 있다.\n\ninputs\n\n{'input_ids': tensor([[ 8127,  5859,  5789, 22309,     8,    69,   484,  6560,     4,    20,\n           572,  1258,     4,  9872, 46301,  1076,   147,    67,  3807,  1215,\n          3993,    17,     8,   787,     3,     1],\n        [  465,  1076,    62,   565,    81,  1676,   992, 60049,  1044, 17400,\n            17,     8,   745,   466,  3900,    40,    16,   165,   688,     1,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0]])}\n\n\nmodel.generate()에 입력을 넣고 출력을 생성한다. 이때 빔서치를 하기 위해 num_beams=5로 설정한다.\n\nkoreans = model.generate(\n    **inputs,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\nkoreans.shape\n\ntorch.Size([2, 20])\n\n\n생성된 결과를 디코딩해보면 다음처럼 나쁘지 않게 번역되는 것을 확인할 수 있다.\n\n[ \n    tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(korean)) for korean in koreans\n]\n\n['&lt;pad&gt; 딥러닝 틀이 잘 개발되기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.&lt;/s&gt;',\n '&lt;pad&gt; 이 시스템은 HuggingFace의 T5 모델을 하루 동안 사용해 만든 시스템입니다.&lt;/s&gt;&lt;pad&gt;']\n\n\n마지막으로 테스트 셋에 대해서 몇개 문장을 가져와 번역해보자. 만들어 놓은 tokenized_datasets과 data_collator를 pytorch DataLoader에 그대로 전달해서 데이터 로더를 만들 수 있다.\n\nfrom torch.utils.data import DataLoader\n\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n)\n\n이터레이터로 만들어 한 미니 배치만 가져온다.\n\ntest_dataloader_iter = iter(test_dataloader)\n\n\ntest_batch = next(test_dataloader_iter)\n\n콜레이터에 의해 반환된 미니 배치에는 다음처럼 labels, decoder_input_ids 따위도 가지고 있으므로 모델에 입력하기 위해 input_ids, attention_mask만 남긴다.\n\ntest_batch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\n\ntest_input = { key: test_batch[key] for key in ('input_ids', 'attention_mask') }\n\n\nkoreans = model.generate(\n    **test_input,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\n이제 입력문장, 정답 그리고 생성된 문장을 비교하기 위해 우선 test_batch.labels에 -100으로 인코딩된 부분을 패딩 코튼으로 교체 한다.\n\nlabels =  np.where(test_batch.labels != -100, test_batch.labels, tokenizer.pad_token_id)\n\n\neng_sents = tokenizer.batch_decode(test_batch.input_ids, skip_special_tokens=True)[10:20]\n\n\nreferences = tokenizer.batch_decode(labels, skip_special_tokens=True)[10:20]\n\n\npreds = tokenizer.batch_decode( koreans, skip_special_tokens=True )[10:20]\n\n\nfor s in zip(eng_sents, references, preds):\n    print('English   :', s[0])\n    print('Reference :', s[1])\n    print('Translated:', s[2])\n    print('\\n')\n\nEnglish   : Yes, I'll see you at the parking lot at 3 p.m.\nReference : 네, 오후 3시에 주차장에서 뵙죠.\nTranslated: 네, 오후 3시에 주차장에서 뵙겠습니다.\n\n\nEnglish   : I'm happy to see Jessica Huh take over my role.\nReference : Jessica Huh가 제 역할을 인계받게 되어서 저는 기니다.\nTranslated: 제시카 허가 제 역할을 맡아서 기뻐요.\n\n\nEnglish   : I agree with you that she is qualified for the position.\nReference : 저도 부장님 의견대로 그녀가 이 자리의 적임자라고 생각합니다.\nTranslated: 나는 그녀가 이 직책에 자질이 있다고 당신과 동의합니다.\n\n\nEnglish   : Nick, I was told that your department will be divided into two.\nReference : Nick, 당신 부서가 둘로 나뉜다면서요?\nTranslated: 닉, 당신의 부서가 두 가지로 나뉘게 될 거라고 들었습니다.\n\n\nEnglish   : Yes, all staff involved in online advertising will have their own office space located on the 2nd floor.\nReference : 네, 다음 달 초부터 온라인 광고와 관련된 직원들은 2층에 위치한 개별 사무실 공간을 사용한다고 합니다.\nTranslated: 네, 온라인 광고에 관련된 모든 직원들은 2층에 각자의 사무실 공간이 위치해 있습니다.\n\n\nEnglish   : What happens to the remaining staff?\nReference : 남은 직원들은 어떻게 되나요?\nTranslated: 남은 스태프에게 무슨 일이 일어났나요?\n\n\nEnglish   : The remaining staff will stay in the current space on the 3rd floor, and the division will continue to be called the advertising department.\nReference : 남은 직원들은 3층 현재 자리에 남을 것이고, 부서는 계속 광고 부서로 불린다고 하네요.\nTranslated: 나머지 직원들은 3층 현 공간에 머물게 되며, 이 부서는 계속 광고부서로 불리게 된다.\n\n\nEnglish   : I have a question about the year-end tax adjustment.\nReference : 이번 연말 정산 관련해서 질문이 있어요.\nTranslated: 저는 연말정산에 대해 궁금한 점이 있습니다.\n\n\nEnglish   : Is there any problem?\nReference : 무슨 문제라도 있으신가요?\nTranslated: 혹시 문제가 있나요?\n\n\nEnglish   : I am registering my dependent this time, so do I need to submit any particular documents?\nReference : 제가 이번에 부양가족을 등록하려고 하는데, 따로 제출해야 하는 서류가 있나요?\nTranslated: 이번에 내 국적 등록을 하고 있으니 특별히 서류 제출을 해야 하나요?\n\n\n\n\n두 시간동안 대충 1 에폭만 학습한 것치고는 꽤 그럴듯 하게 번역을 하는 것을 알 수 있다.\n\n\n\n이상으로 영어-한국어 번역기를 처음부터 학습시키는 방법을 정리했다. 이 글을 글쓴이가 의도한대로 빠르게 읽고 이해하기 위해서는 트랜스포머에 대한 이해가 선행되야 한다. 트랜스포머에 대한 자세한 설명은 진짜로 주석달린 트랜스포머를 참고하자. 하지만 트랜스포머나 스퀀스 투 시퀀스 모델에 대해 잘 모른다 하더라도 한국어 번역기를 만들고자 할때 느끼는 막막함은 어느정도 해소할 수 있으리라 생각한다.\n이 글을 읽고 코드를 실행해보고 나서 DataCollatorForSeq2Seq나 Seq2SeqTrainer 를 쓰지 않고 직접 이 부분을 만들어서 모델을 학습 시켜본다면 트랜스포머를 이용한 번역 작업기 만들기를 훨씬 더 상세히 이해할 수 있을 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#본-글의-목적",
    "href": "posts/em/em_algorithm.html#본-글의-목적",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "본 글의 목적",
    "text": "본 글의 목적\n머신러닝을 공부하다 보면 한번은 보게되는 알고리즘이 바로 EM 알고리즘이다. 많은 문헌에서 이 알고리즘을 설명할 때 K-평균 군집화로 시작해서 가우시안 혼합으로 끝을 맺는다. 하지만 두 알고리즘에 대해서 설명하는 것은 EM 알고리즘의 적용 예를 설명하는 것이지 EM 알고리즘을 근본적으로 이해하기 위한 논리를 설명하는 것이 아니어서 해당 내용을 모두 읽어봐도 EM 알고리즘이 도대체 무엇인지 감을 잡기 힘든 경우가 대부분이다.\nEM 알고리즘에 대해서 자세한 설명이 부족하게 된 원인은 개인적인 견해지만 EM 알고리즘이 확률과 통계를 기반으로 하는 알고리즘이기 때문이라 생각한다. 확률과 통계는 알아야 할 내용도 많고 매우 추상적이기 때문에 (적어도 나에게는) 기본적으로 쉽게 접근할 수 없는 문제가 있다. 그런데 그런 내용들이 복잡하게 얽혀 있다면 지면의 한계 또는 난이도의 제약으로 충분한 설명을 하지 못하는 것이 어쩌면 당연할 수도 있다는 생각이 든다.\n이는 관련 분야 전공자들도 어느정도 인정하는 부분인데(https://bayestour.github.io/blog/2019/06/23/EM_algorithm.html) 이런 어려움은 정식으로 출판된 문헌에서도 확인할 수 있다. “The Elements of Statistical Learning”을 예로 들면 EM 알고리즘을 설명하는 8.5절에 뭉크의 절규 아이콘이 붙어 있다.\n확률, 통계에 대한 초보적 지식을 가진 공대생이 참고할 만한 좋은 책은 “패턴인식과 머신러닝”(이하 PRML로 표기)인데 9장 전체를 할애하여 EM 알고리즘을 설명하고 있다. 그런데 이 교재 역시 내용을 전개하는 순서가 좋지 못해서 전체적인 맥락을 이해하기 매우 힘들다.\n이런 이유로 이 글은 PRML의 설명을 재구성하여 가능한 쉬운 예와 코드를 곁들여 EM 알고리즘을 이해하는 것을 목적으로 한다. 그렇기 때문에 수식 번호와 기호법은 PRML과 동일하게 구성하였다. 수식 번호에 (x.x)형식은 PRML 수식을 그대로 사용한 것이다. 혹시나 이 글을 읽고 PRML을 다시 읽을 때 혼란을 최소화 하기 위해서이다.\n가급적 쉽게 설명하려고 많은 고민을 하였으나 기본적으로 이 글을 읽기 위한 선수 지식이 있음을 피할 수는 없었다. 이 글을 읽기 위한 선수 지식은 다음과 같다.\n\n이항분포\n가능도 함수\n경사 하강법\n최대 가능도 추정MLE:Maximum Likelihood Estimation\n라그랑지 승수와 간단한 제약 최적화\npython 문법과 scipy.optimize.minimize() 함수 사용법\n\n(선수 지식이 이 정도인데 이 글 정말 쉽게 이해할 수 있는 글 맞는건지?? ;;;)"
  },
  {
    "objectID": "posts/em/em_algorithm.html#기호",
    "href": "posts/em/em_algorithm.html#기호",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "기호",
    "text": "기호\n다음에 이 글 전반에 걸쳐 사용하는 기호를 정리하였다.\n\n\\(x\\), \\(\\mathbf{x}\\) : \\(D\\)차원 벡터인 데이터, 스칼라인 경우 \\(D=1\\)\n\\(N\\) : 데이터 \\(\\mathbf{x}\\)의 개수\n\\(N_k\\) : \\(k\\)번째 분포에서 샘플링된 데이터 개수\n\\(\\mathbf{X}\\) : 데이터 \\(\\mathbf{x}_n\\)이 행인 행렬. 차원은 (N,D)\n\\(K\\) : 데이터를 샘플링한 분포의 개수\n\\(\\mathbf{z}\\) : \\(K\\)차원 벡터인 잠재변수, 이 잠재변수는 원핫인코딩된 다항변수이다.\n\\(\\mathbf{Z}\\) : 잠재변수 \\(\\mathbf{z}_n\\)이 행인 행렬. 차원은 (N,K)\n\\(n_t\\) : 이항분포에서 시도 횟수"
  },
  {
    "objectID": "posts/em/em_algorithm.html#문제-설정",
    "href": "posts/em/em_algorithm.html#문제-설정",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "문제 설정",
    "text": "문제 설정\n본격적인 설명을 하기 앞서 EM알고리즘을 개략적으로 설명한 다음 글을 읽어보길 추천한다.\n\n“박준석, 2019, EM 알고리즘 이해 및 구현하기”(https://bayestour.github.io/blog/2019/06/23/EM_algorithm.html)\n\n간단하게 문제를 설정하고 매우 직관적으로 EM알고리즘을 설명하는 좋은 글이다. EM 알고리즘에 대해 어느 정도 이해를 하고 있는 것 같은데 깔끔하게 정리가 안되는 느낌을 가지고 있다면 꼭 한번 읽어보길 추천한다.\n본 글에서는 윗 글에서 다루고 있는 똑같은 사례에 좀 더 자세한 설명을 추가하는 것으로 논의를 시작하고자 한다. 박준석(2019)의 원문격에 해당하는 논문(https://www.nature.com/articles/nbt1406?proof=true) 에서는 베르누이 확률분포를 따르는 동전을 한 세트에 열번씩 다섯 세트 던지는 상황을 이야기 하고 있다. 이것을 박준석(2019)에서는 \\(n_t=10\\)인 이항분포에서 다섯 번 샘플링하는 방식으로 이야기하고 있다. 본 글에서도 후자를 기준으로 하며 그 문제는 다음과 같다.\n\n\\(\\text{Bin}(x \\mid n_t =10, \\mu_1)\\)과 \\(\\text{Bin}(x \\mid n_t=10, \\mu_2)\\)인 이항분포 두 개가 있다. 두 분포로 부터 독립적으로 다섯 번 샘플링을 하는데 (10, 4, 3, 7, 8)처럼 샘플링이 되었다. 이 정보를 가지고 \\(\\mu_1\\), \\(\\mu_2\\)를 추정하시오.\n\n이 상황을 코드로 구현하면 다음과 같다.\n이 글은 다음 링크를 통해 colab에서 직접 실행하면서 읽을 수 있다.\n\n\n# 글 전체에서 필요한 모듈을 임포트한다.\nimport numpy as np\nimport itertools\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# 구글 코랩에서 그래프에 LaTeX를 원활히 쓰기 위한 설정으로 코랩이 아니면 실행 안함\nmatplotlib.rc('text', usetex=True)\nmatplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng &gt; /dev/null\n\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nExtracting templates from packages: 100%\n\n\n\n# 두 이항분포의 알지 못하는 파라미터\nMU_1, MU_2 = 0.8, 0.45\n\n# 이항분포에서 시행횟수 n_t\nn_t = 10\n\n\n# 샘플링\nnp.random.seed(1)\n\n# 적당히 샘플링하고\nx_from_p1 = np.random.binomial(n_t, MU_1, 3)\nx_from_p2 = np.random.binomial(n_t, MU_2, 2)\n\n# 섞어서 X를 만든다.\nX = np.concatenate((x_from_p1, x_from_p2))\nnp.random.shuffle(X)\n\nprint(X)\n\n[10  4  3  7  8]\n\n\n여기서 우리가 하고자 하는 것은 데이터 (10, 4, 3, 7, 8)을 이용하여 두 분포의 파라미터 \\(\\mu_1\\)과 \\(\\mu_2\\)를 추정하는 것이다. 위 코드에서 \\(\\mu_1=0.8\\), \\(\\mu_2=0.45\\)로 둔 것이 확인되지만 원래 이 숫자는 우리가 추정해야 하는 것이다.\n만약 샘플 다섯 개가 분포 하나로 부터 나온 간단한 경우라면 분포의 파라미터를 추정하기 위해 데이터에 대한 최대 가능도 추정을 하면 된다. 하지만 샘플링하는 분포는 두 개이며 어느 분포에서 어떤 데이터가 샘플링되었는지 모르는 상황이다. 따라서 데이터에 대한 가능도 함숫값을 계산할 수 가 없다. 각 데이터가 어느 분포에서 샘플링되었는지 알고 있다면 각 분표별로 데이터를 나누고 각각 최대 가능도 추정을 하면 될것이다. 결국 위 상황에 대해서 모든 정보를 다 알고 있다고 말할 수 있으려면 샘플링된 숫자 다섯 개가 어느 분포에서 샘플링되었는지도 알아야 한다.\n샘플링된 데이터를 \\(x_n\\)으로 쓰기로 하자. 그리고 \\(x_n\\)이 어느 분포에서 생성되었는지를 나타내는 카테고리 변수를 \\(\\mathbf{z}_n\\)으로 쓰기로 하자. 앞서 말한 것처럼 모든 정보를 다 알고 있다고 하려면 \\(x_1\\), \\(x_2\\), \\(x_3\\), \\(x_4\\), \\(x_5\\)와 이에 해당하는 \\(\\mathbf{z}_1\\), \\(\\mathbf{z}_2\\), \\(\\mathbf{z}_3\\), \\(\\mathbf{z}_4\\), \\(\\mathbf{z}_5\\)도 모두 알아야 한다. 우리가 가진 데이터가 1번 분포, 2번 분포, 2번 분포, 1번 분포, 1번 분포에서 생성되었다면 \\(\\mathbf{z}_n\\)은 각각 다음과 같을 것이다.\n\\[\n\\mathbf{z}_1 = (1, 0)^\\text{T} \\\\\n\\mathbf{z}_2 = (0, 1)^\\text{T} \\\\\n\\mathbf{z}_3 = (0, 1)^\\text{T} \\\\\n\\mathbf{z}_4 = (1, 0)^\\text{T} \\\\\n\\mathbf{z}_5 = (1, 0)^\\text{T}\n\\]\n이제 \\(x_n\\), \\(\\mathbf{z}_n\\)을 모두 모아 행렬 \\(\\mathbf{X}\\), \\(\\mathbf{Z}\\)로 표기하자. 행렬 \\(\\mathbf{X}\\)에서 한 행은 \\(x_n\\)인데 만약 샘플링되는 데이터가 벡터라면 \\(\\mathbf{x}_n^{\\text{T}}\\)가 될 것이다. 행렬 \\(\\mathbf{Z}\\)에서 한 행은 \\(\\mathbf{z}_n^{\\text{T}}\\)이다. 행렬 \\(\\mathbf{X}\\)와 \\(\\mathbf{Z}\\)는 다음과 같게 된다.\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5\n\\end{bmatrix}= \\begin{bmatrix}\n10\\\\ 4\\\\ 3\\\\ 7\\\\ 8\n\\end{bmatrix} \\qquad\n\\mathbf{Z} = \\begin{bmatrix}\n\\mathbf{z}_1^{\\text{T}} \\\\\n\\mathbf{z}_2^{\\text{T}} \\\\\n\\mathbf{z}_3^{\\text{T}} \\\\\n\\mathbf{z}_4^{\\text{T}} \\\\\n\\mathbf{z}_5^{\\text{T}}\n\\end{bmatrix}=\\begin{bmatrix}\n1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 0\n\\end{bmatrix}\n\\]\n간단하게 아래 코드로 행렬 \\(\\mathbf{X}\\), \\(\\mathbf{Z}\\)를 만들어 두자.\n\n# X: (N,D), (5,1)\nX = X.reshape(-1,1)\nprint(X)\n\n# Z: (N,K), (5,2)\nZ = np.array([[1,0],[0,1],[0,1],[1,0],[1,0]])\nprint(Z)\n\n# 노트북 전체에 사용될 전역 변수 설정\nN, D = X.shape\nK = 2\n\n[[10]\n [ 4]\n [ 3]\n [ 7]\n [ 8]]\n[[1 0]\n [0 1]\n [0 1]\n [1 0]\n [1 0]]\n\n\n이렇게 두 행렬 \\(\\{\\mathbf{X}, \\mathbf{Z}\\}\\)가 모두 주어지는 데이터를 완전 데이터 세트complete data set라 한다. 완전 데이터 세트일 때 최대 가능도 추정을 실제로 해보자."
  },
  {
    "objectID": "posts/em/em_algorithm.html#완전-데이터-세트에-대한-최대-가능도-추정",
    "href": "posts/em/em_algorithm.html#완전-데이터-세트에-대한-최대-가능도-추정",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "완전 데이터 세트에 대한 최대 가능도 추정",
    "text": "완전 데이터 세트에 대한 최대 가능도 추정\n\n가능도 함수\n문제 설정에서 주어진 문제는 확률분포가 2개인 경우지만 \\(K\\)개로 가정하고 이야기하자. 주어진 \\(K\\)개 분포중 특정 \\(k\\)번째 분포가 선택될 확률을 \\(\\pi_k\\)로 표시하자. 그러면 어떤 샘플 \\(x\\)에 대한 확률질량함수는 다음처럼 주어진 확률분포함수를 선형조합하여 얻을 수 있을 것이다. 이때 선형조합 계수는 \\(\\pi_k\\)가 될 것이다.\n\\[\np(x)  = \\sum_{k=1}^K \\pi_k \\text{Bin}(x \\mid n_t, \\mu_k) \\tag{9.7}\n\\]\n직관적으로 식(9.7)이 맞을 것 같지만 정말 그렇게 되는지는 불완전 데이터 세트에 대한 가능도 함수를 구할 때 다시 정식으로 유도해보자.\n앞서 살펴봤듯이 \\(x\\)가 어떤 분포에서 샘플링 되었는지를 나타내는 잠재변수latent variable \\(\\mathbf{z}\\)는 \\(K\\)차원 멀티누이multinoulli 변수이다. 따라서 변수 \\(\\mathbf{z}\\)의 확률질량함수는 식(9.10)처럼 쓸 수 있다.\n\\[\np(\\mathbf{z}) = \\prod_{k=1}^{K} \\pi_k^{z_k} \\tag{9.10}\n\\]\n\\(\\mathbf{z}\\)가 주어졌다면 샘플 \\(x\\)가 어느 분포를 따르는지 알 수 있으므로 해당 샘플의 확률분포 함수는 그 \\(\\mathbf{z}\\)가 가리키는 분포의 \\(\\text{Bin}(x \\mid n_t, \\mu_k)\\)가 된다. 다시말해 \\(\\mathbf{z}\\)가 주어진 조건하에서 \\(x\\)의 확률분포 함수는 식(9.11)처럼 결정되게 된다.\n\\[\np(x \\mid \\mathbf{z}) = \\prod_{k=1}^K \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} \\tag{9.11}\n\\]\n\\(\\mathbf{z}\\)에서 \\(z_k\\)는 오직 하나만 1이고 나머지는 0이므로 잘 생각해보면 식(9.11)이 타당함을 알 수 있다. 이제 \\(x\\)와 \\(\\mathbf{z}\\)의 결합확률분포를 생각하자.\n\\[\np(x,\\mathbf{z}) = p(x \\mid \\mathbf{z})p(\\mathbf{z}) = \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n파라미터를 \\(\\boldsymbol{\\theta}=(\\pi_1, \\pi_2, ..., \\pi_k, \\mu_1, \\mu_2, ..., \\mu_k)^{\\text{T}}\\)로 쓰면 완전 데이터 세트의 \\(\\boldsymbol{\\theta}\\)에 대한 가능도 함수는 다음과 같다.\n\\[\np(x,\\mathbf{z} \\mid \\boldsymbol{\\theta}) = \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n모든 데이터를 고려하기 위해 데이터에 대한 인덱스 \\(n\\)을 도입하고 독립성 가정하에서 가능도를 구하기 위해 모두 곱해주자.\n\\[\np(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \\text{Bin}(x_n \\mid n_t, \\mu_k)^{z_{nk}}\n\\]\n이제 가능도 함수에 로그를 적용하면 최종적으로 완전 데이터 세트에서 로그 가능도 함수가 구해진다.\n\\[\n\\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) = \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\tag{9.36}\n\\]\n식(9.36)에서 \\(z_{nk}\\)를 모두 알고 있으므로 로그 가능도함수의 함숫값을 실제로 구할 수 있고 이를 이용하여 수지척으로 식(9.36)을 최적화 할 수 있다.\n\n\n수치적 방법\n가능도 함수를 구했으므로 이 가능도 함수를 기울기 하강법을 이용하여 직접 최적화 시켜볼 수 있다.\n\nfrom scipy.stats import binom\nfrom scipy.optimize import minimize, Bounds\n\n식(9.36)을 목적함수로 설정하고 나머지 최적화 과정은 사이파이 최적화 함수를 쓰기로 하자. 위처럼 필요한 모듈을 임포트 한다. 결정해야하는 변수는 \\(\\mu_1\\), \\(\\mu_2\\), \\(\\pi_1\\), \\(\\pi_2\\)이고 초기값은 적당히 초기화 한다.\n\n# 초기 파라미터를 주어진 데이터 중에서 아무거나 골라 만든다.\nnp.random.seed(34)\n\nmu_0 = X[np.random.choice(N, K)] / n_t\n\n# pi는 동일하게 설정한다.\npi_0 = np.array([0.5, 0.5])\n\nprint(mu_0)\nprint(pi_0)\n\n[[0.4]\n [0.3]]\n[0.5 0.5]\n\n\n이제 식(9.36)을 그대로 코딩한다.\n\ndef loglikelihood_XZ_(mu, pi, X, Z):\n    \"\"\"\n    eq(9.36)\n    mu     : (K,D)\n    pi     : (K,)\n    X      : (N,D) global variable\n    Z      : (N,K)\n    -----------------------------------------\n    N,D,K : gloval variables\n    \"\"\"\n    # N, D, K = X.shape[0], X.shape[1], Z.shape[1]\n\n    sigma_n = 0.0\n\n    for n in range(N):\n        sigma_k = 0.0\n\n        for k in range(K):\n            loglikelihood_x = np.log(binom.pmf(X[n,0], n_t, mu[k,0])+1.0e-8)\n            sigma_k += Z[n,k] * (np.log(pi[k]+1.0e-8) + loglikelihood_x)\n\n        sigma_n += sigma_k\n\n    return sigma_n\n\nscipy.optimize.minimize()함수를 사용하기 위해서는 최적화 변수가 1차원 벡터 형식으로 전달되어야 한다. 위 loglikelihood_XZ_()함수는 \\(\\mu_k\\)와 \\(\\pi_k\\)를 (K,D), (K,) 형태로 전달받으므로 minimize()함수에 바로 사용할 수 없다. 그래서 \\(\\mu_k\\)와 \\(\\pi_k\\) 한 줄로 펴서 벡터 형태로 만든 다음 전달할 래퍼함수를 하나 더 만든다.\n\ndef loglikelihood_XZ(theta, X, Z):\n    \"\"\"\n    theta[:K*D] : mu, (K,D)\n    theta[K*D:] : pi, (K,)\n    X           : (N,D)\n    Z           : (N,K)\n    -----------------------------------\n    N,D,K : gloval variables\n    \"\"\"\n\n    # N, D, K = X.shape[0], X.shape[1], Z.shape[1]\n\n    mu = theta[:K*D].reshape(K,D)\n    pi = theta[-K:]\n\n    return loglikelihood_XZ_(mu, pi, X, Z)\n\n위 함수는 모든 \\(\\mu_k\\)와 \\(\\pi_k\\)를 담은 1차원 벡터 theta를 전달 받는다. 그 후 적당히 theta를 분리하고 loglikehood_XZ_()함수에 전달하고 있다.\n앞서 정의한 두 함수가 같은 값을 계산하는지 확인해본다. 같은 값이 나오면 최종적으로 음수를 곱해서 NLL(Negative Log Likelihood)로 만든다. 사이파이에서 제공하는 minimize()함수는 최소화를 수행하기 때문이다.\n\n# 래퍼함수와 원함수 결과가 같게 나오는지 확인\ntheta = np.hstack((mu_0.flatten(),  pi_0))\nprint(loglikelihood_XZ_(mu_0, pi_0, X, Z))\nprint(loglikelihood_XZ(theta, X, Z))\n\n# 로그 가능도 함수의 최대화를 마이너스 로그 가능도함수의 최소화로 바꾸기 위해\n# 보조 함수 정의\ndef negative_loglikelihood_XZ(theta, X, Z):\n    return -loglikelihood_XZ(theta, X, Z)\n\n-23.26286598620995\n-23.26286598620995\n\n\n이제 각 변수에 대해서 적당히 바운드 제약조건을 설정한다. 각 파라미터에 대해 \\(0 \\le \\pi_k, \\mu_k \\le 1\\)가 보장되어야 할것이다.\n\n# 바운드 제약조건\nbounds = Bounds((0., 0.,   # mu_k의 하한\n                 0., 0),   # pi_k의 하한\n                (1., 1.,   # mu_k의 상한\n                 1., 1.))  # pi_k의 상한\n\n\\(\\pi_k\\)는 멀티누이 변수 \\(\\mathbf{z}\\)의 파라미터이기 때문에 모두 더해서 1이 되어야 하므로 다음처럼 제약조건을 추가한다.\n\\[\n\\sum_{k=1}^K \\pi_k = 1\n\\]\n\n# 등호제약조건, p(z)에 대한 제약조건 다 더해서 1\n# sum pi = 1\ndef constraint(theta):\n    pi = theta[-K:]\n    return pi.sum() - 1.\n\ncons   = ( {'type': 'eq',   'fun': constraint   }, )\n\n이제 목적함수와 제약조건을 minimize()함수에 넘기면 최적화 과정이 수행된다.\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\nprint('Init. input:', x)\n\nres = minimize(negative_loglikelihood_XZ, x, args=(X, Z), method='slsqp',\n               bounds=bounds, constraints=cons,\n               options={'iprint': 2, 'disp': True})\n\nprint(res)\n\nprint(\"sum pi_k:\",res.x[2:].sum())\n\nInit. input: [0.4 0.3 0.5 0.5]\n  NIT    FC           OBJFUN            GNORM\n    1     6     2.326287E+01     5.484928E+01\n    2    13     1.895467E+01     4.216554E+01\n    3    20     1.694546E+01     3.614093E+01\n    4    27     1.525500E+01     3.100563E+01\n    5    34     1.433698E+01     2.762727E+01\n    6    41     1.327568E+01     2.376365E+01\n    7    47     1.170940E+01     2.275331E+01\n    8    53     1.132655E+01     1.118621E+01\n    9    59     1.112874E+01     7.772592E+00\n   10    65     1.110341E+01     7.117206E+00\n   11    71     1.110208E+01     7.069603E+00\n   12    77     1.110207E+01     7.071547E+00\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 11.102073936079918\n            Iterations: 12\n            Function evaluations: 77\n            Gradient evaluations: 12\n     fun: 11.102073936079918\n     jac: array([-3.89838219e-03,  4.10699844e-03, -4.99865413e+00, -5.00201964e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 77\n     nit: 12\n    njev: 12\n  status: 0\n success: True\n       x: array([0.83331527, 0.35004671, 0.60016153, 0.39983847])\nsum pi_k: 1.0\n\n\n몇 회 반복 후 다음처럼 해를 찾게 된다.\nx: array([0.83331527, 0.35004671, 0.60016153, 0.39983847])\n수치적으로 찾은 해는\n\\[\n\\mu_1 = 0.83331527, \\quad \\mu_2=0.35004671, \\quad \\pi_1=0.60016153, \\quad \\pi_2=0.39983847\n\\]\n이며 출력 마지막 줄 sum pi_k: 1.0를 보면 \\(\\pi_k\\)에 대한 제약조건도 잘 지켜지고 있음을 알 수 있다. 사실 이 정도 문제는 가능도 함수가 복잡하지 않아서 직접 미분하여 해석적으로 최대 가능도 해를 구할 수 있다.\n\n\n해석적 방법\n가능도 함수를 직접 미분하여 최적해를 바로 찾아보자. 식(9.36)을 \\(\\mu_j\\)와 \\(\\pi_j\\)로 미분한다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\n&= \\frac{\\partial}{\\partial \\mu_j} \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk}  \\left\\{ \\frac{\\partial}{\\partial \\mu_j} \\ln \\pi_k +  \\frac{\\partial}{\\partial \\mu_j} \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k)  \\right\\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\left[ \\frac{\\partial}{\\partial \\mu_j}  \\left\\{ \\ln \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_k^{x_n} (1-\\mu_k)^{(n_t - x_n)} \\right\\} \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\left[ \\frac{\\partial}{\\partial \\mu_j}  \\left\\{ \\ln \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} + {x_n} \\ln \\mu_k + {(n_t - x_n)} \\ln  (1-\\mu_k) \\right\\} \\right]\n\\end{aligned}\n\\]\n위 과정에서 세 번째 등호는 \\(\\ln \\pi_k\\)가 \\(\\mu_j\\)의 함수가 아니므로 성립한다. 마지막 줄에서 \\(\\{ \\}\\)안을 미분하면 \\(k=j\\)인 경우를 제외하고는 모두 0이 된다. \\(\\{ \\}\\)안 첫째 항은 \\(\\mu_j\\)에 대해서 상수라서 사라지고 둘째항과 셋째항을 미분하면 아래와 같다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) &= \\sum_{n=1}^N z_{nj} \\left( \\frac{x_n}{\\mu_j} - \\frac{n_t - x_n}{1-\\mu_j} \\right) \\\\\n&= \\sum_{n=1}^N z_{nj} \\left( \\frac{x_n - n_t \\mu_j}{\\mu_j (1-\\mu_j)} \\right)\n\\end{aligned}\n\\]\n마지막 식을 0으로 두고 정리한다.\n\\[\n\\sum_{n=1}^N z_{nj} \\left( \\frac{x_n - n_t \\mu_j}{\\mu_j (1-\\mu_j)} \\right)=0\n\\]\n인덱스 \\(n\\)에 관계없는 항을 합산 기호 밖으로 뽑아낸다.\n\\[\n\\frac{1}{\\mu_j (1-\\mu_j)} \\sum_{n=1}^N z_{nj} \\left(x_n - n_t \\mu_j \\right) = 0\n\\]\n그러면 합산 기호에 의한 항이 0이 되어야 하므로\n\\[\n\\sum_{n=1}^N z_{nj} \\left(x_n - n_t \\mu_j \\right) = 0\n\\]\n합산 기호를 분배하고\n\\[\n\\sum_{n=1}^N z_{nj} x_n - \\sum_{n=1}^N z_{nj} n_t \\mu_j = 0\n\\]\n이항한다.\n\\[\nn_t \\mu_j \\sum_{n=1}^N z_{nj} = \\sum_{n=1}^N z_{nj} x_n\n\\]\n좌변에 \\(\\mu_j\\)만 남기고 우변으로 넘기면\n\\[\n\\mu_j = \\frac{\\sum_{n=1}^N z_{nj} x_n }{n_t \\sum_{n=1}^N z_{nj} }\n\\]\n위 식에서 $ {n=1}^N z{nj}=N_j$이므로 최종적으로\n\\[\n\\mu_j= \\frac{\\sum_{n=1}^N z_{nj} x_n}{n_t N_j} \\tag{1}\n\\]\n구해진 식(1)은 \\(j\\)번째 분포를 따르는 샘플들의 평균으로 완전 데이터 세트에 대한 최대 가능도 해가 구해짐을 말해준다.\n이제 \\(\\pi_j\\)에 대해서 미분하여 같은 과정을 반복한다. 단 이 때는 수치적 방법에서와 마찬가지로 \\(\\pi_k\\)를 모두 더해서 1이 되어야 한다는 제약조건을 고려해야한다.\n\\[\n\\sum_{k=1}^K \\pi_k = 1\n\\]\n물론 \\(\\mu_k\\)에도 $ 0 _k $라는 제약조건이 있지만 미분할 때 반영하지 않은 이유(http://www.iro.umontreal.ca/~slacoste/teaching/ift6269/A19/ 에 lecture 4 참고) 는 제약 없이 구해진 해가 제약조건을 만족시키기 때문이다. 식(1)을 보면 구해진 최종해가 \\(\\mu_j \\in [0,1]\\)임을 알 수 있다.\n\\(\\pi_j\\)로 미분하는 과정은 이렇게 제약조건을 반영해야해서 조금 번거롭지만 그리 복잡하진 않기 때문에 직접 해보기로 하자. 라그랑지 승수lagrange multiplier \\(\\lambda\\)를 도입하고 라그랑지안lagrangian을 구성하자.\n\\[\n\\mathcal{L}(\\boldsymbol{\\theta}, \\lambda)= \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\tag{2}\n\\]\n식(2)를 \\(\\pi_j\\)에 대해서 미분하는 과정은 다음과 같다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\pi_j} & \\left\\{ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} \\\\\n&= \\frac{\\partial}{\\partial \\pi_j} \\left[  \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\}  + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K    z_{nk} \\left\\{\\frac{\\partial}{\\partial \\pi_j} \\ln \\pi_k + \\frac{\\partial}{\\partial \\pi_j} \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\right\\}  + \\lambda \\left(  \\sum_{k=1}^K  \\frac{\\partial}{\\partial \\pi_j}(\\pi_i -1) \\right) \\\\\n&= \\sum_{n}^N \\left(  \\frac{z_{nj}}{\\pi_j}\\right) + \\lambda\n\\end{aligned} \\tag{3}\n\\]\n식(2)를 \\(\\lambda\\)에 대해 미분하면 다음과 같다.\n\\[\n\\frac{\\partial}{\\partial \\lambda} \\left\\{ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} =\\sum_{i=1}^K \\pi_i -1 \\tag{4}\n\\]\n이제 식(3),(4)를 모두 0으로 두고 연립방정식을 풀어서 해를 구한다.\n\\[\n\\sum_{n}^N \\left(  \\frac{z_{nj}}{\\pi_j}\\right) + \\lambda = 0 \\tag{5}\n\\]\n\\[\n\\sum_{j=1}^K \\pi_j -1 = 0 \\tag{6}\n\\]\n식(5)으로 부터\n\\[\n\\pi_j = -\\frac{N_j}{\\lambda} \\tag{7}\n\\]\n을 얻고 식(7)을 식(6)에 대입하면 \\(\\lambda = -N\\)을 구할 수 있다. 따라서 최종적으로\n\\[\n\\pi_j = \\frac{N_j}{N} \\tag{8}\n\\]\n완전 데이터 세트에 대한 최대 가능도 추정은 식(1)과 식(8)로 주어진다. 확률분포에 대한 인덱스를 일관적으로 사용하기 위해 인덱스를 \\(k\\)로 바꾸고 다시 적으면 다음과 같다.\n\\[\n\\mu_k= \\frac{\\sum_{n=1}^N z_{nk} x_n}{n_t N_k} \\tag{9}\n\\]\n\\[\n\\pi_k = \\frac{N_k}{N} \\tag{10}\n\\]\n식(9), (10)은 최대 가능도 추정을 통해 구해진 분포의 파라미터 \\(\\mu_k\\)는 \\(k\\)번 째 분포를 따르는 샘플들의 평균이며 혼합계수 \\(\\pi_k\\)는 간단히 분포를 따르는 샘플들의 비가 됨을 알려준다.\n이 결과를 주어진 데이터에 적용해보면 10, 7, 8은 \\(\\text{Bin}(x \\mid n_t = 10, \\mu_1)\\)로 부터 나온 데이터이므로 식(9)에 의해\n\\[\n\\mu_1 = \\frac{10+7+8}{10 \\times 3}=\\frac{25}{30}= 0.833333\n\\]\n4, 3은 \\(\\text{Bin}(x \\mid n_t=10, \\mu_2)\\)로 부터 나온 데이터이므로\n\\[\n\\mu_2 = \\frac{4+3}{10 \\times 2}=\\frac{7}{20}= 0.35\n\\]\n이 된다. 혼합계수에 대해서는 식(10)에 의해 해 \\[\n\\pi_1 = \\frac{3}{5}=0.6, \\qquad \\pi_2 = \\frac{2}{5}=0.4\n\\]\n로 완전 데이터 세트의 최대 가능도 해를 구할 수 있다. 수치적으로 구한 해와 거의 같은 해가 구해지는 것을 확인할 수 있다.\n지금까지 과정을 통해 완전 데이터 세트에 대해서는 가능도 함수를 최대화 시키는 방식으로 비교적 간단하게 파라미터를 추정할 수 있다는 사실을 알았다. 하지만 문제는 우리에게 행렬 \\(\\mathbf{Z}\\)에 대한 정보가 전혀 없다는 것이다. 다시말해 우리에게 주어진 데이터는 불완전 데이터 세트incomplete data set이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#모르는-데이터를-어떻게-할-것인가",
    "href": "posts/em/em_algorithm.html#모르는-데이터를-어떻게-할-것인가",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "모르는 데이터를 어떻게 할 것인가?",
    "text": "모르는 데이터를 어떻게 할 것인가?\n\n평균이란?\n이쯤에서 잠시 머리를 식힐겸 평균에 대해서 이야기해보자. 우리는 주어진 데이터를 정리하는 개념으로 평균을 인식하는 경우가 많다. 예를 들어 10명인 반의 시험 점수 10개를 다 들여다보기 보다 평균점만 보고 그 반의 성적을 대강 짐작하는 식으로 평균을 사용하게 된다. 하지만 평균은 우리가 알고 싶은 숫자를 잘 모를 때 사용할 수도 있다. 비슷한 수준의 학생이 입학한다고 했을 때 열한 번째 학생의 성적을 모르지만 앞서 구해놓은 평균으로 예측해볼 수 있다는 것이다.\n예를 들어보자. 당신은 프로야구팀 감독이며 팀 성적이 좋아서 한국시리즈까지 진출했다. 내일 1차전이 열리는데 우리팀의 승률을 알고 싶다고 하자. 우리팀의 승률은 상대팀의 선발투수에 따라서 달라지는 것이 당연할 것이다. 수석코치에게 물어보니 시즌 전적을 바탕으로 볼 때 1선발이 등판하면 우리팀 승률은 0.45, 2선발이 등판하면 0.6이라고 한다. 모종의 이유로 1차전에 1선발이 등판할지 2선발이 등판할지 예측할 수 없다면 수석코치는 감독에게 1차전 승률을 어떻게 보고해야할까? 대부분 사람들이 당연하다는 듯이 다음처럼 평균을 구할 것이다.\n\\[\n\\frac{0.45+0.6}{2}=0.525 \\tag{11}\n\\]\n위 식은 1선발과 2선발이 등판할 확률을 모르기 때문에 같다고 임의로 결정한 결과이다. 그런데 만약 1선발이 등판할 확률이 80%라면 예측이 좀 달라져야하지 않을까?\n\\[\n0.8 \\times 0.45 + 0.2 \\times 0.6 = 0.48 \\tag{12}\n\\]\n즉 시합 열 번중에 여덟 번은 1선발이 등판하고 두 번은 2선발이 등판한다고 했을 때 평균을 구한 것이다. 만약 상대팀이 선발등판 예고를 2선발로 했다면 더 볼 것도 없이 승률은 0.6이 된다.\n\\[\n0 \\times 0.45 + 1 \\times 0.6 = 0.6 \\tag{13}\n\\]\n지금 우리가 무엇을 하고 있는지 생각해보자.\n상대팀의 선발투수 등판 상태 상태에는 1선발이 나오거나 2선발이 나오거나 두가지 상태가 있다. 다만 이 두 가지 중 어떤 상태로 결정될지는 알 수 없다. 대신 각 상태에 대한 확률을 ‘모두’ 알고 있다면 모든 상태에 대해서 평균을 계산하여 내일 승률을 예측할 수 있는 것이다. 식(11)에서는 각 상태에 50%씩 확률을 할당한 경우, 식(12)는 1선발이 등판하는 상태1에 80%, 2선발이 등판하는 상태2에 20%를 할당한 경우, 식(13)은 상태1에 0%, 상태2에 100%를 할당한 경우에 승률을 계산한 것이다.\n이렇게 알 수 없는 정보가 있는데 그 정보가 가질 수 있는 모든 상태에 대해서 확률을 할당할 수 있다면 평균을 구해서 모르는 부분을 채울 수 있는 것이다.\n\n\n전체적인 전략\n우리는 \\(\\mathbf{Z}\\)가 어떤 모양인지 모른다. 여기서 앞선 논의를 적용해보자.\n\n\\(\\mathbf{Z}\\)를 모르기 때문에 \\(\\mathbf{Z}\\)가 가능한 모든 상태를 생각해보자.\n그리고 그 상태들에 확률을 부여할 수 있다고 가정해보자.\n그러면 모든 상태에 대해서 가능도 함숫값을 구하고 그렇게 구해진 함숫값들을 해당 \\(\\mathbf{Z}\\)에 부여된 확률을 이용해서 평균낼 수 있지 않을까?\n그렇게만 할 수 있다면 그 가능도 함숫값의 평균을 최대화하는 파라미터를 찾을 수 있을 것이다.\n\n마지막 4번 문장에서 평균을 기댓값이란 용어로 바꿔보자.\n\n“그렇게만 할 수 있다면 그 가능도 함숫값의 기댓값을 최대화하는 파라미터를 찾을 수 있을 것이다.”\n\n우리가 알고 싶어하는 기댓값 최대화라는 이슈가 등장한 것이다!\n다시 정리하자. 우리에게 완전 데이터 세트가 주어져 있다면 데이터의 파라미터에 대한 가능도 함숫값을 계산할 수 있고 이를 통해 최대화를 수행할 수 있다. 하지만 불완전 데이터 세트가 주어졌기 때문에 가능도 함숫값을 계산할 수 없다. 대신 모르는 데이터에 대해서 가능한 모든 상태를 상정하고 가능도 함숫값을 구해 그것들의 평균을 계산한다. 이 평균을 최대화 해보자는 것이다.\n현재 설정된 문제에서 가능한 \\(\\mathbf{Z}\\)는 모두 \\(2^5=32\\)가지가 있다. 이제 우리에게 주어진 첫 번째 과제는 존재 가능한 모든 \\(\\mathbf{Z}\\)에 대해서 확률을 부여하는 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#불완전-데이터-세트에-대한-최대-가능도-추정",
    "href": "posts/em/em_algorithm.html#불완전-데이터-세트에-대한-최대-가능도-추정",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "불완전 데이터 세트에 대한 최대 가능도 추정",
    "text": "불완전 데이터 세트에 대한 최대 가능도 추정\n\n가능도 함수\n현재 주어진 데이터 세트가 불완전 데이터 세트라면 \\(\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\\)를 구할 수 없기 때문에 \\(\\mathbf{z}\\)를 주변화 시켜 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접구하는 전략을 써볼 수 있다. 주어진 \\(\\mathbf{X}\\)를 가장 잘 발생시킬것 같은 \\(\\boldsymbol{\\theta}\\)를 찾는 것이 최대 가능도 추정이므로 궁극적으로는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화 시키고 싶은 것이다.\n앞서 직관적인 방법으로 식(9.7)로 \\(p(x)\\)를 정의했었는데 여기서 정식으로 유도해보도록 하자. 우선 식(9.10)은 멀티누이 변수의 확률질량함수이므로 그 자체로 타당하다.\n\\[\np(\\mathbf{z}) = \\prod_{k=1}^{K} \\pi_k^{z_k} \\tag{9.10}\n\\]\n다음으로 어떤 확률분포인지 알려주는 \\(\\mathbf{z}\\)가 주어졌다면 \\(x\\)에 대한 확률분포는 주어진 이항분포 \\(K\\)개중 하나가 되므로 식(9.11)도 어렵지 않게 이해할 수 있다.\n\\[\np(x \\mid \\mathbf{z}) = \\prod_{k=1}^K \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} \\tag{9.11}\n\\]\n확률의 곱법칙에 의해 \\(p(x,\\mathbf{z})=p(\\mathbf{z})p(x \\mid \\mathbf{z})\\)이므로 다음이 성립하고\n\\[\np(x,\\mathbf{z}) = \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n이를 확률의 합법칙에 의해 \\(\\mathbf{z}\\)에 대해서 주변화 하면 \\(p(x)\\)를 얻을 수 있다.\n\\[\np(x) = \\sum_{\\mathbf{z}}  \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n위 식에서 \\(\\prod\\)에 의해 곱해지는 항 \\(K\\)개는 \\(z_k\\)가 오직 하나만 1이고 나머지는 다 0인 상태이다. 따라서 한 개 항만 살아남는다. 그런 항 \\(K\\)개를 모든 \\(\\mathbf{z}\\)에 대해서 다 더하고 있으므로 결과적으로는 \\(K\\)개 항만 더해지는 것으로 다음처럼 정리된다.\n\\[\np(x) = \\sum_{\\mathbf{z}}  \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} = \\sum_{k=1}^K \\pi_k \\text{Bin}(x \\mid n_t, \\mu_k) \\tag{9.12}\n\\]\n직관적으로 정의 했던 식(9.7)을 다시 얻을 수 있다. 굳이 힘들게 \\(p(x)\\)를 유도한 이유는 유도 과정에서 \\(\\mathbf{z}\\)를 주변화해야만 하고 그로 인해 최종 식에 \\(\\sum_{k}\\)가 등장한다는 것을 보이기 위함이다. 식(9.12)에서 \\(\\boldsymbol{\\theta}=(\\pi_1, \\pi_2, ..., \\pi_k, \\mu_1, \\mu_2, ..., \\mu_k)^{\\text{T}}\\)로 두면\n\\[\np(x \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^K \\pi_k \\text{Bin}(x \\mid n_t, \\mu_k)\n\\]\n로 쓸 수 있고, 모든 데이터를 고려하기 위해 데이터에 인덱스 \\(n\\)을 도입하면\n\\[\np(x_n \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)\n\\]\n이 된다. \\(\\boldsymbol{\\theta}\\)에 대한 가능도를 구하기 위해 모든 데이터에 대해 곱해준다.\n\\[\np(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\prod_{n=1}^N \\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)\n\\]\n이제 위 식에 로그를 적용하면\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta} )\n= \\sum_{n=1}^N \\ln \\left\\{  \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)  \\right\\} \\tag{9.14}\n\\]\n불완전 데이터 세트에 대한 로그 가능도 함수가 구해진다. 이제 \\(\\mathbf{Z}\\)를 몰라도 데이터에 대한 가능도 함숫값을 구할 수 있다.\n\n\n수치적 방법\n식(9.14)를 최대화 시키기 위해 수치적 수법을 적용해보자. 과정은 완전 데이터 세트에 대한 수치 최적화 과정과 완전히 동일하다.\n\ndef loglikelihood_X_(mu, pi, X):\n    \"\"\"\n    EQ(9.14)\n    mu     : (K,D)\n    pi     : (K,)\n    X      : (N,D)\n    --------------------------------------------\n    N,D,K  : gloval variables\n    \"\"\"\n    # N, D = X.shape[0], X.shape[1]\n\n    sigma_n = 0.0\n\n    for n in range(N):\n        sigma_k = 0.0\n\n        for k in range(K):\n            likelihood_x = binom.pmf(X[n,0], n_t, mu[k,0])\n            sigma_k += pi[k]*likelihood_x\n\n        sigma_n += np.log(sigma_k+1.0e-8)\n\n    return sigma_n\n\ndef loglikelihood_X(theta, X):\n    \"\"\"\n    theta[:K*D] : mu,     (K,D)\n    theta[K*D:] : pi,     (K,)\n    X           : (N,D)\n    \"\"\"\n    # N, D = X.shape[0], X.shape[1]\n\n    mu = theta[:K*D].reshape(K,D)\n    pi = theta[-K:]\n\n    return loglikelihood_X_(mu, pi, X)\n\n\n# 래퍼함수와 원함수 결과가 같게 나오는지 확인\nprint(mu_0, pi_0)\ntheta = np.hstack((mu_0.flatten(),  pi_0))\nprint(\"{:.6f}\".format(loglikelihood_X_(mu_0, pi_0, X)))\nprint(\"{:.6f}\".format(loglikelihood_X(theta, X)))\n\n# 로그 가능도 함수의 최대화를 마이너스 로그 가능도함수의 최소화로 바꾸기 위해\n# 보조 함수 정의\ndef negative_loglikelihood_X(theta, X):\n    return -loglikelihood_X(theta, X)\n\n[[0.4]\n [0.3]] [0.5 0.5]\n-21.484619\n-21.484619\n\n\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\nprint('Init. input:', x)\n\nres = minimize(negative_loglikelihood_X, x, args=(X,), method='SLSQP',\n               bounds=bounds, constraints=cons,\n               options={'iprint': 2, 'disp': True})\n\nprint(res)\n\nprint(\"sum pi_k:\",res.x[2:].sum())\n\nInit. input: [0.4 0.3 0.5 0.5]\n  NIT    FC           OBJFUN            GNORM\n    1     6     2.148462E+01     4.848786E+01\n    2    13     1.577919E+01     2.542786E+01\n    3    20     1.505575E+01     2.600846E+01\n    4    27     1.393468E+01     2.321400E+01\n    5    34     1.327015E+01     2.112075E+01\n    6    41     1.246911E+01     1.746856E+01\n    7    48     1.210598E+01     1.640239E+01\n    8    55     1.129506E+01     1.103584E+01\n    9    62     1.100341E+01     7.409348E+00\n   10    68     1.097328E+01     7.157866E+00\n   11    74     1.096708E+01     7.083495E+00\n   12    80     1.096662E+01     7.070962E+00\n   13    86     1.096662E+01     7.071087E+00\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 10.96661829509231\n            Iterations: 13\n            Function evaluations: 86\n            Gradient evaluations: 13\n     fun: 10.96661829509231\n     jac: array([-2.94947624e-03,  1.06954575e-03, -4.99991703e+00, -5.00010955e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 86\n     nit: 13\n    njev: 13\n  status: 0\n success: True\n       x: array([0.83809726, 0.37573062, 0.57156103, 0.42843897])\nsum pi_k: 1.0\n\n\n구해진 결과는\nx: array([0.83809726, 0.37573062, 0.57156103, 0.42843897])\n이다. 원래 파라미터 변수명으로 적어보면\n\\[\n\\mu_1 = 0.83809726, \\quad \\mu_2=0.37573062, \\quad \\pi_1=0.57156103, \\quad \\pi_2=0.42843897\n\\]\n이다. 완전 데이터 세트에서 결과와 비슷한지만 비교해보자. 첫 행은 완전 데이터 세트에 대한 결과이고 다음 행은 불완전 데이터 세트에 대한 결과이다.\n\n\\(\\mu_1 = 0.83331527, \\quad \\mu_2=0.35004671, \\quad \\pi_1=0.60016153, \\quad \\pi_2=0.39983847\\)\n$ _1 = 0.83809726, _2=0.37573062, _1=0.57156103, _2=0.42843897$\n\n어느정도 유사하게 추정된 것을 확인할 수 있다. 데이터가 더 많았다면 두 결과는 더 비슷해질것이다.\n\n\n해석적 방법\n이제 완전 데이터 세트에서 했던 것처럼 직접 미분하여 해를 찾아보자. 이 과정에서 \\(\\mathbf{z}\\)를 주변화한 것이 어떤 결과를 낳게되는지 확인할 수 있다. 다만 미분 과정에 지저분해서 계산이 꽤 성가시다. 그래서 미분 과정에 별 관심이 없다면 식(9.22)까지 바로 건너 뛰기로 하자.\n먼저 \\(\\mu_j\\)로 미분한다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n&= \\frac{\\partial}{\\partial \\mu_j}\\left[\\sum_{n=1}^N \\ln \\left\\{  \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)  \\right\\} \\right] \\\\\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\frac{\\partial}{\\partial \\mu_j} \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) \\\\\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\frac{\\partial}{\\partial \\mu_j} \\sum_{k=1}^K  \\pi_k \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_k^{x_n} (1-\\mu_k)^{(n_t - x_n)}\n\\end{aligned}\\tag{14}\n\\]\n미분하는 부분만 따로 때서 써보면\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} & \\sum_{k=1}^K  \\pi_k \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix}    \\mu_k^{x_n} (1-\\mu_k)^{(n_t - x_n)} \\\\\n&=   \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{ x_n \\mu_j^{(x_n -1)} (1-\\mu_j)^{(n_t-x_n)} - \\mu_j^{x_n}(n_t - x_n)(1-\\mu_j)^{(n_t - x_n-1)} \\right\\} \\\\[10pt]\n&= \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{ x_n \\mu_j^{x_n} \\mu_j^{-1}(1-\\mu_j)^{(n_t-x_n)}- \\mu_j^{x_n}(n_t - x_n) (1-\\mu_j)^{(n_t - x_n)}(1-\\mu_j)^{-1} \\right\\} \\\\[10pt]\n&= \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{\\frac{x_n \\mu_j^{x_n}(1-\\mu_j)^{(n_t-x_n)}}{\\mu_j} - \\frac{(n_t - x_n) \\mu_j^{x_n} (1-\\mu_j)^{(n_t - x_n)}}{(1-\\mu_j)} \\right\\}\n\\end{aligned} \\tag{15}\n\\]\n편미분 과정에서 \\(j=k\\)인 경우만 남게 되고 합산 기호는 사라진다.\n미분 결과 식(15)를 다시 식(14)에 대입하고 정리하자.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n&=\\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{\\frac{x_n \\mu_j^{x_n}(1-\\mu_j)^{(n_t-x_n)}}{\\mu_j} - \\frac{(n_t - x_n) \\mu_j^{x_n} (1-\\mu_j)^{(n_t - x_n)}}{(1-\\mu_j)} \\right\\} \\\\[5pt]\n&=\\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j  \\left\\{\\frac{x_n \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_j^{x_n}(1-\\mu_j)^{(n_t-x_n)}}{\\mu_j} - \\frac{(n_t - x_n) \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_j^{x_n} (1-\\mu_j)^{(n_t - x_n)}}{(1-\\mu_j)} \\right\\} \\\\[5pt]\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j  \\left\\{\\frac{x_n \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\mu_j} - \\frac{(n_t - x_n) \\text{Bin}(x_n \\mid n_t, \\mu_j)}{(1-\\mu_j)} \\right\\} \\\\[5pt]\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j  \\left\\{ \\frac{x_n \\text{Bin}(x_n \\mid n_t, \\mu_j)-\\mu_j n_t \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\mu_j (1-\\mu_j)} \\right\\}\\\\[5pt]\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) }   \\left\\{ \\frac{\\pi_j x_n \\text{Bin}(x_n \\mid n_t, \\mu_j)- \\pi_j \\mu_j n_t \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\mu_j (1-\\mu_j)} \\right\\} \\\\[5pt]\n&=  \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) }   \\left\\{ \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j) ( x_n -  \\mu_j n_t) }{\\mu_j (1-\\mu_j)} \\right\\} \\\\[5pt]\n&= \\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   \\frac{  x_n -  \\mu_j n_t }{\\mu_j (1-\\mu_j)}\n\\end{aligned}\n\\]\n마지막 식을 0으로 두고 정리한다.\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   \\frac{  x_n -  \\mu_j n_t }{\\mu_j (1-\\mu_j)} =0\n\\]\n인덱스 \\(n\\)에 관계없는 항을 합산 기호 밖으로 빼고\n\\[\n\\frac{  1 }{\\mu_j (1-\\mu_j)} \\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   (x_n -  \\mu_j n_t) =0\n\\]\n양변에 \\(\\mu_j (1-\\mu_j)\\)를 곱하면\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   (x_n -  \\mu_j n_t) =0\n\\]\n합산 기호를 분배하면\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   x_n - \\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot \\mu_j n_t =0\n\\]\n이므로\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot \\mu_j n_t =\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   x_n\n\\]\n가 되고 적당히 이항하면\n\\[\n\\mu_j =\\frac{1}{n_t  \\sum_{n=1}^N \\dfrac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) }}\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   x_n\n\\]\n위 식에서 앞 쪽 분수 분모의 합산항을\n\\[\nN_j=\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\tag{9.18}\n\\] 을 로 두면 최종적으로 다음과 같다.\n\\[\n\\mu_j = \\frac{1}{n_t N_j} \\sum_{n=1}^N \\left( \\frac{ \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j) } \\right) x_n \\tag{9.17}\n\\]\n이제 \\(\\pi_j\\)로 미분할 차례이다. 여기서는 완전 데이터 세트에서처럼 제약조건을 고려하여 라그랑지안을 구성하고 \\(\\pi_j\\)와 라그랑지 승수 \\(\\lambda\\)로 미분한다.\n\\[\n\\mathcal{L}(\\boldsymbol{\\theta}, \\lambda)= \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\tag{16}\n\\]\n식(16)을 \\(\\pi_j\\)에 대해서 미분하는 과정은 다음과 같다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\pi_j} & \\left\\{ \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} \\\\\n&= \\frac{\\partial}{\\partial \\pi_j} \\left[  \\sum_{n=1}^N \\ln \\left\\{  \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)  \\right\\} + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right] \\\\\n&= \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) + \\lambda\n\\end{aligned} \\tag{17}\n\\]\n식(16)를 \\(\\lambda\\)에 대해 미분하면 다음과 같다.\n\\[\n\\frac{\\partial}{\\partial \\lambda} \\left\\{ \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} =\\sum_{i=1}^K \\pi_i -1 \\tag{18}\n\\]\n이제 식(17),(18)을 모두 0으로 두고 연립방정식을 풀어서 해를 구한다.\n\\[\n\\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) + \\lambda = 0 \\tag{19}\n\\]\n\\[\n\\sum_{j=1}^K \\pi_j -1 = 0 \\tag{20}\n\\]\n식(19) 양변에 \\(\\pi_j\\)를 곱한다.\n\\[\n\\pi_j \\left[ \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) + \\lambda \\right] = 0\n\\]\n대괄호를 풀고 이항하면\n\\[\n\\pi_j  \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right)  = - \\pi_j  \\lambda\n\\]\n양변을 인덱스 \\(j\\)에 대해 합산해도 등호는 성립한다.\n\\[\n\\sum_{j=1}^K  \\pi_j  \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right)  = - \\sum_{j=1}^K \\pi_j  \\lambda \\tag{21}\n\\]\n식(21) 우변에 식(20)을 적용하면\n\\[\n\\underbrace{\\sum_{j=1}^K   \\sum_{n=1}^N \\left(  \\frac{\\pi_j  \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right)  }_N= -  \\lambda \\tag{22}\n\\]\n식(22)에서 좌변은 \\(N\\)이 된다. 직관적으로 잘 이해가 되지 않으면 잠시 후 다시 알아보도록 하자. 어쨌든 최종적으로\n\\[\n\\lambda = -N \\tag{23}\n\\]\n임을 알 수 있다. 이제 식(23)을 식(19)에 다시 대입하고 정리한다.\n\\[\n\\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) - N  = 0\n\\]\n\\(N\\)을 이항하고 양변에 \\(\\pi_j\\)를 곱하면\n\\[\n\\pi_j \\left\\{ \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) \\right\\}   = \\pi_j N\n\\]\n\\(N\\)을 다시 이항하면\n\\[\n\\pi_j = \\frac{ \\sum_{n=1}^N \\left(  \\dfrac{ \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) }{N}\n\\]\n분자를 식(9.18)로 바꿔쓰면\n\\[\n\\pi_j = \\frac{N_j}{N} \\tag{9.22}\n\\]\n를 얻을 수 있다.\n지루한 미분 과정이 마무리되었고 식(9.17), (9.18), (9.22)의 인덱스 \\(j\\)를 \\(k\\)로 바꾸고 같이 정리해보면 다음과 같은 결과를 얻게 된다.\n\\[\n\\begin{aligned}\n& \\mu_k = \\frac{1}{n_t N_k} \\sum_{n=1}^N \\left( \\frac{ \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j) } \\right) x_n  \\\\[10pt]\n& \\pi_k = \\frac{N_k}{N} \\\\[10pt]\n& \\text{where}\\,\\,\\, N_k = \\sum_{n=1}^N \\left( \\frac{ \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j) } \\right)\n\\end{aligned}\n\\]\n그런데 \\(\\mu_k\\)를 계산하는 결과를 보면 \\(\\mu_k\\)가 계산 과정에서 다시 나타나고 있다. \\(\\mu_k\\)를 구하는데 \\(\\mu_k\\)가 사용되고 있어서 닫힌형식의 해가 아님을 알 수 있다.\n\n반복법\n기껏 힘들게 미분을 해서 해석적으로 해를 찾았지만 해의 형태가 닫힌형식이 아니라는 결과를 확인했다. 왜 이런 결과가 나오게 되었을까? 그 이유는 모르는 데이터 \\(\\mathbf{z}\\)를 주변화해서 없애는 과정 때문에 필연적으로 로그 안에 합산 기호가 나타나기 때문이다. 로그가 지수족 분포함수에 직접 작용하게 되면 지수함수를 상쇄시켜 계산이 간단해지는 장점이 있는데 여기서는 합산 기호 때문에 그런 순기능이 발생하지 않았던 것이다. 그래서 미분 과정도 매우 복잡하며 얻게된 해도 쓸모없어 보이는 형태를 띄고 있는 것이다. 결국 이런 식으로 해를 찾을 수 없다는 결론에 이르게 되는데 여기서 좀 과감한 방법을 시도해보자.\n\\[\n\\begin{aligned}\n& \\color{#318CE7}{\\mu_k} \\color{black} = \\frac{1}{n_t N_k} \\sum_{n=1}^N \\left( \\frac{ \\color{#E52B50}{\\pi_k} \\color{black}{\\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_k}\\color{black}{)}}{\\sum_{j=1}^K \\color{#E52B50}{\\pi_j} \\color{black}{ \\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_j} \\color{black}{)} } \\right) x_n \\\\[10pt]\n& \\color{#318CE7}{ \\pi_k } \\color{black}{ = \\frac{N_k}{N}} \\\\[10pt]\n& \\text{where}\\,\\,\\, N_k = \\sum_{n=1}^N \\left( \\frac{ \\color{#E52B50}{\\pi_k} \\color{black}{ \\text{Bin}(x_n \\mid n_t,} \\color{#E52B50}{\\mu_k} \\color{black}{)}}{\\sum_{j=1}^K \\color{#E52B50}{\\pi_j} \\color{black}{ \\text{Bin}(x_n \\mid n_t,} \\color{#E52B50}{\\mu_j} \\color{black}{)} } \\right)\n\\end{aligned}\n\\]\n식(9.17), (9.18), (9.22)에서 적당히 파라미터에 색을 입혔다. 빨간색 파라미터들은 현재 설정된 파라미터라고 가정하고 이것을 통해 파란색 새로운 파리미터를 계산한다고 생각해보자. 그렇게 계산된 새로운 파라미터를 다시 빨간색에 대입하고 파란색 파라미터를 구해내는 식으로 진행하는 것이다.\n하지만 이런 방식은 다분히 인위적인 것이어서 반복법이 해를 점진적으로 개선해줄 것이라는 보장이 없다. 현재까지는 일종의 궁여지책으로 보여지는데 일단 실험부터 해보도록 하자.\n위 식에서 괄호로 묶인 부분을 간단히 \\(\\gamma_{nk}\\)로 다시 적고 실험해보자.\n\\[\n\\gamma_{nk}=\\frac{ \\color{#E52B50}{\\pi_k} \\color{black}{\\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_k}\\color{black}{)}}{\\sum_{j=1}^K \\color{#E52B50}{\\pi_j} \\color{black}{ \\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_j} \\color{black}{)} }   \\tag{24}\n\\]\n\nmu = mu_0\npi = pi_0\n\n# 설명처럼 스무번 정도 반복해본다.\nfor i in range(20) :\n    # E-step\n    Gamma = np.array([ pi[k]*binom.pmf(X, n_t, mu[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\n    Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n    # M-step\n    Nk = Gamma.sum(axis=0)\n    mu = ((Gamma * X).sum(axis=0) / (Nk*n_t)).reshape(-1,1)\n    pi = Nk / N\n\nprint(\"mu:\")\nprint(mu)\nprint(\"pi: \",pi)\n\nmu:\n[[0.83811241]\n [0.3757192 ]]\npi:  [0.57154992 0.42845008]\n\n\n놀랍게도 수치적으로 구한 최적해와 거의 동일한 해로 수렴하는 것을 확인할 수 있다. 왜 이렇게 되는지 아직 명확하게 이해할 수는 없지만 식(9.17)을 보고 최대한 그럴듯한 이유를 생각해보기로 하자.\n새로 정의한 \\(\\gamma_{nk}\\)는 현재 피라미터 \\(\\pi_k\\), \\(\\mu_k\\)에서 데이터 \\(x_n\\)이 \\(k\\)번째 확률분포에서 발생할것 같은 정도를 나타낸다. 식을 보면 \\(x_n\\)의 \\(k\\)번 째 확률질량 함숫값을 모든 확률분포에서 구한 확률질량 함숫값의 합으로 나누고 있다. 따라서 \\(k\\)번 째 확률분포의 \\(x_n\\)에 대한 책임값responsibility이라고 하기도 한다. 무슨 이야긴지 실제 예를 보면서 이야기해보자.\n\nmu = mu_0\npi = pi_0\n\nprint(\"Init. mu: \")\nprint(mu)\n\nGamma = np.array([ pi[k]*binom.pmf(X, n_t, mu[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\nGamma /= Gamma.sum(axis=1, keepdims=True)\n\nprint(\"Gamma: \")\nprint(Gamma)\n\nprint(\"Summation Gamma for K\")\nprint(Gamma.sum(axis=1))\n\nInit. mu: \n[[0.4]\n [0.3]]\nGamma: \n[[0.94668864 0.05331136]\n [0.55621735 0.44378265]\n [0.44620687 0.55379313]\n [0.82510466 0.17489534]\n [0.88007654 0.11992346]]\nSummation Gamma for K\n[1. 1. 1. 1. 1.]\n\n\n\\(\\gamma_{nk}\\)에서 각 행은 \\(x_n\\)에 대한 \\(k\\)번 째 분포의 책임 정도를 나타내고 있다. 예를 들어 \\(x_1\\)에 대해서는 1번 분포가 0.946정도로 책임을 지며 2번 분포는 0.053정도 책임을 진다는 것이다. \\(x_2\\)에 대해서는 1번 분포가 0.556, 2번 분포가 0.443정도 책임을 진다. 다르게 말하면 현재 파라미터 상태에서는 \\(x_1\\)은 95%정도는 1번 분포에서 나왔을것 같고 5%정도만 2번 분포에서 나왔을 것 같다는 말이 된다. 따라서 \\(\\gamma\\)의 모든 행을 \\(k\\)에 대해 다 더하면 1이 되고 그렇기 때문에 식(22)에서 좌변이 \\(N\\)이 되었던 것이다. 이렇게 다섯 개 데이터에 대해서 어느 분포에서 나왔을 것 같은지를 모두 계산했다.\n이제 식(9.17)처럼 \\(\\gamma_{nk}\\)에 \\(x_n\\)을 직접 곱하게 되면 \\(x_n\\)값을 \\(\\gamma_{nk}\\) 비율대로 쪼개서 각 분포에 할당하게 될 것이다.\n\nprint(\"Gamma*X: \")\nprint(Gamma * X)\nprint(\"Summation Gamma*X for K\")\nprint((Gamma * X).sum(axis=1))\n\nGamma*X: \n[[9.46688636 0.53311364]\n [2.22486939 1.77513061]\n [1.3386206  1.6613794 ]\n [5.77573259 1.22426741]\n [7.04061236 0.95938764]]\nSummation Gamma*X for K\n[10.  4.  3.  7.  8.]\n\n\n위 코드로 계산 해보면 \\(x_1=10\\)인데 1번 분포에 9.466…정도를 할당하고 2번 분포에 0.533… 정도를 할당하게 된다. 다른 모든 데이터도 이렇게 각 분포에 실제 값을 적당히 찢어서 할당하게 된다. 당연히 \\(k\\)에 대해서 다 더하면 실제 데이터 값이 나오게 된다. 실제라면 데이터가 이렇게 쪼개져서 할당될 수 없고 두 분포 중 한쪽으로만 (hard 하게) 배분되어야 하지만 우리는 어느 분포로 할당해야할지 모르기 때문에 적당히 그럴듯 하게 쪼개서 (soft 하게) 할당한 것이라 이해하면 된다.\n이렇게 대충 그럴것이라 생각되는 정도로 데이터를 각 분포에 할당했다면 완전 데이터 가능도에서 한 것처럼 각 분포에 할당된 데이터들을 가지고 평균을 계산해서 파라미터를 추정할 수 있는 것이다.\n\\[\n\\mu_k= \\frac{\\sum_{n=1}^N z_{nk} x_n}{n_t N_k} \\tag{9}\n\\]\n식(9)를 다시보면 \\(k\\)번 째 분포에 할당된 데이터의 총합을 데이터의 개수를 나타내는 \\(N_k\\)로 나누어 평균을 구하는 것을 알 수 있다. 추가로 \\(n_t\\)를 더 나누는 것은 이항분포가 이미 \\(n_t\\)번 베르누이 시행의 합산이기 때문이다. 불완전 데이터 세트에서 \\(N_k\\)의 의미를 따져보면 \\(k\\)번 째 분포에 할당될 것 같은 데이터의 개수이므로 3개, 4개 같이 딱 떨어지기 보다는 3.4와 같이 소수가 될 것이다.\n\nN_k = (Gamma).sum(axis=0,keepdims=True)\nprint(\"N_k: \")\nprint( N_k  )\n\nprint(\"Sum of N_k: \")\nprint(N_k.sum())\n\nprint(\"Sum of all data assigned to k-th distribution\")\nprint( (Gamma*X).sum(axis=0,keepdims=True)  )\n\nprint(\"mu after the first iteration\")\nprint( (Gamma*X).sum(axis=0,keepdims=True) / (N_k*n_t)  )\n\nN_k: \n[[3.65429405 1.34570595]]\nSum of N_k: \n5.0\nSum of all data assigned to k-th distribution\n[[25.84672129  6.15327871]]\nmu after the first iteration\n[[0.70729725 0.45725284]]\n\n\n코드를 통해 \\(N_k\\)를 찍어보면 1번 분포에 3.65개, 2번 분포에 1.34개 정도 할당을 한것을 확인할 수 있다. 물론 다 더하면 5가 된다. 이제 적당히 분리된 데이터를 분포별로 다 더하고 \\(n_t N_k\\)로 나누면 한번 반복이 완전히 완료되고 업데이트된 파라미터가 구해지게 된다.\n[[0.70729725 0.45725284]]\n한번 반복하여 업데이트 한 이 값을 눈여겨 봐두자.\n이런 분석 과정을 통해 식(9.17), (9.18), (9.22)는 현재 파라미터 상태에서 그럴듯하게 새로운 파라미터를 추정하고 있음을 어렴풋이 알 수 있다. 그리고 돌이켜보면 \\(\\gamma_{nk}\\)를 구하는 과정과 앞서 알아본 야구 감독이 1선발 등판, 2선발 등판 상황에 확률을 부여하고 승률의 평균을 구하는 과정이 꽤 닮아있다는 느낌을 받을 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#mathbfz의-사후확률",
    "href": "posts/em/em_algorithm.html#mathbfz의-사후확률",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "\\(\\mathbf{Z}\\)의 사후확률",
    "text": "\\(\\mathbf{Z}\\)의 사후확률\n앞선 글에서 느닷없이 반복법을 들먹이면서 결과적으로는 해가 수렴함을 보였다. 이번 편에서는 이 현상을 평균 관점에서 해석해보도록 하자. 1편에서 평균을 이용하는 전체적인 전략을 이야기 했었다. 다시 한번 상기해보면 다음과 같다.\n\n\\(\\mathbf{Z}\\)를 모르기 때문에 \\(\\mathbf{Z}\\)가 가능한 모든 상태를 생각해보자.\n그리고 그 상태들에 확률을 부여할 수 있다고 가정해보자.\n그러면 모든 상태에 대해서 가능도 함숫값을 구하고 그렇게 구해진 함숫값들을 해당 \\(\\mathbf{Z}\\)에 부여된 확률을 이용해서 평균낼 수 있지 않을까?\n그렇게만 할 수 있다면 그 가능도 함숫값의 평균을 최대화하는 파라미터를 찾을 수 있을 것이다.\n\n그리고 존재 가능한 모든 \\(\\mathbf{Z}\\)(우리 문제에서는 32가지)에 대해서 확률을 부여하는 것을 해결해야 한다고 했다. 우선 존재 가능한 모든 행렬 \\(\\mathbf{Z}\\)를 만들어보자.\n\ndef possible_Z(rows, N):\n    R = [rows for i in range(N)]\n    return np.array( list(itertools.product(*R)) )\n\n#R = [[1,0],[0,1]]\nR = np.eye(K)\nZs = possible_Z(R, N)\nZs.shape\n\n(32, 5, 2)\n\n\n위 함수를 이용해서 존재 가능한 모든 \\(\\mathbf{Z}\\)를 만들 수 있다. 분포가 두개인 경우 서른 두가지 (5,2) 행렬이 만들어지며 몇 가지 확인해보면 다음과 같다.\n\nprint(Zs[0])\nprint(Zs[10])\nprint(Zs[31])\n\n[[1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]]\n[[1. 0.]\n [0. 1.]\n [1. 0.]\n [0. 1.]\n [1. 0.]]\n[[0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]]\n\n\n현재 \\(\\mathbf{z}\\)에 대한 정보는 전혀 없는 상태이므로 \\(x\\)가 주어졌을 때 \\(\\mathbf{z}\\)에 대한 사후확률을 계산해야 한다. \\(x\\)가 주어졌을 때\\(z_k=1\\)일 확률을 \\(p(z_k=1 \\mid x)\\)로 쓰면 베이즈 정리에 의해 다음과 같다.\n\\[\n\\begin{aligned}\n\\gamma(z_k) = p(z_k=1 \\mid x) &= \\frac{p(z_k=1)p(x \\mid z_k =1)}{\\sum_{j=1}^K p(z_j=1)p(x \\mid z_j=1) } \\\\[5pt]\n&= \\frac{\\pi_k \\text{Bin}(x \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x \\mid n_t, \\mu_j)}\n\\end{aligned}\n\\]\n두 번째 등호를 위해 식(9.10)과 (9.11)을 사용하였다.\n\\[\np(\\mathbf{z}) = \\prod_{k=1}^{K} \\pi_k^{z_k} \\tag{9.10}\n\\]\n\\[\np(x \\mid \\mathbf{z}) = \\prod_{k=1}^K \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} \\tag{9.11}\n\\]\n우리 문제에서 \\(x\\)는 \\(N\\)개가 있으므로 \\(x\\)에 인덱스 \\(n\\)을 도입하면\n\\[\n\\begin{aligned}\n\\gamma(z_{nk}) = p(z_{nk}=1 \\mid x_n) &= \\frac{p(z_{nk}=1)p(x_n \\mid  z_{nk} =1)}{\\sum_{j=1}^K p(z_{nj}=1)p(x_n \\mid z_{nj}=1) } \\\\[5pt]\n&= \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}\n\\end{aligned}\n\\]\n가 되므로 식(22)에서 구했던 \\(\\gamma_{nk}\\)가 \\(\\mathbf{z}\\)의 사후확률임을 알 수 있다. 따라서 벡터 변수 \\(\\mathbf{z}\\)에 대해서는 식(9.10)에 의해\n\\[\np(\\mathbf{z}_n \\mid \\mathbf{x}_n)= \\prod_{k=1}^K \\left\\{ \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right\\}^{z_{nk}}\n\\]\n가 된다.\n이제 모든 데이터를 고려한 \\(\\mathbf{Z}\\)에 대한 \\(\\mathbf{X}\\)를 조건으로 하는 사후확률은 \\(x\\)를 독립적으로 샘플링했다는 가정하에서 다음처럼 쓸 수 있다.\n\\[\np(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) = \\prod_{n=1}^N p( \\mathbf{z}_n \\mid x_n, \\boldsymbol{\\theta}) \\tag{25}\n\\]\n앞서 얻은 결과를 이용하면 최종적으로 다음과 같다.\n\\[\n\\begin{aligned}\np(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})\n&= \\prod_{n=1}^N \\prod_{k=1}^K \\left\\{ \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right\\}^{z_{nk}} \\\\\n&=  \\prod_{n=1}^N \\prod_{k=1}^K \\gamma_{nk}^{z_{nk}}\n\\end{aligned} \\tag{26}\n\\]\n식(26)을 이용하면 모든 \\(\\mathbf{Z}\\) 행렬에 대해서 확률을 부여할 수 있다. 조금 후 자주 사용할 수식이기 때문에 우선 아래 코드로 구현해 놓는다.\n\ndef set_posterior_Z(X, theta_old):\n    X = X\n    theta_old = theta_old\n\n    def posterior_Z(Z):\n        \"\"\"\n        EQ(26)\n        Z : (N,K)\n        X : (N,D)\n        theta_old: (4,), (mu_1, mu_2, pi_1, pi_2)\n        --------------------------------------\n        N,D,K : gloval variables\n        \"\"\"\n        mu = theta_old[:K*D].reshape(K,D)\n        pi = theta_old[-K:]\n\n        Gamma = np.array([ pi[k]*binom.pmf(X, n_t, mu[k][0])\n                         for k in range(K) ]).transpose(1,0,2).squeeze()\n\n        Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n        return Gamma[Z==1].prod()\n\n    return posterior_Z\n\n# 현재 X와 theta_old를 조건으로 하는 Z의 확률분포를 세팅한다.\ntheta_old = np.hstack((mu_0.flatten(), pi_0))\nposterior_Z = set_posterior_Z(X, theta_old)\n\n코드가 약간 복잡한데 조건으로 주어진 \\(\\mathbf{X}\\), \\(\\boldsymbol{\\theta}\\)를 함수 인자로 받지 않도록 하기 위해 클로져를 사용해서 함수를 만들어 리턴한다. 이제 set_posterior_Z()를 호출해서 사후확률을 구해주는 함수를 돌려 받으면 \\(\\mathbf{Z}\\)만 넘겨서 확률분포함수의 함숫값을 간편하게 구할 수 있다.\n\n# 이제 특정 Z에 대한 확률분포함숫값을 구할 수 있다.\nposterior_Z(Zs[0])\n\n0.17061508495234065"
  },
  {
    "objectID": "posts/em/em_algorithm.html#완전-데이터-세트-가능도-함수의-평균",
    "href": "posts/em/em_algorithm.html#완전-데이터-세트-가능도-함수의-평균",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "완전 데이터 세트 가능도 함수의 평균",
    "text": "완전 데이터 세트 가능도 함수의 평균\n이제 식(26)을 이용하여 완전 데이터 세트에 대한 가능도의 \\(\\mathbf{Z}\\)에 대한 평균 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)을 구할 수 있다. 구체적으로 계산하기 전에 개념적으로 앞선 반복법에서 고정하는 파라미터(빨간색)와 조정하는 파리미터(파란색)로 구분한 이유를 잠시 알아보기로 하자.\n완전 데이터 세트의 가능도 함수 \\(\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\\)의 평균을 \\(\\mathbf{Z}\\)에 대해서 구하려면 \\(\\mathbf{Z}\\)에 대한 확률분포를 사용하여 다음처럼 하면 된다.\n\\[\n\\sum_{\\mathbf{Z}} p\\left(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}\\right) \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\tag{27}\n\\]\n식(27)을 최대화 하려면 \\(p\\left(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}\\right)\\)의 \\(\\boldsymbol{\\theta}\\)는 고정하고 \\(\\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})\\)의 \\(\\boldsymbol{\\theta}\\)를 조절해야 할 것이다. 식(27)을 \\(\\mathcal{Q}\\)라는 함수로 다시 적으면 다음과 같다.\n\\[\n\\mathcal{Q}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}\\right) = \\sum_{\\mathbf{Z}} p\\left(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}\\right) \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\tag{9.30}\n\\]\n식(9.30)에서 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)는 \\(\\mathbf{Z}\\)의 사후확률분포를 구하기 위한 고정된 현재 파라미터이고, \\(\\boldsymbol{\\theta}\\)는 \\(\\mathcal{Q}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}\\right)\\)를 최대화 하기 위해 우리가 조정하는 파라미터이다. 이렇게 완전 데이터 세트 가능도 함수의 평균을 구하는 과정에서 이미 지정된 파라미터와 최적화 될 파라미터가 자연스럽게 구분되게 된다. 이것으로 반복법에서 변수를 느닷없이 두 부류로 나눈 것이 \\(\\mathbf{Z}\\)의 사후확률을 구하는 것과 완전 데이터 세트의 가능도 함수 평균을 최대화하는 과정과 관계 있다는 것을 어렴풋이 알 수 있다.\n\n수치적 방법: 반복법에 대한 당위성\n앞서 변수를 두 부류로 나눈것에 대한 이유를 대강 알아보았으니 이번에는 왜 반복법을 사용했는가에 대한 이유를 알아보자. 식(9.30)을 최대화 시키는 \\(\\boldsymbol{\\theta}\\)를 찾으면 되므로 여기서도 수치적 방법을 동원해서 해를 찾아보자. 우선 \\(\\mathcal{Q}\\) 함수를 준비한다. 이미 모든 부속 함수를 만들어 놓았으므로 쉽게 코딩할 수 있다.\n\ndef Q(theta, theta_old):\n    \"\"\"\n    theta     : (4,), (mu_1, mu_2, pi_1, pi_2)\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    -------------------------------------------\n    gloval variables\n    X  : (N,D)\n    Zs : (K^N, N, K)\n    N,D,K\n    \"\"\"\n    # K = Zs.shape[2]\n\n    p_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\n    ret = 0.0\n    for Z_i in Zs : # 모든 Z에 대해서 평균을 낸다.\n        # eq (9.30)\n        ret += p_Z_given_X_and_theta_old(Z_i)*loglikelihood_XZ(theta, X, Z_i)\n\n    return ret\n\n# scipy.optimize.minimize 사용을 위해 -Q로 만들어 놓는다.\ndef negative_Q(theta, theta_old):\n    return -Q(theta, theta_old)\n\n이제 초기값을 설정하고 이전에 만들어 둔 제약조건을 사용해서 최적화 함수를 호출한다.\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\ntheta_old = np.copy(theta)\nprint('Init. input:', x)\n\nres = minimize(negative_Q, x, args=(theta_old), method='SLSQP',\n               bounds=bounds, constraints=cons,\n               options={'iprint': 2, 'disp': True})\n\nprint(res)\n\nprint(\"sum pi_k:\",res.x[2:].sum())\n\nInit. input: [0.4 0.3 0.5 0.5]\n  NIT    FC           OBJFUN            GNORM\n    1     6     2.389727E+01     4.848931E+01\n    2    13     1.906314E+01     3.115227E+01\n    3    20     1.754121E+01     2.277992E+01\n    4    27     1.648351E+01     1.696680E+01\n    5    34     1.624252E+01     1.531351E+01\n    6    41     1.595074E+01     1.074425E+01\n    7    47     1.570851E+01     9.368190E+00\n    8    53     1.556181E+01     6.907221E+00\n    9    59     1.555421E+01     7.045966E+00\n   10    65     1.555393E+01     7.073587E+00\n   11    71     1.555392E+01     7.071202E+00\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 15.553919769048395\n            Iterations: 11\n            Function evaluations: 71\n            Gradient evaluations: 11\n     fun: 15.553919769048395\n     jac: array([-1.69980526e-03,  4.49538231e-04, -4.99988914e+00, -5.00030005e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 71\n     nit: 11\n    njev: 11\n  status: 0\n success: True\n       x: array([0.70728766, 0.45726058, 0.73087497, 0.26912503])\nsum pi_k: 1.0\n\n\n성공적으로 최적화가 수행되었고 구해진 해는 아래와 같다.\nx: array([0.70728766, 0.45726058, 0.73087497, 0.26912503])\n1편과 2편에서 구한 수치해와는 조금 차이가 있다. 첫 두 파라미터는 0.8, 0.45 근처값이 최적해인데 구해진 해는 0.707, 0.457로 최적해로 수렴하다 만것같은 느낌이 든다. 이런 결과가 나온 이유는 여기서 우리가 최적화 시킨 함수는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)이 아니고 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)이기 때문이다. 가능도 함수를 직접 최적화 시키지 못하고 그 평균을 최적화 시켰기 때문에 우리가 원하는 최적점까지 가지 못한 것이다.\n2편에서 반복법을 설명할 때 한 번 반복 후 업데이트 된 해를 눈여겨 봐두자고 했었는데 구해진 해가 그 해와 아주 비슷한 것을 알 수 있다. 2편 반복법에서 첫 번째로 업데이트된 해를 다시보자.\n[[0.70729725 0.45725284]]\n거의 같은 해라는 것을 알 수 있을 것이다. 이는 우연이 아니며 나중에 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 직접 미분하여 구한 해가 식(9.17), (9.22)와 일치하는 것을 알아보도록 할것이다.\n그렇기에 구해진 해를 theta_old로 재설정하고 최적화를 다시 수행하기를 반복하면 원하는 최적해로 수렴할 수 있을 것 같다는 예상을 할 수 있다.\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\ntheta_old = np.copy(theta)\nprint('Init. input:', x)\n\nfor i in range(20) :\n    res = minimize(negative_Q, x, args=(theta_old), method='SLSQP',\n                bounds=bounds, constraints=cons,\n                options={'iprint': 0, 'disp': True})\n\n    if i % 5 == 0:\n        print(res.x)\n\n    x = np.copy(res.x)\n    theta_old = np.copy(res.x)\n\nInit. input: [0.4 0.3 0.5 0.5]\n[0.70728766 0.45726058 0.73087497 0.26912503]\n[0.8375278  0.37398245 0.57384792 0.42615208]\n[0.83806643 0.37565554 0.57195203 0.42804797]\n[0.83811421 0.37567489 0.57181128 0.42818872]\n\n\n결과를 보면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접 미분하여 구한 결과인 식(9.17), (9.18), (9.22)를 반복적으로 적용하여 구한 해로 수렴하는 듯 보인다.\n지금까지 과정을 다시한번 정리해보자.\n\n2편에서는 \\(\\mathbf{z}\\)에 대한 정보가 없기 때문에 \\(\\mathbf{z}\\)를 주변화 시켜서 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 구하고 이를 직접 수치적으로 또 해석적으로 최적화 시켰다.\n해석적인 과정에서 구해진 해는 닫힌 형식이 아니라서 궁여지책으로 반복법을 제안하였고 신기하게도 그 방법이 수치해로 수렴하는 것을 확인하였다.\n이번에는 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 계산하고 이를 최적화하였다. \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 유도하는 과정에서 고정해야하는 파라미터와 조절해야하는 파라미터를 자연스럽게 분리할 수 있었다.\n3번에서 최적화 결과는 만족스럽지 못했는데 2번에서 반복법을 여기서도 그대로 적용해보니 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)에 대한 최적해가 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 최적해로 수렴하는 듯 보였다.\n\n현재까지 실험으로 미뤄보면 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 반복적으로 최대화 시킨 결과가 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 최적해로 수렴한다고 결론 내릴 수 있다. 우리의 궁극적인 목적은 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화 시키는 것이기 때문에 결국 목적이 달성된 것이다.\n\n\n해석적 방법\n1, 2편과 마찬가지로 이번에는 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)에 대한 해석적 방법을 고려해보자. 그러기 위해서 먼저 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 우리 문제에 대해서 목적함수로 정식화 해야 한다. 그 후 정식화된 목적함수를 미분할 것이다.\n\n목적함수 정식화\n이제 식(9.36)에 기대값을 취하면 다음과 같다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{Z}} \\left[  \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\n&= \\mathbb{E}_{\\mathbf{Z}} \\left[ \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}_{\\mathbf{Z}} [ z_{nk} ] \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\}\n\\end{aligned}\n\\]\n위 식에서 \\(\\mathbb{E}_{\\mathbf{Z}} [ z_{nk} ]\\) 부분을 풀어야 하는데 이를 위해 \\(x_n\\)이 독립적으로 샘플링 되었다는 독립성 가정에 의해 앞서 유도한 \\(\\mathbf{Z}\\)의 사후확률분포인 다음식을 보자.\n\\[\np(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) = \\prod_{n=1}^N p( \\mathbf{z}_n \\mid x_n, \\boldsymbol{\\theta}) \\tag{25}\n\\]\n식(25)는 인덱스 \\(n\\)에 대해 단순히 곱해진 식이므로 행렬 \\(\\mathbf{Z}\\)에 대한 확률분포를 구성하는 행벡터 \\(\\mathbf{z}_n\\)들은 모두 독립이다. 이런 이유로 \\(\\mathbf{Z}\\)의 사후확률분포에서 \\(z_{nk}\\)의 기댓값은 \\(z_{nk}\\)가 속하는 벡터 \\(\\mathbf{z}_n\\)에 대해서만 기대값을 고려하면 된다. 이를 확률변수 \\(X\\)와 \\(Y\\)가 결합되어 있을 때 결합확률분포에서 \\(X\\)에 대한 기댓값을 구하는 경우를 예를 들어 생각해보면 이해가 쉬워진다. 결합확률분포 \\(p(x, y)\\)에 대해서 확률변수 \\(X\\)의 기댓값은 다음처럼 쓸 수 있다.\n\\[\n\\mathbb{E}_{p(x,y)}[X] = \\sum_i \\sum_j x_i p(x_i, y_i)\n\\]\n그런데 \\(X\\), \\(Y\\)가 독립이면 다음처럼 된다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{p(x,y)}[X] &= \\sum_i \\sum_j x_i p(x_i, y_i) \\\\\n&= \\sum_i \\sum_j x_i p(x_i)p(y_i) \\\\\n&= \\sum_i x_i p(x_i) \\sum_j p(y_j) \\\\\n&= \\mathbb{E}_{p(x)}[X]\n\\end{aligned}\n\\]\n결국 \\(p(x)\\)에 대한 \\(X\\)의 기댓값이 된다. 마찬가지로 \\(z_{nk}\\)의 기댓값은 다음처럼 벡터 \\(\\mathbf{z}_n\\)의 합산에 의해 주어지게 된다. 다시말해 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})\\)에 대한 기댓값에서 \\(p( \\mathbf{z}_n \\mid x_n, \\boldsymbol{\\theta})\\)에 대한 기댓값으로 바뀌게 되었다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ]\n&= \\sum_{\\mathbf{z}_n}  z_{nk} \\cdot p(\\mathbf{z}_n \\mid x_n) \\\\[5pt]\n&= \\sum_{\\mathbf{z}_n}  z_{nk} \\frac{p(x_n, \\mathbf{z}_n)}{p(x_n)} \\\\[5pt]\n&= \\sum_{\\mathbf{z}_n}   \\frac{z_{nk} \\cdot p(x_n, \\mathbf{z}_n)}{p(x_n)} \\\\[5pt]\n&= \\frac{ \\sum_{\\mathbf{z}_n}  z_{nk} \\cdot p(x_n, \\mathbf{z}_n)}{ \\sum_{\\mathbf{z}_n}  p(x_n, \\mathbf{z}_n)} \\\\\n\\end{aligned} \\tag{9.39-1}\n\\]\n위 식에서 \\(\\sum_{\\mathbf{z}_n}\\)은 \\(\\mathbf{z}_n\\) 벡터가 가질 수 있는 모든 벡터들에 대해서 합산하라는 뜻이다. 데이터 \\(x_n\\)하나에 대응되는 \\(\\mathbf{z}_n\\) 벡터 \\(N\\)개에 대해서 합산하라는 뜻이 아님을 주의해야 한다. 예를들어 \\(K=2\\)이면 \\(\\mathbf{z}_n = (0,1)^{\\text{T}}\\), \\(\\mathbf{z}_n = (1,0)^{\\text{T}}\\)인 두 경우에 대해서 합산하라는 뜻이다. \\(p(x_n, \\mathbf{z}_n)\\)은 아래와 같으므로\n\\[\np(x_n, \\mathbf{z}_n) = \\prod_{k=1}^K \\left[ \\pi_k \\text{Bin}(x_n \\mid \\mu_k) \\right]^{z_{nk}}\n\\]\n위 식을 대입하면\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ]\n&= \\frac{ \\sum_{\\mathbf{z}_n}  z_{nk} \\cdot p(x_n, \\mathbf{z}_n)}{ \\sum_{\\mathbf{z}_n}  p(x_n, \\mathbf{z}_n)} \\\\[5pt]\n&= \\frac{ \\sum_{\\mathbf{z}_n} z_{nk} \\prod_{k'=1}^K [\\pi_{k'} \\text{Bin}(x_n \\mid \\mu_{k'})]^{z_{nk'}}}{ \\sum_{\\mathbf{z}_n} \\prod_{j=1}^K [\\pi_j \\text{Bin}(x_n \\mid \\mu_j)]^{z_{nj}} }\n\\end{aligned} \\tag{9.39-2}\n\\]\n가 된다. 식(9.39-2)에서 분모를 보자. 모든 \\(\\mathbf{z}_n\\)에 대해서 \\(\\prod_{j=1}^K\\)를 계산하고 있는데 \\(\\mathbf{z}_n\\)은 오직 한 자리만 1인 원핫 벡터이므로 결국 분모는 \\(\\sum_{j=1}^K\\)가 된다. 분자도 같은 이유로 \\(K\\)항이 합산되는데 \\(z_{nk}\\)가 곱해지기 때문에 \\(k\\)번 째 항만 남고 나머지는 0이 곱해져 다 사라지게 된다. 따라서 최종적으로 다음과 같다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ]\n&= \\frac{ \\sum_{\\mathbf{z}_n} z_{nk} \\prod_{k'=1}^K [\\pi_{k'} \\text{Bin}(x_n \\mid \\mu_{k'})]^{z_{nk'}}}{ \\sum_{\\mathbf{z}_n} \\prod_{j=1}^K [\\pi_j \\text{Bin}(x_n \\mid \\mu_j)]^{z_{nj}} } \\\\[5pt]\n&= \\frac{\\pi_k \\text{Bin}(x_n \\mid \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid \\mu_j)} = \\gamma(z_{nk})\n\\end{aligned} \\tag{9.39}\n\\]\n주어진 결과는 이전에 계산한 책임값 \\(\\gamma_{nk}\\)가 된다.\n이제 식(9.36)에 기대값을 취해 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 직접 구할 수 있다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{Z}} \\left[  \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\n&= \\mathbb{E}_{\\mathbf{Z}} \\left[ \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}_{\\mathbf{Z}} [ z_{nk} ] \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ] \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\gamma(z_{nk}) \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\}\n\\end{aligned} \\tag{9.40}\n\\]\n식(9.40)에서 기댓값의 선형성이 사용되었다. 아래 코드로 구현 해두도록 하자.\n\ndef E_Z_loglikelihood_XZ(theta, X) :\n    \"\"\"\n    EQ (9.40)\n    theta     : (4,), (mu_1, mu_2, pi_1, pi_2)\n    -------------------------------------------\n    gloval variables\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    N,D,K\n    \"\"\"\n    # N, D = X.shape[0], X.shape[1]\n\n    mu = theta[:K*D].reshape(K,D)\n    pi = theta[-K:]\n\n    mu_old = theta_old[:K*D].reshape(K,D)\n    pi_old = theta_old[-K:]\n\n    # 전역변수 theta_old로 계산하는 것 주의!!\n    Gamma = np.array([ pi_old[k]*binom.pmf(X, n_t, mu_old[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\n    Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n    binom_x_given_mu_k = np.array([ binom.pmf(X, n_t, mu[k][0])\n                                for k in range(K) ]).transpose(1,0,2).squeeze()\n\n    return (Gamma*(np.log(pi+1.0e-8).reshape(1,-1)\n            + np.log(binom_x_given_mu_k+1.0e-8))).sum()\n\n식(9.30)과 (9.40)은 궁극적으로 같은 식이다. 아래 코드로 두 함수의 값을 찍어보면 같은 값이 찍히는 것을 확인할 수 있다.\n\ntheta = np.hstack((mu_0.flatten(), pi_0))\ntheta_old = theta\n\nprint( \"Q(theta, theta_old) : {:.6f}\".format(Q(theta, theta_old)) )\nprint( \"Ez[ln p(X,Z|theta)] : {:.6f}\".format(E_Z_loglikelihood_XZ(theta,  X)) )\n\nQ(theta, theta_old) : -23.897270\nEz[ln p(X,Z|theta)] : -23.897270\n\n\n이제 정식화가 끝났으니 미분하여 0으로 두고 최적해를 찾는 일만 남았다.\n\n\n목적함수 미분\n식(9.36)과 식(9.40)에서 차이점은 \\(z_{nk}\\)가 \\(\\gamma(z_{nk})\\)로 변한것 밖에 없으므로 식(9.40)을 \\(\\mu_k\\)로 미분하고 0으로 두어 최적해를 구하는 과정은 식(9.36)에서 계산한 것과 동일하며 최종적으로 다음처럼 될것이다.\n\\[\n\\mu_k = \\frac{\\sum_{n=1}^N \\gamma(z_{nk}) x_n }{n_t \\sum_{n=1}^N \\gamma(z_{nk}) } \\tag{28}\n\\]\n\\(\\gamma(z_{nk})\\)는 다음과 같으므로\n\\[\n\\gamma(z_{nk}) = \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}\n\\]\n식(28)에 대입하면\n\\[\n\\mu_k = \\frac{\\sum_{n=1}^N \\left( \\dfrac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right) x_n }{n_t \\sum_{n=1}^N \\left( \\dfrac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right) }  \\tag{29}\n\\]\n가 된다. 식(29)는 식(9.17)과 정확히 동일한 식이다.\n\\(\\pi_k\\)로 미분하는 과정도 역시 동일하게 식(5), (6)과 유사한 다음 두식을 얻을 수 있다.\n\\[\n\\sum_{n}^N \\left(  \\frac{\\gamma(z_{nk})}{\\pi_k}\\right) + \\lambda = 0 \\tag{30}\n\\]\n\\[\n\\sum_{j=1}^K \\pi_j -1 = 0 \\tag{31}\n\\]\n식(30), (31)을 연립해서 풀면\n\\[\n\\pi_k = \\frac{N_k}{N} \\tag{32}\n\\]\n를 얻을 수 있는데 이 역시 식(9.22)와 동일한 식이다.\n이 과정을 통해 알 수 있는 사실은 식(9.17), (9.22)와 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 미분하여 얻은 식 (29), (32)는 완전히 동일하다는 점이고, 더욱 중요한 사실은 두 식들이 모양은 같지만 두 번째 구한 식(29), (32)들은 닫힌 형식의 해라는 점이다. 왜냐하면 식(29), (32)에서 우변에 나타난 \\(\\mu_k\\), \\(\\pi_k\\)들은 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 구성할 때 미리 지정한 파라미터들이기 때문이다.\n그렇기 때문에 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 미분하여 얻은 최적해인 식(9.17), (9.22)를 통해서는 원칙적으로 어떤 해도 구할 수 없었지만 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 미분하여 찾은 최적해는 그것이 비록 만족스런 최적해가 아닐지라도 적어도 계산은 가능해 진 것이다. 즉 \\(\\mu_k\\), \\(\\pi_k\\)를 임의로 지정하고 식(29), (32)를 통해 개선된 파라미터를 계산할 수 있는 것이다. 하지만 이 한번의 업데이트를 통해서 만족스런 해에 도달할 수 없다는 것을 앞선 수치과정으로 확인했다. 아울러 만족스런 해를 얻기 위해서는 식(29), (32)를 반복적으로 시도해야 한다는 사실도 앞선 수치적 방법을 통한 실험으로 알 수 있었다.\n이제 우리에게 남은 마지막 의문은 왜 반복법을 사용하면 해가 수렴하는가 하는 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#반복법이-수렴하는-이유",
    "href": "posts/em/em_algorithm.html#반복법이-수렴하는-이유",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "반복법이 수렴하는 이유",
    "text": "반복법이 수렴하는 이유\n2편과 3편에 걸쳐 제안하고 실험한 반복법의 해가 $ ( )$의 최대값으로 수렴하는 이유를 알아보기 위해 다음 식(9.70)과 같은 분해를 알아보자.\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})= \\mathcal{L}(q, \\boldsymbol{\\theta}) + KL(q \\,||\\, p) \\tag{9.70}\n\\]\n식 (9.70)에서 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)가 무엇인지 유도하기 위해\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\ln \\sum_{\\mathbf{Z}}  p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\n\\]\n에서 우변에 먼저 임의의 \\(q(\\mathbf{Z})\\)를 도입하자.\n\\[\n\\begin{aligned}\n\\ln \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\n&= \\ln \\left\\{ \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\\\[5pt]\n& \\ge \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\qquad \\because \\text{Jensen's inequality}\\\\[5pt]\n&=  \\mathcal{L}(q, \\boldsymbol{\\theta})\n\\end{aligned}\n\\]\n위 식에서 부등식은 옌센 부등식으로 로그 함수는 오목함수라서 \\(\\mathbb{E}[f(x)] \\le f(\\mathbb{E}[x])\\)가 되어서 성립한다. \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 다음처럼 정의되고 정의 그 자체로 최적화 하고자 하는 파라미터를 조건으로 하는 데이터의 로그 가능도 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 하한이 됨을 알 수 있다.\n\\[\n\\mathcal{L}(q, \\boldsymbol{\\theta}) = \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\tag{9.71}\n\\]\n계속해서 식(9.70)을 완성해보자. 완전 데이터 데이터 세트의 확률분포는\n\\[\np(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})  = p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n\\]\n이며, 로그를 취하면\n\\[\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})  = \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\tag{9.73}\n\\]\n식(9.73)을 식(9.71)에 대입하면\n\\[\n\\begin{aligned}\n\\mathcal{L}(q, \\boldsymbol{\\theta}) &= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\left\\{ \\frac{p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) - \\ln q(\\mathbf{Z})\\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})  - \\ln q(\\mathbf{Z}) \\right\\} \\qquad \\because \\text{eq(9.73)} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) +  \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) - \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln q(\\mathbf{Z}) \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) - \\ln q(\\mathbf{Z}) \\right\\} + \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{\\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})}{\\ln q(\\mathbf{Z}) } \\right\\} + \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\\\[5pt]\n&= -KL(q \\,||\\, p) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\qquad \\because \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) = 1 \\\\[5pt]\n&= -KL(q \\,||\\, p) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n\\end{aligned}\n\\]\n\\(-KL(q \\, || \\, p)\\)를 이항하면 식(9.70)이 완성된다. 따라서 식(9.70)에서 \\(KL(q \\, || \\, p)\\)는 식(9.72)와 같다.\n\\[\nKL(q \\,||\\, p) = - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})}{ q(\\mathbf{Z}) } \\tag{9.72}\n\\]\n아니면 대수적으로 아래처럼 \\(\\ln (\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접 분해할 수 도 있다.\n\\[\n\\begin{aligned}\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})  \\qquad \\because \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) = 1\\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})} \\qquad \\because p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})  = p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\left\\{ \\frac{p(\\mathbf{Z},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})}  \\frac{q(\\mathbf{Z})}{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})} \\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln \\frac{p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} + \\ln \\frac{q(\\mathbf{Z})}{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})} \\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{X},\\mathbf{z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} + \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{q(\\mathbf{Z})}{p(\\mathbf{z}\\,|\\,\\mathbf{x},\\boldsymbol{\\theta})} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{X},\\mathbf{Z}\\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{Z},\\boldsymbol{\\theta})}{ q(\\mathbf{Z}) } \\\\[5pt]\n&= \\mathcal{L}(q, \\boldsymbol{\\theta}) + KL(q \\,||\\, p)\n\\end{aligned} \\tag{9.70}\n\\]\n정리를 위해 식(9.70), (9.71), (9.72)를 함께 다시 써보면\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})= \\mathcal{L}(q, \\boldsymbol{\\theta}) + KL(q \\,||\\, p) \\tag{9.70}\n\\]\n\\[\n\\mathcal{L}(q, \\boldsymbol{\\theta}) = \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\tag{9.71}\n\\]\n\\[\nKL(q \\,||\\, p) = - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})}{ q(\\mathbf{Z}) } \\tag{9.72}\n\\]\n위 식은 \\(\\mathbf{X}\\)에 대한 로그 가능도 함수가 임의로 선택된 \\(q(\\mathbf{Z})\\)에 대해서 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(KL(q \\,||\\, p)\\)로 분해됨을 알려준다. \\(q(\\mathbf{Z})\\)란 \\(\\mathbf{Z}\\)에 대한 분포로 우리는 \\(\\mathbf{Z}\\)에 대해 아는 것이 없으므로 임의로 선택해야 하는 분포이다. 그런데 \\(q(\\mathbf{Z})\\)를 잘 선택하면 \\(KL(q \\,||\\, p)=0\\)으로 만들어 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})=\\mathcal{L}(q, \\boldsymbol{\\theta})\\) 로 만들 수 있다. 바로 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})\\)로 선택하면 식(9.72)에서 \\(KL(q \\,||\\, p)=0\\)이 됨을 알 수 있다. 식(9.70)을 구체적으로 시각화 해보자. 그림을 그리기위해 현재 우리가 가지고 있는 파라미터를 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)라 표시하고 설정해둔다.\n\ntheta_old = np.hstack((mu_0.flatten(), pi_0))\n\n\\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 하한 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(\\text{KL}(q\\,||\\,p)\\)로 나눠 그리기 위해서 식(9.71)을 로그 성질을 이용하여 풀어 적어보면\n\\[\n\\mathcal{L}(q, \\boldsymbol{\\theta}) = \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln q(\\mathbf{Z}) \\tag{33}\n\\]\n위 식에서 두 번째 항은 \\(q(\\mathbf{Z})\\)의 엔트로피라는 것을 알 수 있다. 먼저 두번째 항을 계산하기 위해 임의의 \\(q(\\mathbf{Z})\\)를 넘겨받아 엔트로피를 계산하는 함수를 만든다.\n\ndef entropy_Z(q) :\n    \"\"\"\n    주어진 확률 분포 q(Z)에 대해서 엔트로피를 계산한다.\n    q : Z를 받아서 확률분포를 계산하는 임의의 함수\n    \"\"\"\n    entropy = 0.0\n\n    for Z_i in Zs :\n        pz = q(Z_i)\n        entropy += pz*np.log(pz+1.0e-8)\n\n    return entropy\n\n\\(\\mathbf{Z}\\)에 대한 분포를 임의로 만들어 엔트로피를 계산해본다.\n\ndef dummy_q(*Z):\n    \"\"\"\n    Z에 대해 확률분포를 계산하는 더미함수\n    여기서는 모든 Z에 균등한 함숫값을 계산\n    gloval variables\n    Zs : (K^N, N, K)\n    \"\"\"\n    return 1. / Zs.shape[0]\n\n# 아무 의미없는 q(Z) 테스트\ndummy_q(Zs[1])\n\n0.03125\n\n\n엔트로피를 계산하면 잘 계산되는 것을 확인할 수 있다.\n\nentropy_Z(dummy_q)\n\n-3.4657355827997796\n\n\n\\(q(\\mathbf{Z})\\)를 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)로 제안받아 엔트로피를 계산해보자.\n\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\nentropy_Z(p_Z_given_X_and_theta_old)\n\n-2.4126501017806823\n\n\n이제 부속이 모두 준비되었으므로 \\(q(\\mathbf{Z})\\)를 제안받아 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 계산하는 함수를 만든다. \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 로그가능도의 하한lower bound라는 의미로 ELBO(Evidence Lower BOund)라고 하기도 한다.\n\n# Evidence Lower BOund, abbreviated as ELBO\ndef ELBO(q, theta) :\n    \"\"\"\n    global variables\n    X         : (N,D)\n    Zs        : (K^N, N, K)\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    K\n    \"\"\"\n    E = 0.0\n\n    for Z_i in Zs :\n        E += q(Z_i)*loglikelihood_XZ(theta, X, Z_i)\n\n    return E - entropy_Z(q)\n\nELBO(q, theta)를 사용하여 식(9.70)을 시각화해보기 위해 첫 번째로 \\(q(\\mathbf{Z})\\)를 dummy_q()로 두고 그림을 그려보자.\n\n# 제안분포를 dummy_q()로 선택하여 ELBO를 계산\nl_q_theta_dummy = ELBO(dummy_q, theta_old)\nln_p_X_theta = loglikelihood_X(theta_old, X)\nprint(\"l(q,theta)    : {:.6f}\".format(l_q_theta_dummy))\nprint(\"ln p(X|theta) : {:.6f}\".format(ln_p_X_theta))\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\nmargin = 0.5\n\n# ln p(X|theta_old)\nax.plot([0, 6], [ln_p_X_theta]*2, 'r', lw='3')\nax.text(3.8, ln_p_X_theta-margin,\n        r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='r', fontsize=20)\n\n# L(q_dummy,theta_old)\nax.plot([0, 3], [l_q_theta_dummy]*2, 'b', lw='3')\nax.text(0.8, l_q_theta_dummy-margin,\n        r\"$\\mathcal{L}(q_{\\text{dummy}}, \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\n\n# KL\nax.text(0, ln_p_X_theta+margin, r\"$\\text{KL}(q || p)$\",  color='g', fontsize=20)\nax.arrow(0.5, l_q_theta_dummy, 0, ln_p_X_theta-l_q_theta_dummy,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\nax.arrow(0.5, ln_p_X_theta, 0, l_q_theta_dummy-ln_p_X_theta,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\n\nax.set_ylim([-27, -20])\nplt.xticks([])\nplt.yticks(fontsize=15)\nplt.show()\n\nl(q,theta)    : -23.002070\nln p(X|theta) : -21.484619\n\n\n\n\n\n현재 파라미터 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)에서 dummy_q()를 \\(q(\\mathbf{Z})\\)로 하여 \\(\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}})\\)값을 계산해보면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})\\) 보다 약간 작고 그 차이가 \\(\\text{KL}(q\\,||\\,p)\\)로 나타나는 것을 확인할 수 있다.\n이제 \\(q(\\mathbf{Z})\\)를 \\(\\mathbf{Z}\\)에 대한 사후확률분포로 바꿔보자. \\(q(\\mathbf{Z}) = p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)로 설정한다.\n\n# ELBO를 계산할 때 제안하는 q(Z)를 Z의 사후확률분포로 제안한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nl_q_theta = ELBO(p_Z_given_X_and_theta_old, theta_old)\nln_p_X_theta = loglikelihood_X(theta_old, X)\nprint(\"l(q,theta)    : {:.6f}\".format(l_q_theta))\nprint(\"ln p(X|theta) : {:.6f}\".format(ln_p_X_theta))\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\n# ln p(X|theta_old)\nax.plot([0, 6], [ln_p_X_theta]*2, 'r', lw='3')\nax.text(3.8, ln_p_X_theta-margin, r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='r', fontsize=20)\n\n# L(q(Z|X, theta_old),theta_old)\nax.plot([0, 3], [l_q_theta]*2, 'b', lw='3')\nax.text(0.8, l_q_theta-margin, r\"$\\mathcal{L}(q(\\mathbf{Z}\\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}), \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\nax.arrow(0.5, l_q_theta_dummy, 0, ln_p_X_theta-l_q_theta_dummy,\n         head_width=0.1, head_length=0.2, fc='b', ec='b', length_includes_head=True)\n\n\n# L(q_dummy,theta_old)\nax.plot([0, 3], [l_q_theta_dummy]*2, 'b--', lw='3')\nax.text(0.8, l_q_theta_dummy-margin, r\"$\\mathcal{L}(q_{\\text{dummy}}, \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\n\n# KL\nax.text(0, ln_p_X_theta+margin, r\"$\\text{KL}(q || p)=0$\",  color='g', fontsize=20)\n\nax.set_ylim([-27, -20])\nplt.xticks([])\nplt.yticks(fontsize=15)\nplt.show()\n\nl(q,theta)    : -21.484619\nln p(X|theta) : -21.484619\n\n\n\n\n\n예상대로 \\(\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}})= \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})\\)가 되며 \\(\\text{KL}(q\\,||\\,p)\\)가 사라진다.\n\n\\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 완전 데이터 세트 로그가능도 평균 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)의 관계\n이쯤에서 지금까지 이야기한 완전 데이트 세트에 대한 로그 가능도 평균과 그것을 반복적으로 최대화 시키는 과정이 식(9.70)과 어떻게 연결되는지 알아보자.\n\\(q(\\mathbf{Z})\\)를 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)로 설정하면 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 식(9.74)처럼 될 것이다.\n\\[\n\\begin{aligned}\n\\mathcal{L}(q, \\boldsymbol{\\theta})\n&= \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) - \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\\\\n&=  \\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) + \\text{const}\n\\end{aligned} \\tag{9.74}\n\\]\n놀랍게도 식(9.74)에서 첫 번째 항은 \\(\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})\\) 또는 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)가 된다. 그리고 두 번째 항은 현재 파라미터 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)에만 의존하고 \\(\\boldsymbol{\\theta}\\)와는 상관없는 상수항이다. 따라서 \\(q(\\mathbf{Z})\\)를 \\(\\mathbf{Z}\\)의 사후확률분포로 설정하고 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)을 계산하는 것이 결국 \\(\\mathbf{Z}\\)의 사후확률분포하에서 완전 데이터 세트의 로그가능도 평균을 구하는 것과 대등한 것이다.\n결국 \\(\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})\\) 또는 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 \\(\\boldsymbol{\\theta}\\)대해 최대화 하는 것이 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 최대화 하는 것이라는 사실을 알 수 있다.\n방금까지 과정이 EM 알고리즘에서 기대값을 구성하는 단계인 Expectation 단계이다.\n정리하면 우리는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화 하는 \\(\\boldsymbol{\\theta}\\)를 찾고 싶은데 \\(\\ln(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접 최대화하는 것은 어려우니 \\(\\mathbf{Z}\\)의 사후확률을 도입하여 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 하한인 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 최대한 키워 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)와 동일하게 만들었다.\n그리고 이때 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 완전 데이터 세트에 대한 가능도의 기댓값 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)와 상수로 구성되기 때문에 결과적으로 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 최대화 시키는 것이 곧 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 최대화 시키는 것을 알았다. 또 중요한 점은 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)이 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 하한이기 때문에 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)을 증가시키면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)는 무조건 증가할 것이라는 점이다.\n\\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 최대화 시키는 것은 이미 3편에서 알아본 것과 같다. \\(\\gamma\\)를 구해 Expectation 단계를 수행한다. \\(\\gamma\\)를 구하는 과정이 곧 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)를 제안하는 것임을 상기하자. 그렇게 제안된 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)하에서 계산된 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 직접 미분하여 얻은 식(29), (32)를 수행한다.\n이 두 과정을 묶어 EM_step()이라는 함수로 만들자.\n\ndef EM_step() :\n    \"\"\"\n    gloval variables\n    X : (N,D)\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    n_t\n    \"\"\"\n    mu_old = theta_old[:K*D].reshape(K,D)\n    pi_old = theta_old[-K:]\n\n    Gamma = np.array([ pi_old[k]*binom.pmf(X, n_t, mu_old[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\n    Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n    Nk = Gamma.sum(axis=0)\n    mu_new = ((Gamma * X).sum(axis=0) / (Nk*n_t)).reshape(-1,1)\n    pi_new = Nk / N\n\n    return mu_new, pi_new\n\n실제 EM_step()을 수행하고 새롭게 얻어진 파라미터를 theta_new로 설정한다.\n\nmu_new, pi_new = EM_step()\ntheta_new = np.hstack((mu_new.flatten(), pi_new))\n\nprint(theta_old)\nprint(theta_new)\n\n[0.4 0.3 0.5 0.5]\n[0.70729725 0.45725284 0.73085881 0.26914119]\n\n\n새롭게 계산된 파라미터 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)에 의해 증가된 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 그려보자.\n\n# ELBO를 계산할 때 제안하는 q(Z)를 Z의 사후확률분포로 제안한다.\n# theta_old가 theta_new로 업데이트 되었으나 사후분포를 구성할 때 사용된\n# theta_old는 고정된 상태이다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nl_q_theta_new = ELBO(p_Z_given_X_and_theta_old, theta_new)\nln_p_X_theta_new = loglikelihood_X(theta_new, X)\nprint(\"l(q,theta)    : {:.6f}\".format(l_q_theta_new))\nprint(\"ln p(X|theta) : {:.6f}\".format(ln_p_X_theta))\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\n# ln p(X|theta_new)\nax.plot([0, 6], [ln_p_X_theta_new]*2, 'r', lw='3')\nax.text(3.8, ln_p_X_theta_new-margin*2, r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{new}})$\",\n        color='r', fontsize=20)\nax.arrow(3.5, ln_p_X_theta, 0, ln_p_X_theta_new-ln_p_X_theta,\n         head_width=0.1, head_length=0.2, fc='r', ec='r', length_includes_head=True)\n\n# ln p(X|theta_old)\nax.plot([3, 6], [ln_p_X_theta]*2, 'r--', lw='3')\nax.text(3.8, ln_p_X_theta-margin*2, r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='r', fontsize=20)\n\n# L(q(Z|X, theta_old),theta_new)\nax.plot([0, 3], [l_q_theta_new]*2, 'b', lw='3')\nax.text(0.8, l_q_theta_new-margin*2, r\"$\\mathcal{L}(q(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}), \\boldsymbol{\\theta}^{\\text{new}})$\",\n        color='b', fontsize=20)\nax.arrow(0.5, l_q_theta, 0, l_q_theta_new-l_q_theta,\n         head_width=0.1, head_length=0.2, fc='b', ec='b', length_includes_head=True)\n\n# L(q(Z|X, theta_old),theta_old)\nax.plot([0, 3], [l_q_theta]*2, 'b--', lw='3')\nax.text(0.8, l_q_theta-margin*2, r\"$\\mathcal{L}(q(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}), \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\n\n# KL\nax.text(0, ln_p_X_theta_new+margin, r\"$\\text{KL}(q || p)$\",  color='g', fontsize=20)\nax.arrow(0.5, l_q_theta_new, 0, ln_p_X_theta_new-l_q_theta_new,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\nax.arrow(0.5, ln_p_X_theta_new, 0, l_q_theta_new-ln_p_X_theta_new,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\n\nax.set_ylim([-23, -11])\nplt.xticks([])\nplt.yticks(fontsize=15)\nplt.show()\n\nl(q,theta)    : -13.141270\nln p(X|theta) : -21.484619\n\n\n\n\n\n\\(\\boldsymbol{\\theta}^{\\text{old}}\\)에서 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)로 파라미터가 조정되면서 두 함수 모두 점선 위치에서 증가하였다. 증가된 정도를 보면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{new}})\\)가 \\(\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{new}})\\)보다 조금 더 증가되었다. 그 이유는 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)에서 \\(\\boldsymbol{\\theta}^{\\text{new}}\\) 변경되면서 현재 파라미터 상태에서 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{new}})\\)는 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)와 달라졌으며 그 결과 \\(KL(q \\,||\\, p)\\)는 다음과 같이 0이 아니기 때문이다.\n\\[\nKL(q \\,||\\, p) = - \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta}^{\\text{old}}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta}^{\\text{new}})}{ p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta}^{\\text{old}}) } \\neq 0\n\\]\n이 한번의 반복으로 원래 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)보다 더 큰 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\) 함숫값을 주는 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)를 찾아내게 되었다. 이 과정이 Maximization 과정이다.\n이제 \\(q(\\mathbf{Z})\\)를 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{new}})\\)로 재설정하는 Expectation 과정을 반복하면 두 번째 그림 상태로 돌아가게 된다. 여기서 다시 Maximization 과정을 수행하여 조금 더 큰 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\) 함숫값을 주는 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)를 구할 수 있다. 이 과정을 반복하는 동안 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)는 절대 줄어들지 않으며 결과적으로 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 최대화하는 과정을 반복하면서 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화하게 되는 것이다.\n이것으로 평균을 최대화 시키는 방법이 왜 유효하고 그것의 반복이 왜 수렴하는지 알 수 있게 되었다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#em-알고리즘-시각화",
    "href": "posts/em/em_algorithm.html#em-알고리즘-시각화",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "EM 알고리즘 시각화",
    "text": "EM 알고리즘 시각화\n지금까지 아주 길게 완전 데이트 세트 가능도 평균의 최대화를 반복하는 과정이 \\(\\ln(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화시키는 이유에 대해서 알아보았다. 이제 실제 우리가 설정한 문제에서 어떤 식으로 최대화가 일어나고 있는지 시각화하면서 이 길고 지루한 과정을 마무리해보자.\n우리가 다루고 있는 문제에서 구하고자 하는 파라미터는 총 4개이므로 이 모든 파라미터가 최적화되는 과정을 시각화 할 수 없다. 그러므로 \\(\\boldsymbol{\\theta}\\)에서 \\(\\mu_1\\)에 대해서만 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 관계를 시각화 해보자.\n현재 파라미터 상태를 적당히 설정한다.\n\nmu_old = np.array([[0.6],[0.45]])\npi_old = np.array([0.6, 0.4])\ntheta_old = np.hstack((mu_old.flatten(), pi_old))\n\n이전에 만들어 놓은 loglikelihood_X()를 이용해서 \\(\\mu_1\\)만을 변수로 하는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 만든다.\n\ndef loglikelihood_X_mu1(mu_1, X):\n    return loglikelihood_X(np.array([mu_1, 0.45, 0.6, 0.4]), X)\n\n이제 \\(\\mu_1\\)에 따른 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 그려본다. 우리 목적은 국부적으로 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)가 가장 높은 곳을 찾아가는 것이다.\n\nmus = np.linspace(0, 1, 100)\n\n# ln( p(X|theta) )\nloglikelihood_X_by_mu1 = np.array([ loglikelihood_X_mu1(mu,  X)\n                                    for mu in mus ])\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\n\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\n\nplt.show()\n\n\n\n\n그림을 보면 0.8x 정도에서 최대점이 형성되는 것을 알 수 있다.\n완전 데이터 집합의 로그가능도 평균도 \\(\\mu_1\\)만의 함수로 다시 만든다.\n\ndef E_Z_loglikelihood_XZ_mu1(mu_1, X):\n    # mu_1을 제외한 나머지 파라미터들은 이미 최적화된 것으로 가정하고 고정\n    return E_Z_loglikelihood_XZ(np.array([mu_1, 0.45, 0.6, 0.4]), X)\n\n이제 현재 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)를 조건으로 하는 사후확률분포를 이용하여 전체 \\(\\mu_1\\)에 대해서 \\(\\mathcal{L}(q, \\mu_1)\\)을 그려본다. 이때 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)도 함께 그려 상황을 확인해보자.\n\nmus = np.linspace(0, 1, 100)\n\n# 현재 파라미터로 Z의 사후확률 q(Z|X,mu_1^{old})를 세팅한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\n# L(q,mu_1)를 mu_1에 대해서 다시 그린다.\nl_q_theta = np.array([ E_Z_loglikelihood_XZ_mu1(mu, X)\n                        - entropy_Z(p_Z_given_X_and_theta_old) for mu in mus ])\n\n# 현재 \\mu_1^{old}에서 L(q,mu_1) 값을 계산한다.\nl_q_theta_0 = E_Z_loglikelihood_XZ_mu1(mu_old[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_old[0,0], mu_old[0,0]], [-23,-10], 'k--')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n그림에서 확인할 수 있듯이 현재 파리미터에서 \\(\\mathcal{L}(q, \\mu_1^{\\text{old}}=0.6)\\)과 \\(\\ln p(\\mathbf{X} \\mid \\mu^{\\text{old}}_1=0.6)\\)가 같아지는 모습을 확인할 수 있다. 이제 이 상태에서 EM 스탭을 한번 반복한다.\n\n# EM_step으로 새로운 파라미터를 얻는다.\nmu_new, pi_new = EM_step()\n\n# 업데이트된 mu_1^{new}에서 L(q,mu_1) 값을 계산한다.\nmax_l_q_theta = E_Z_loglikelihood_XZ_mu1(mu_new[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_new[0,0], mu_new[0,0]], [-23,-10], 'k--')\nax.plot(mu_new[0,0], max_l_q_theta, 'ro')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\nEM_step() 결과 \\(\\mu_1^{\\text{old}}\\)가 \\(\\mathcal{L}(q, \\mu_1)\\)의 최대점으로 이동한 것을 확인할 수 있다. 파란 점보다 빨간 점이 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)의 최대점에 조금 더 가까워졌다. 새롭게 구해진 파라미터를 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)로 업데이트하고 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)를 조건으로 다시 \\(\\mathbf{Z}\\)의 사후확률분포를 계산한다.\n\nmu_old = mu_new\npi_old = pi_new\ntheta_old = np.hstack((mu_old.flatten(), pi_old))\n\n# 현재 파라미터로 Z의 사후확률을 세팅한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nprint(mu_old)\nprint(pi_old)\n\n[[0.7428154 ]\n [0.43070022]]\n[0.67058506 0.32941494]\n\n\n새롭게 구성된 사후확률하에서 \\(\\mathcal{L}(q, \\mu_1)\\)을 다시 그려보면 \\(\\mu_1^{\\text{old}}=0.743\\)에서 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)과 접할 것이다.\n\nl_q_theta = np.array([ E_Z_loglikelihood_XZ_mu1(mu, X)\n                       - entropy_Z(p_Z_given_X_and_theta_old) for mu in mus ])\n\nl_q_theta_0 = E_Z_loglikelihood_XZ_mu1(mu_old[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_old[0,0], mu_old[0,0]], [-23,-10], 'k--')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n예상대로 새롭게 구성된 \\(\\mathcal{L}(q, \\mu_1)\\)이 \\(\\mu_1=0.743\\)에서 접하는 것을 확인할 수 있다. 이제 다시 EM_step()를 실행한다.\n\nmu_new, pi_new = EM_step()\nmax_l_q_theta = E_Z_loglikelihood_XZ_mu1(mu_new[0,0], X) \\\n                  - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_new[0,0], mu_new[0,0]], [-23,-10], 'k--')\nax.plot(mu_new[0,0], max_l_q_theta, 'ro')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n다시 \\(\\mathcal{L}(q, \\mu_1)\\)의 최대값으로 파라미터가 업데이트 된다. 이런 과정을 계속 반복하면서 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)의 국부 최대점으로 수렴하게 되는 것이다.\n\nmu_old = mu_new\npi_old = pi_new\ntheta_old = np.hstack((mu_old.flatten(), pi_old))\n\n# 현재 파라미터로 Z의 사후확률을 세팅한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nprint(mu_old)\nprint(pi_old)\n\n[[0.81242457]\n [0.37383545]]\n[0.60686538 0.39313462]\n\n\n\nl_q_theta = np.array([ E_Z_loglikelihood_XZ_mu1(mu, X)\n                       - entropy_Z(p_Z_given_X_and_theta_old) for mu in mus ])\n\nl_q_theta_0 = E_Z_loglikelihood_XZ_mu1(mu_old[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_old[0,0], mu_old[0,0]], [-23,-10], 'k--')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n\nmu_new, pi_new = EM_step()\nmax_l_q_theta = E_Z_loglikelihood_XZ_mu1(mu_new[0,0], X) \\\n                  - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_new[0,0], mu_new[0,0]], [-23,-10], 'k--')\nax.plot(mu_new[0,0], max_l_q_theta, 'ro')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n이렇게 나름 복잡하다면 복잡한 EM 알고리즘을 알아봤다. 소설처럼 술술 읽히는 글은 아닐지라도 EM 알고리즘을 어렴풋이 이해하고 있는 분들께는 나름 도움이 될 것이라는 믿음으로 글을 마무리한다."
  },
  {
    "objectID": "posts/em/em_algorithm.html",
    "href": "posts/em/em_algorithm.html",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "",
    "text": "2020.07.09 조준우(metamath@gmail.com)"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#필요-패키지-설치",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#필요-패키지-설치",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "시작하기전에 필요한 라이브러리를 설치한다. 본인 컴퓨터에 이미 관련 라이브러리가 설치되어 있다면 설치하지 않아도 된다.\n먼저 허깅페이스의 트랜스포머스 라이브러리를 설치한다.\n\n!pip install transformers\n\n그 다음은 데이터 셋을 다운받는기 위해 다음 명령을 실행해서 허깅페이스 Datasets 라이브러리를 설치한다.\n\n!pip install datasets\n\n그리고 T5 모델의 토크나이저가 sentencepiece를 사용하므로 다음을 실행해서 설치한다.\n\n!pip install sentencepiece\n\n또 모델 평가를 위해 허깅페이스 evaluate 라이브러리와 BLEU 점수를 계산하기위해 sacrebleu를 설치한다.\n\n!pip install evaluate\n\n\n!pip install sacrebleu"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#data-set",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#data-set",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "모두 설치가 완료되었다면 데이터 셋을 다운받아야 한다. 먼저 허깅페이스 사이트에 접속해서 상단 메뉴에 Datasets를 클릭하고 아래처럼 검색 조건을 맞추면\n\n좌측 작은 메뉴에서 Languages를 선택한다.\nLanguages 하단에 보이는 여러 언어중에 Korean을 선택한다.\n다시 우측 검색 필터 창에 en을 적는다.\n\n데이터 셋 네 개가 보이는데 이 중에서 bongsoo/news_talk_en_ko를 사용하도록 하겠다.\nbongsoo/news_talk_en_ko를 클릭해서 나오는 화면에서 Files and Versions를 클릭하면 tsv 파일이 보이는데 이 파일에는 영어 문장과 한국어 문장이 한줄에 탭 문자로 구분되어 적혀있다. 로컬 디스크이 이 파일을 다운받고 파일을 읽어보면 다음처럼 확인된다.\n\n[노트] 로컬 또는 코랩 런타임에 파일을 다운 받지 않았다면 굳이 다운받을 필요없고 이 셀은 스킵하자. 그냥 데이터 파일 한줄에 영어 문장과 짝이 되는 한국어 문장이 탭문자로 구분되어 있다는 것만 알면 된다. 실제 데이터를 가져올 때는 허깅페이스를 통해 다운 받게 된다.)\n\n\n!head -5 news_talk_en_ko_train_130000.tsv\n\nSkinner's reward is mostly eye-watering.    스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\nEven some problems can be predicted.    심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\nOnly God will exactly know why. 오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\nBusinesses should not overlook China's dispute. 중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\nSlow-beating songs often float over time.   박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n데이터 파일은 아주 단순한 형태인 것을 알 수 있다. 직접 tsv파일을 다운받아서 사용해도 되나 허깅페이스 허브로 부터 바로 다운받아 사용하는 편이 더 편하다. 다음 명령으로 다운받을 수 있다.\n\n# 데이터 셋을 다운받을 함수를 임포트 한다.\nfrom datasets import load_dataset\n\n\n# 좀 전에 알아본 체크포인트를 사용해서 데이터를 받아온다.\nen_ko = load_dataset(\"bongsoo/news_talk_en_ko\")\n\nUsing custom data configuration bongsoo--news_talk_en_ko-e7f00bc8f76f18d5\nFound cached dataset csv (/home/metamath/.cache/huggingface/datasets/bongsoo___csv/bongsoo--news_talk_en_ko-e7f00bc8f76f18d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n\n\n\n\n\n이제 데이터 객체를 확인해보면 DatasetDict라는 것을 알 수 있고 안에 train 키만 있는 것이 확인된다.\n\nen_ko\n\nDatasetDict({\n    train: Dataset({\n        features: [\"Skinner's reward is mostly eye-watering.\", '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.'],\n        num_rows: 1299999\n    })\n})\n\n\ntrain키에 Dataset 객체가 하나 있는데 features가 첫번째 데이터로 되어있고 행수는 1299999개인 것을 보아 데이터 파일에 컬럼명이 적혀있는 헤더라인이 없어서 첫줄을 헤더로 읽은것 같다. 첫줄을 데이터로 다시 집어 넣고 컬럼명은 en, ko로 설정하기 위해 데이터 셋을 pandas로 읽어드린다.\n\nimport pandas as pd\n\n\n# 허깅페이스 데이터셋을 판다스 포맷으로 세팅\nen_ko.set_format(type=\"pandas\")\n\n\n# 'train'키의 모든 행을 DataFrame df에 할당\ndf = en_ko[\"train\"][:]\n\n# 잘 담겼는지 확인한다.\ndf.head()\n\n\n\n\n\n\n\n\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n\n\n0\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n1\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n2\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n3\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n4\nI can't even consider uninsured treatments.\n보험 처리가 안 되는 비급여 시술은 엄두도 못 낸다.\n\n\n\n\n\n\n\n예상처럼 첫 줄이 헤더가 되었으니 이를 수정한 DataFrame을 만든다.\n\nexample_0 = list(df.columns)\nexample_0\n\n[\"Skinner's reward is mostly eye-watering.\",\n '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.']\n\n\n적당히 조작해서 컬럼명이 en, ko가 되게 하고 example_0가 첫 행이 되도록 만든다.\n\nexample_0_df = pd.DataFrame({col: [value] for col, value in zip(('en', 'ko'), example_0)})\n\n\ndf.columns = ('en', 'ko')\n\n\nen_ko_df = pd.concat([example_0_df, df],).reset_index(drop=True)\nen_ko_df.head()\n\n\n\n\n\n\n\n\nen\nko\n\n\n\n\n0\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n1\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n2\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n3\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n4\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n\n\n\n\n\n이렇게 데이터 셋을 DataFrame으로 만들었다. 이제 이 en_ko_df로 부터 다시 허깅페이스 데이터 셋을 생성하자.\n\nfrom datasets import Dataset\n\n\ndataset = Dataset.from_pandas(en_ko_df)\n\n\ndataset\n\nDataset({\n    features: ['en', 'ko'],\n    num_rows: 1300000\n})\n\n\n다시 데이터 셋을 확인해보면 features가 제대로 표시되고 샘플 수도 1300000개 인것을 확인할 수 있다.\n이렇게 만들어진 DataFrame으로 부터 데이터 셋이 잘 초기화되는 것을 확인했으니 en_ko_df를 세조각으로 쪼개서 tsv파일로 저장하자.\n\n# 각 데이터 셋의 샘플수를 정한다.\nnum_train = 1200000\nnum_valid = 90000\nnum_test = 10000\n\n설정된 크기만큼 DataFrame을 자른다.\n\nen_ko_df_train = en_ko_df.iloc[:num_train]\n\n\nen_ko_df_valid = en_ko_df.iloc[num_train:num_train+num_valid]\n\n\nen_ko_df_test = en_ko_df.iloc[-num_test:]\n\n다시 tsv파일로 저장한다.\n\nen_ko_df_train.to_csv(\"train.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_valid.to_csv(\"valid.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_test.to_csv(\"test.tsv\", sep='\\t', index=False)\n\n이렇게 tsv파일 세개로 데이터를 정리했다. 이제 필요할때 이 파일을 읽어 허깅페이스 데이터셋을 만들 수 있다.\n아래처럼 스플릿을 정의한 사전을 load_dataset에 넘기면 된다. 이때 delimiter를 탭 문자로 지정해야 한다.\n\ndata_files = {\"train\": \"train.tsv\", \"valid\": \"valid.tsv\", \"test\": \"test.tsv\"}\n\n\ndataset =  load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n\nUsing custom data configuration default-02a3611b1810efcd\n\n\nDownloading and preparing dataset csv/default to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\nDataset csv downloaded and prepared to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n제대로 로딩되었는지 dataset을 확인해보자.\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 10000\n    })\n})\n\n\nDatasetDict에 train, valid, test 키로 120만 문장, 9만 문장, 1만 문장이 저장된 것을 확인할 수 있다.\n이 데이터 셋에서 개별 샘플에 대한 접근은 [split][feature][row num] 형태로 가능하다.\n\n# train 스플릿에서 영어 3개와 한국어 3개 샘플을 가져온다.\nprint(dataset['train']['en'][:3], dataset['train']['ko'][:3])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n그런데 feature와 row num은 순서를 바꿔서 사용할 수 도 있다.\n\nprint(dataset['train'][:3]['en'], dataset['train'][:3]['ko'])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n데이터를 어떻게 조회하는지는 데이터 구성 방식에 따라 조금씩 다르므로 데이터 셋을 보고 몇번 해보면 금방 접근법을 알 수 있다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#hugging-face",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#hugging-face",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "데이터 셋 준비를 마쳤으니 학습할 차례이다. 허깅페이스에서 제공하는 필요 클래스를 임포트 한다.\n먼저 선학습 모델을 사용하기 위한 클래스를 임포트 한다. AutoTokenizer는 선학습된 모델이 사용한 토크나이저를 읽기 위해 필요하며 AutoModelForSeq2SeqLM은 시퀀스 투 스퀀스 방식으로 작동하는 선학습된 모델을 불러 올 때 마지막에 분류기 헤드를 붙여서 모델을 로딩하기 위해 사용한다.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n2023-03-01 16:05:02.191320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-01 16:05:02.266647: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-03-01 16:05:02.281905: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-01 16:05:02.592853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n\n다음은 데이터 콜레이터를 임포트한다. 시쿼스 투 시퀀스 학습 과정은 인코더 입력 시퀀스, 디코더 입력 시퀀스, 디코더 출력 시퀀스를 필요로 하는데 미니배치로 부터 이를 적절히 정리해서 모델에 입력하는 작업이 필요하다. 예를 들면 미니 배치 내에 있는 인코더 입력 시퀀스의 길이를 맞춘다든지 디코더 입력시퀀스를 오른쪽으로 한칸 쉬프트시켜 디코더 출력 시퀀스를 만드는 작업등이 콜레이터에서 일어나는 작업인데 이런 작업을 DataCollatorForSeq2Seq가 자동으로 처리하게 된다.\n\nfrom transformers import DataCollatorForSeq2Seq\n\n그리고 학습에 필요한 클래스를 임포트 한다. 학습에 필요한 설정을 Seq2SeqTrainingArguments에 정의하고 실제 학습은 Seq2SeqTrainer로 하게 된다. Seq2SeqTrainer는 generate()함수를 제공한다.\n\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n허깅페이스 라이브러리로는 마지막으로 데이터 셋을 로딩하는 함수와 번역 결과를 측정할 함수를 로딩한다.\n\nfrom datasets import load_dataset, load_metric\n\n그외 필요한 각종 라이브러리를 임포트 한다.\n\nimport numpy as np\nimport torch\nimport multiprocessing\n\n허깅페이스에서 파이토치 기반 구현을 사용하므로 gpu가 있다면 device를 세팅한다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\n미리 학습된 모델의 체크포인트를 세팅한다. 여기서 사용할 모델은 한국어와 영어에 미리 학습된 KE-T5모델을 사용한다. T5모델은 트랜스포머의 인코더, 디코더 구조를 모두 사용하는 모델로 번역기를 만들 때 사용할 수 있는 모델이다. 아래처럼 모델 체크 포인트와 T5 모델에 입력될 최대 토큰 길이를 설정한다.\n\nmodel_ckpt = \"KETI-AIR/ke-t5-base\"\nmax_token_length = 64"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#tokenizer",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#tokenizer",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "먼저 모델 체크 포인트를 사용하여 KE-T5 모델이 학습할때 함께 사용한 토크나이저를 불러온다. 허깅페이스 트랜스포머스 라이브러리를 사용할 때 가장 핵심이 되며 익숙해지기 쉽지 않은 부분이 이 토크나이저라고 개인적으로 생각한다.\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n\n토크나이저를 로딩할때 sentencepiece가 없다고 에러가 나면 위 제시한 라이브러리가 설치 안된 것이므로 설치하고 다시 시도한다.\n토크나이저를 불러왔으니 현재 사용하는 데이터셋에서 샘플을 가져와 토크나이징해보는 것이 좋을 것이다. 학습 세트에서 10번 샘플을 가지고 실험해보자. 먼저 10번 샘플을 뿌려보고\n\ndataset['train'][10]['en'], dataset['train'][10]['ko']\n\n('Any academic achievement requires constant repetition.',\n '어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.')\n\n\n토크나이저에 각 문장을 입력하고 토큰화된 상태로 돌려 받는다.\n\ntokenized_sample_en = tokenizer(dataset['train'][10]['en'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_en\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenized_sample_ko = tokenizer(dataset['train'][10]['ko'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_ko\n\n{'input_ids': [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n문장에 토큰으로 쪼개지고 각 토큰이 숫자로 변환된 것을 볼 수 있다. 이렇게 숫자화된 토큰을 input_ids로 반환하고 추가로 트랜스포머 인코더, 디코더에 쓰일 패딩 마스크도 함께 attention_mask로 돌려준다. 마스크가 모두 1인 이유는 샘플이 하나밖에 없어서 이다. 샘플 몇개를 더 실험해보면\n\ntokenizer(dataset['train'][:3]['en'], \n          max_length=max_token_length, \n          padding=True, truncation=True)\n\n{'input_ids': [[388, 6809, 2952, 17, 8, 32204, 43, 8023, 6687, 28, 9495, 91, 3, 1], [4014, 322, 3170, 147, 67, 23274, 3, 1, 0, 0, 0, 0, 0, 0], [11783, 4412, 96, 6556, 709, 1632, 3, 1, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}\n\n\n미니배치에 있는 샘플의 최대길이메 맞춰서 패딩되는 모습을 확인할 수 있다. 실제로 어떻게 토큰화 되었는지 확인해보자.\n\npd.DataFrame(\n    [\n        tokenized_sample_en['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_en['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nids\n13941\n10114\n25542\n9361\n20526\n742\n32268\n12520\n3\n1\n\n\ntokens\n▁Any\n▁academic\n▁achievement\n▁requires\n▁constant\n▁re\npet\nition\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\n\npd.DataFrame(\n    [\n        tokenized_sample_ko['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_ko['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nids\n404\n12663\n15\n10775\n2334\n6\n15757\n21\n29819\n1736\n26778\n4342\n15\n1701\n3\n1\n\n\ntokens\n▁어떤\n▁학문\n이\n든지\n▁일정\n의\n▁성취\n를\n▁이루기\n▁위해서는\n▁끊임없는\n▁반복\n이\n▁필요하다\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\nKE-T5를 학습할때 학습된 규칙대로 토큰화가 진행된다. 영어에서 repetition은 re, pet, ition으로 쪼개진 것을 볼 수 있고, 한국어에서 성취를은 성취, 를로 쪼개지고 학문이든지는 학문, 이, 든지로 쪼개진것을 볼 수 있다. 토큰 앞에 _표시는 이 토큰 앞에는 공백이 있어야 한다는 의미다. 그리고 마지막에 엔드 토큰인 &lt;/s&gt;가 항상 붙게 되는 것도 확인할 수 있다.\n이제 앞서 tsv파일로 부터 로딩한 dataset내의 문장을 모두 토크나이저를 사용해서 숫자로 바꾸는 작업을 해야 한다. 즉 문자로된 문장을 숫자로 바꿔 특성화 해야 한다. dataset.map()함수에 각 샘플을 토큰화 하는 함수를 만들어 전달하면 map()이 모든 샘플에 대해 전달받은 함수를 적용하게 되는데 함수는 이렇게 작성하면 된다.\n\ndef convert_examples_to_features(examples):\n    ###########################################################################\n    # with 쓰는 옛날 방식\n    # input_encodings = tokenizer(examples['en'], \n    #                             max_length=max_token_length, truncation=True)\n    \n    # Setup the tokenizer for targets\n    # with tokenizer.as_target_tokenizer():\n    # target_encodings = tokenizer(text_target=examples['ko'], \n    #                             max_length=max_token_length, truncation=True)\n    #\n    #\n    # return {\n    #     \"input_ids\": input_encodings[\"input_ids\"],\n    #     \"attention_mask\": input_encodings[\"attention_mask\"],\n    #     \"labels\": target_encodings[\"input_ids\"]\n    # }\n    \n    # 그런데 이렇게 하면 인풋하고 한번에 처리 가능함.\n    model_inputs = tokenizer(examples['en'],\n                             text_target=examples['ko'], \n                             max_length=max_token_length, truncation=True)\n    \n    return model_inputs\n\nconvert_examples_to_features()가 하고 싶은 일은 dataset에 있는 “어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.”라는 샘플 문장을 [404,12663,15,10775,2334,6,15757,21,29819,1736,26778,4342,15,1701,3,1]라는 정수로 바꾸는 것이다. convert_examples_to_features()가 dataset에 적용될 때 넘겨 받는 examples는 다음과 같이 넘어 온다.\nexamples= {'en':['sent1', 'sent2', ... , 'sent1000'], # 이건 문장 1000개짜리 리스트\n           'ko':['sent1', 'sent2', ... , 'sent1000']}\n기본으로 미니 배치 사이즈는 1000으로 세팅되어 있다.(함수 기본인자는 여기서 확인 가능)\n미니 배치로 넘어온 문장 샘플을 영어 문장과 한국어 문장을 각각 인풋과 타겟으로 토큰화하고 이로 부터 input_ids, attention_mask, labels로 묶어 리턴하는 방식이 예전에 쓰던 방식으로 함수 위쪽에 주석처리 되어 있다. 타겟 문장을 토큰화 할 때 타겟에서 필요로 하는 특수 토큰을 추가하는 경우 이를 처리하기위해 타겟 토큰 토큰화 때는 with tokenizer.as_target_tokenizer():라는 컨텍스트 매니저를 사용했는데 최근 업데이트에서는 그냥 tokenizer에 text_target인자에 타겟 문장을 넣어서 한번에 다 처리할 수 있다. 이렇게 model_inputs을 반환하면 dataset에 있던 각 레코드 마다 en, ko 특성에 추가로 input_ids, attention_mask, labels 특성이 더 추가 되게 된다. 사실 en, ko 특성은 더이상 필요없기 때문에 convert_examples_to_features()를 적용할 때 없애라는 인자를 세팅한다.\n바로 dataset에 함수를 적용해보자. 그냥 해도되나 좀 더 빠르게 하기 위해 num_proc 인자에 스레드 개수를 지정한다.\n\nNUM_CPU = multiprocessing.cpu_count() \nNUM_CPU\n\n20\n\n\n그리고 remove_columns 인자에 기존 특성 이름인 en, ko를 전달해서 기존 특성은 제거하게 한다. 이 특성이 있으면 이후 콜레이터가 샘플들을 미니 배치로 묶을 때 패딩처리를 못하게 된다.\n\ntokenized_datasets = dataset.map(convert_examples_to_features, \n                                 batched=True, \n                                 # 이걸 쓰지 않으면 원 데이터 'en', 'ko'가 남아서\n                                 # 아래서 콜레이터가 패딩을 못해서 에러남\n                                 remove_columns=dataset[\"train\"].column_names,\n                                 num_proc=NUM_CPU) \n\n\n[노트] dataset.map()이 실행되면서 출력되는 출력은 생략됨\n\nconvert_examples_to_features()이 dataset의 모든 샘플에 다 적용되고 나면 tokenized_datasets는 다음처럼 된다.\n\ntokenized_datasets\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 10000\n    })\n})\n\n\n기존에 있던 특성 en, ko는 사라졌고 en은 input_ids와 attention_mask로 ko는 labels로 바뀐것을 확인할 수 있다. 예를 들어 학습 세트에 10번 데이터를 보면 다음처럼 다 숫자라 바뀌게 된것이다.\n\ntokenized_datasets['train'][10]\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'labels': [404,\n  12663,\n  15,\n  10775,\n  2334,\n  6,\n  15757,\n  21,\n  29819,\n  1736,\n  26778,\n  4342,\n  15,\n  1701,\n  3,\n  1]}\n\n\n토크나이저를 써서 숫자로 부터 토큰화 해보면 다음과 같다.\n\nprint( '원 데이터    :', dataset['train'][10]['en'] )\nprint( '처리 후 데이터:', tokenized_datasets['train'][10]['input_ids'] )\nprint( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['input_ids']) )\n\nprint('\\n')\nprint( '원 데이터    :', dataset['train'][10]['ko'] )\nprint( '처리 후 데이터:', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['labels']) )\nprint( '토큰화       :', tokenized_datasets['train'][10]['labels'] )\n\n원 데이터    : Any academic achievement requires constant repetition.\n처리 후 데이터: [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1]\n토큰화       : ['▁Any', '▁academic', '▁achievement', '▁requires', '▁constant', '▁re', 'pet', 'ition', '.', '&lt;/s&gt;']\n\n\n원 데이터    : 어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.\n처리 후 데이터: ['▁어떤', '▁학문', '이', '든지', '▁일정', '의', '▁성취', '를', '▁이루기', '▁위해서는', '▁끊임없는', '▁반복', '이', '▁필요하다', '.', '&lt;/s&gt;']\n토큰화       : [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1]\n\n\n데이터 특성화를 모두 마쳤으므로 이제 모델을 로딩하자. AutoModelForSeq2SeqLM를 사용해서 선학습 모델을 불러오면 선학습된 T5모델 마지막에 파인튜닝할 수 있는 분류 헤드를 붙인 모델을 반환한다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#model",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#model",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n위처럼 모델을 로딩하고 모델 출력 시켜보면 T5 모델 레이어가 매우 길게 출력되는데 제일 마지막 부분에 다음과 같이 분류 헤드가 붙어 있는 것을 확인할 수 있다. 헤드를 보면 모델에서 출력하는 벡터는 768차원이고 이를 단어장 사이즈인 64128로 변환시키고 있는 것을 알 수 있다.\n(lm_head): Linear(in_features=768, out_features=64128, bias=False)\n이렇게 생성된 model은 인코더-디코더 구조를 가지는 트랜스포머이므로 이 모델을 포워딩 하려면 인코더 인풋과 디코더 인풋을 넣어줘야 한다. 모델을 만들고 가장 먼저해야되는 작업은 포워딩 테스트라고 개인적으로 생각한다. 임의의 입력을 넣고 출력이 의도대로 나오는지 확인하는 것이다. 이런 작업은 직접 만든 모델이 아닐 수록 중요한데 이렇게 해야지 모델이 제대로 작동하는지 또 어떤 구조로 되어 있는지 쉽게 이해할 수 있기 때문이다. 포워드 테스트를 하기위해 간단한 영어문장으로 예제를 준비한다.\n\nencoder_inputs = tokenizer(\n    [\"Studies have been shown that owning a dog is good for you\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\ndecoder_targets = tokenizer(\n    [\"개를 키우는 것이 건강에 좋다는 연구 결과가 있습니다.\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\n영어 문장은 인코더의 입력이 되고 한국어 문장은 디코더의 타겟이 된다. 아래처럼 모두 숫자로 변환되어 있다.\n\nprint( encoder_inputs )\nprint( decoder_targets )\n\ntensor([[24611,    84,   166,  8135,    38,   847,    91,    16,  8146,    43,\n           667,    40,   106,     1]], device='cuda:0')\ntensor([[15833, 12236,   179, 16120, 28117,  1007,  3883,   327,     3,     1]],\n       device='cuda:0')\n\n\n이제 디코더 입력을 만들기위해 model._shift_right를 사용해 디코더 출력을 오른쪽으로 쉬프트 시킨다.\n\ndecoder_inputs = model._shift_right(decoder_targets)\n\ndecoder_inputs와 decoder_targets이 어떻게 다른지 비교해보면\n\npd.DataFrame(\n    [\n        tokenizer.convert_ids_to_tokens(decoder_targets[0]),\n        tokenizer.convert_ids_to_tokens(decoder_inputs[0])\n    ],\n    index=('decoder target', 'decoder input')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\ndecoder target\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n&lt;/s&gt;\n\n\ndecoder input\n&lt;pad&gt;\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n\n\n\n\n\n\n\n위처럼 오른쪽으로 쉬프트된 디코더 입력은 &lt;pad&gt; 토큰이 추가되었다. 이렇게 출력으로 쓰이는 문장을 오른쪽으로 쉬프트시켜 티처포싱Teacher forcing을 진행하게 된다. 다음처럼 model에 인코더 입력, 디코더 입력, 디코더 타겟을 입력하고 포워드 시킨다.\n\n# forward pass\noutputs = model(input_ids=encoder_inputs, \n                decoder_input_ids=decoder_inputs, \n                labels=decoder_targets)\n\nmodel의 outputs에는 다음과 같은 키가 있다.\n\noutputs.keys()\n\nodict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])\n\n\n손실함수 값을 다음처럼 확인할 수 있고 grad_fn이 있기 때문에 output.loss를 백워드 시킬 수 있다.\n\noutputs.loss\n\ntensor(87.8185, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n\n\n인코더의 마지막 상태는 (1, 14, 768)이다. 각 숫자는 순서대로 샘플 수, 스탭 수, 모델 사이즈를 나타낸다. 즉 인코더로 들어가는 14개 토큰이 각각 768차원 벡터로 인코딩되었다.\n\noutputs['encoder_last_hidden_state'].shape\n\ntorch.Size([1, 14, 768])\n\n\nlogit은 디코더 입력 토큰 10개에 대한 그 다음 토큰 예측 10개를 담고있다. 샘플 한개에 대해서 10개 토큰에 대해서 64128개 단어에 대한 확률값이 들어 있다.\n\noutputs['logits'].shape\n\ntorch.Size([1, 10, 64128])\n\n\nlogit에 argmax를 씌워서 토큰화시켜보면 다음과 같다.\n\ntokenizer.convert_ids_to_tokens( torch.argmax(outputs['logits'][0], axis=1).cpu().numpy() )\n\n['큐브', '큐브', '▁비일비재', '▁비일비재', '▁베네', '▁비일비재', '▁베네', '▁베네', '큐브', '큐브']\n\n\n마지막 헤더가 학습이 되지 않았기 때문에 적절한 아웃풋이 나오지 않지만 입력과 출력의 텐서 모양을 보면 포워드 패스가 제대로 작동한다는 것을 알 수 있다.\n지금까지 데이터 셋, 토크나이저, 모델에 대해서 알아봤다. 이제 학습을 위해 두 단계가 남았는데 하나는 데이터를 미니배치 형태로 모아 주는 콜레이터collator와 나머지 하나는 모델을 평가할 매트릭이다"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#collator",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#collator",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "파이토치에서 모델을 학습시키기 위해서 DataLoader를 사용하게 되는데 이 데이터 로더의 역할은 for 루프를 돌면서 데이터 셋으로 부터 샘플을 미니 배치 수만큼 가져오는 것이다. 이때 샘플을 미니 배치 수만큼 무작위로 가져와 어떤 식으로든 각 샘플을 짝맞춤해서 반환해야하는데 크기가 통일된 간단한 이미지 데이터인 경우 특별히 할것이 없지만 서로 크기가 다른 샘플들을 다루는 경우는 반환전 크기 또는 길이를 맞춘다든지 패딩을 한다든지 하는 추가 작업이 필요하게 된다. 이런 작업이 일어나는 곳이 collate_fn으로 지정되는 함수이다.\n시퀀스 투 시퀀스 모델을 학습시킬때 이런 콜레이터 함수가 하는 전형적인 역할은 입력 또는 출력 문자열을 패딩하고 조금 전 모델에서 알아봤듯이 디코더 타겟을 오른쪽으로 한칸 쉬프트 시켜서 디코더 입력으로 만드는 일이다. 앞서 이런 과정을 간단히기 직접 코딩해서 확인했지만 이런 작업을 자동으로 처리해주는 클래스가 DataCollatorForSeq2Seq이다.\n우선 콜레이터를 만들기 위해서는 토크나이저와 모델을 넘겨야 한다.\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n앞서 만들어논 tokenized_datasets에서 샘플 두개를 조회하면 다음처럼 전체 결과는 사전으로 리턴되며 사전의 각 키 아래에 여러 샘플들의 값이 리스트로 들어있게 된다.\n\n# 각 항목아래 샘플들이 리스트 형태로 묶여 반환된다.\ntokenized_datasets[\"train\"][1:3]\n\n{'input_ids': [[4014, 322, 3170, 147, 67, 23274, 3, 1],\n  [11783, 4412, 96, 6556, 709, 1632, 3, 1]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n 'labels': [[6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n  [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]]}\n\n\n콜레이터에 샘플을 넘길 때는 개별 샘플이 사전으로 묶이는 형태가 되어야 되므로 아래처럼 한번 가공하게 된다.\n\n# 콜레이터에는 샘플을 개별 {}로 넘겨야 됨\n[tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n\n[{'input_ids': [4014, 322, 3170, 147, 67, 23274, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1]},\n {'input_ids': [11783, 4412, 96, 6556, 709, 1632, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]}]\n\n\n위에 반환된 결과를 보면 각 샘플이 사전 {}으로 묶이고 샘플 하나에는 input_ids, attention_mask, labels이 존재한다. 각 샘플을 리스트로 묶어서 콜레이터에게 전달하고 반환되는 값을 확인해보자.\n\n# 콜레이터를 돌리면 알아서 패딩하고 쉬프트 시킨다.\nbatch = data_collator(\n    [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n)\n\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n반환된 batch의 키를 확인해보면 decoder_input_ids가 생긴것을 확인할 수 있다.\n\nbatch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\nbatch의 각 키에 어떤 값들이 들어있는지 확인해보자.\n\nbatch\n\n{'input_ids': tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n        [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,    15,\n          1587,     3,     1],\n        [ 9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,  2255,\n             3,     1,  -100]]), 'decoder_input_ids': tensor([[    0,  6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,\n            15,  1587,     3],\n        [    0,  9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,\n          2255,     3,     1]])}\n\n\n출력된 batch를 정리하면 아래처럼 된다.\n{\n    'input_ids': \n        tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n                [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), \n    'attention_mask': \n        tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1, 1, 1, 1]]), \n    'labels': \n        tensor(\n            [[ 6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n             [ 9881,18590,3837,70,4341,1086,677,35,426,2255,3,1,-100]]), \n    'decoder_input_ids': \n        tensor(\n            [[ 0,6842,404,951,5767,15387,    27,   831, 800,  4378, 15,  1587,     3],\n             [ 0,9881,18590,3837,70,4341,1086,677, 35,   426,2255,     3,     1]])\n}\n새로 생긴 decoder_input_ids는 앞에 0()이 붙어 있는 것이 보이고 label에서 끝에 1이 사라져 label이 오른쪽으로 쉬프트된 것임을 알 수 있다. 그리고 또 두번째 샘플 labels에서 마지막에 -100 이 보인다. 이 값은 label이 패딩된 것을 나타내며 손실 함수값을 계산할 때 -100이 있는 위치는 손실을 계산하지 않게 된다. 이렇게 시퀀스 투 시퀀스 모델을 학습하기 위해 필요한 자잘한 작업을 콜레이터가 알아서 자동으로 처리한다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#metric",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#metric",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "마지막으로 학습한 모델을 측정할 매트릭을 준비해야 한다. 번역 모델에서는 주로 BLEU 점수를 사용한다. BLEU 점수는 번역기가 생성한 문장이 레퍼런스(정답이라는 표현을 사용하지 않는 이유는 제대로 된 번역 문장이 오직 하나가 아니기 때문)문장과 얼마나 비슷한지 측정하는 점수라고 생각하면 된다. 단 같은 단어가 반복된다든지 레퍼런스 문장보다 너무 짧은 문장을 생성한다든지 하면 패널티를 부여 한다. 그렇기 때문에 레퍼런스 문장과 길이가 최대한 비슷하고 다양한 단어를 사용하면서 생성된 문장의 단어가 레퍼런스 단어에 많이 보여야 높은 점수를 얻게 된다.\nBLEU를 계산하기 위해 허깅페이스 evaluate 라이브러리와 sacrebleu라이브러리를 제일 처음에 설치했었다.\nsacrebleu 라이브러리는 BLEU 구현체에서 사실상 표준 라이브러리이며 각 모델이 다른 토크나이저를 쓰는 경우 이를 BPE로 통일 시켜 BLEU 점수를 계산한다고 한다. 참고링크\nevaluate라이브러리로 이 sacrebleu를 불러온다.\n\nimport evaluate\n\nmetric = evaluate.load(\"sacrebleu\")\n\n아래와 같은 예제가 있을 때 두 영어 문장을 번역기가 predictions처럼 번역했고 데이터 셋에 두 문장의 레퍼런스 번역이 references처럼 두개씩 있을 때 bleu점수를 계산해보면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 100.00000000000004,\n 'counts': [21, 19, 17, 15],\n 'totals': [21, 19, 17, 15],\n 'precisions': [100.0, 100.0, 100.0, 100.0],\n 'bp': 1.0,\n 'sys_len': 21,\n 'ref_len': 21}\n\n\n첫 예에서는 predictions가 references의 두 문장 중 하나와 완전히 일치하므로 score가 100점이 나왔다. 하지만 약간 다른 식으로 번역을 한다면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"딥러닝 프레임워크가 잘 개발되었기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 25.28116160010779,\n 'counts': [14, 7, 4, 1],\n 'totals': [19, 17, 15, 13],\n 'precisions': [73.6842105263158,\n  41.1764705882353,\n  26.666666666666668,\n  7.6923076923076925],\n 'bp': 0.9000876262522591,\n 'sys_len': 19,\n 'ref_len': 21}\n\n\n점수가 떨어지는 것을 확인할 수 있다. 아래 함수는 모델의 예측과 레이블을 가지고 bleu를 계산하는 헬퍼 함수로 트랜스포머 학습 코스 번역기 매트릭에서 제공하는 코드를 그대로 복사 한 것이다.\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Some simple post-processing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n    \n    return result"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#trainer",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#trainer",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "학습을 간단히 하기위해 허깅페이스에서 제공하는 Seq2SeqTrainer클래스를 사용한다. 학습 세부 조건은 Seq2SeqTrainingArguments를 사용하여 설정한다. 다음 코드로 학습에 필요한 세부 사항을 설정할 수 있다.\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"chkpt\",\n    learning_rate=0.0005,\n    weight_decay=0.01,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=128,\n    num_train_epochs=1,\n    save_steps=500,\n    save_total_limit=2,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"no\",\n    predict_with_generate=True,\n    fp16=False,\n    gradient_accumulation_steps=2,\n    report_to=\"none\" # Wandb 로그 끄기\n)\n\n이런 저런 자잘한 세팅을 해서 training_args를 만들고 trainer를 생성한다. 지금까지 준비한 model, training_args, tokenized_datasets, data_collator, tokenizer, compute_metrics를 넘기면 된다.\n\ntrainer = Seq2SeqTrainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"valid\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n이제 아래 코드로 드디어 학습을 할 수 있다!\n\n[주의] 코랩에서 실행한다면 per_device_train_batch_size를 12정도로 줄여서 학습해야 하는데 학습 시간만 10시간이 넘게 걸린다.\n\n\ntrainer.train()\n\n\n[노트] 학습 과정에서 출력되는 로그 문장들이 너무 길어서 여기선 생략 되었음\n\n학습이 끝났으면 다음 셀을 실행해서 결과를 저장한다.\n\ntrainer.save_model(\"./results\")"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#test",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#test",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "학습과 저장을 성공적으로 마쳤으면 다음 명령으로 모델을 불러올 수 있다.\n\nmodel_dir = \"./results\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\nmodel.cpu();\n\nloading file spiece.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading configuration file ./results/config.json\nModel config T5Config {\n  \"_name_or_path\": \"./results\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"gelu_new\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"gated-gelu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": true,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.24.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64128\n}\n\nloading weights file ./results/pytorch_model.bin\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./results.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n\n\n로딩된 모델을 테스트하기 위해 다음 두 문장을 준비한다.\n\ninput_text = [\n    \"Because deep learning frameworks are well developed, in these days, machine translation system can be built without anyone's help.\",\n    \"This system was made by using HuggingFace's T5 model for a one day\"\n]\n\n모델이 입력하기위해 토크나이저로 토큰화 시킨다.\n\ninputs = tokenizer(input_text, return_tensors=\"pt\", \n                   padding=True, max_length=max_token_length)\n\n/home/metamath/miniconda3/envs/torchflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n\n\ninputs를 확인해보면 input_ids와 attention_mask로 토큰화 된것을 알 수 있다. 첫번째 문장이 더 길기 때문에 두번째 문장의 마스크는 마지막에 0으로 패딩된 것도 확인할 수 있다.\n\ninputs\n\n{'input_ids': tensor([[ 8127,  5859,  5789, 22309,     8,    69,   484,  6560,     4,    20,\n           572,  1258,     4,  9872, 46301,  1076,   147,    67,  3807,  1215,\n          3993,    17,     8,   787,     3,     1],\n        [  465,  1076,    62,   565,    81,  1676,   992, 60049,  1044, 17400,\n            17,     8,   745,   466,  3900,    40,    16,   165,   688,     1,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0]])}\n\n\nmodel.generate()에 입력을 넣고 출력을 생성한다. 이때 빔서치를 하기 위해 num_beams=5로 설정한다.\n\nkoreans = model.generate(\n    **inputs,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\nkoreans.shape\n\ntorch.Size([2, 20])\n\n\n생성된 결과를 디코딩해보면 다음처럼 나쁘지 않게 번역되는 것을 확인할 수 있다.\n\n[ \n    tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(korean)) for korean in koreans\n]\n\n['&lt;pad&gt; 딥러닝 틀이 잘 개발되기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.&lt;/s&gt;',\n '&lt;pad&gt; 이 시스템은 HuggingFace의 T5 모델을 하루 동안 사용해 만든 시스템입니다.&lt;/s&gt;&lt;pad&gt;']\n\n\n마지막으로 테스트 셋에 대해서 몇개 문장을 가져와 번역해보자. 만들어 놓은 tokenized_datasets과 data_collator를 pytorch DataLoader에 그대로 전달해서 데이터 로더를 만들 수 있다.\n\nfrom torch.utils.data import DataLoader\n\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n)\n\n이터레이터로 만들어 한 미니 배치만 가져온다.\n\ntest_dataloader_iter = iter(test_dataloader)\n\n\ntest_batch = next(test_dataloader_iter)\n\n콜레이터에 의해 반환된 미니 배치에는 다음처럼 labels, decoder_input_ids 따위도 가지고 있으므로 모델에 입력하기 위해 input_ids, attention_mask만 남긴다.\n\ntest_batch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\n\ntest_input = { key: test_batch[key] for key in ('input_ids', 'attention_mask') }\n\n\nkoreans = model.generate(\n    **test_input,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\n이제 입력문장, 정답 그리고 생성된 문장을 비교하기 위해 우선 test_batch.labels에 -100으로 인코딩된 부분을 패딩 코튼으로 교체 한다.\n\nlabels =  np.where(test_batch.labels != -100, test_batch.labels, tokenizer.pad_token_id)\n\n\neng_sents = tokenizer.batch_decode(test_batch.input_ids, skip_special_tokens=True)[10:20]\n\n\nreferences = tokenizer.batch_decode(labels, skip_special_tokens=True)[10:20]\n\n\npreds = tokenizer.batch_decode( koreans, skip_special_tokens=True )[10:20]\n\n\nfor s in zip(eng_sents, references, preds):\n    print('English   :', s[0])\n    print('Reference :', s[1])\n    print('Translated:', s[2])\n    print('\\n')\n\nEnglish   : Yes, I'll see you at the parking lot at 3 p.m.\nReference : 네, 오후 3시에 주차장에서 뵙죠.\nTranslated: 네, 오후 3시에 주차장에서 뵙겠습니다.\n\n\nEnglish   : I'm happy to see Jessica Huh take over my role.\nReference : Jessica Huh가 제 역할을 인계받게 되어서 저는 기니다.\nTranslated: 제시카 허가 제 역할을 맡아서 기뻐요.\n\n\nEnglish   : I agree with you that she is qualified for the position.\nReference : 저도 부장님 의견대로 그녀가 이 자리의 적임자라고 생각합니다.\nTranslated: 나는 그녀가 이 직책에 자질이 있다고 당신과 동의합니다.\n\n\nEnglish   : Nick, I was told that your department will be divided into two.\nReference : Nick, 당신 부서가 둘로 나뉜다면서요?\nTranslated: 닉, 당신의 부서가 두 가지로 나뉘게 될 거라고 들었습니다.\n\n\nEnglish   : Yes, all staff involved in online advertising will have their own office space located on the 2nd floor.\nReference : 네, 다음 달 초부터 온라인 광고와 관련된 직원들은 2층에 위치한 개별 사무실 공간을 사용한다고 합니다.\nTranslated: 네, 온라인 광고에 관련된 모든 직원들은 2층에 각자의 사무실 공간이 위치해 있습니다.\n\n\nEnglish   : What happens to the remaining staff?\nReference : 남은 직원들은 어떻게 되나요?\nTranslated: 남은 스태프에게 무슨 일이 일어났나요?\n\n\nEnglish   : The remaining staff will stay in the current space on the 3rd floor, and the division will continue to be called the advertising department.\nReference : 남은 직원들은 3층 현재 자리에 남을 것이고, 부서는 계속 광고 부서로 불린다고 하네요.\nTranslated: 나머지 직원들은 3층 현 공간에 머물게 되며, 이 부서는 계속 광고부서로 불리게 된다.\n\n\nEnglish   : I have a question about the year-end tax adjustment.\nReference : 이번 연말 정산 관련해서 질문이 있어요.\nTranslated: 저는 연말정산에 대해 궁금한 점이 있습니다.\n\n\nEnglish   : Is there any problem?\nReference : 무슨 문제라도 있으신가요?\nTranslated: 혹시 문제가 있나요?\n\n\nEnglish   : I am registering my dependent this time, so do I need to submit any particular documents?\nReference : 제가 이번에 부양가족을 등록하려고 하는데, 따로 제출해야 하는 서류가 있나요?\nTranslated: 이번에 내 국적 등록을 하고 있으니 특별히 서류 제출을 해야 하나요?\n\n\n\n\n두 시간동안 대충 1 에폭만 학습한 것치고는 꽤 그럴듯 하게 번역을 하는 것을 알 수 있다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#마무리",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#마무리",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "이상으로 영어-한국어 번역기를 처음부터 학습시키는 방법을 정리했다. 이 글을 글쓴이가 의도한대로 빠르게 읽고 이해하기 위해서는 트랜스포머에 대한 이해가 선행되야 한다. 트랜스포머에 대한 자세한 설명은 진짜로 주석달린 트랜스포머를 참고하자. 하지만 트랜스포머나 스퀀스 투 시퀀스 모델에 대해 잘 모른다 하더라도 한국어 번역기를 만들고자 할때 느끼는 막막함은 어느정도 해소할 수 있으리라 생각한다.\n이 글을 읽고 코드를 실행해보고 나서 DataCollatorForSeq2Seq나 Seq2SeqTrainer 를 쓰지 않고 직접 이 부분을 만들어서 모델을 학습 시켜본다면 트랜스포머를 이용한 번역 작업기 만들기를 훨씬 더 상세히 이해할 수 있을 것이다."
  }
]