[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML simple works",
    "section": "",
    "text": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers\n\n\n\n\n\n\n\nai\n\n\ncode\n\n\ntransformers\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nJo, Joonu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/jupyter-ipynb/hello.html",
    "href": "posts/jupyter-ipynb/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n이 블로그는 주로 최적화, 머신러닝 따위의 내용을 다루고 있다.\n개인적으로 공부한 내용을 주로 다루며 혹시나 다른 분들께 도움이 될까싶어 가급적 문서는 친절하고 자세하게 적고 있다.\n글은 주로 jupyter notebook으로 작성되며 노트북 파일은 ml-simple-works에 공개 되어 있다.\n혹시나 내용의 오류 지적이나 내용에대한 문의 사항은 metamath@gmail.com으로…"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "트랜스포머가 딥러닝 세상을 지배한 지금 과거 CNN, RNN을 모르고 딥러닝을 안다고 할 수 없듯이 이제는 트랜스포머를 이해하지 못하고 딥러닝을 공부한다고 말할 수 없는 시대가 되었다.\n트랜스포머가 초기 타겟한 작업이 번역이기 때문에 트랜스포머를 설명하는 글에서 단골로 등장하는 예제가 번역기 예제다. 주로 영어-독일어, 영어-스페인어 예제가 많다. 하지만 쉽게 접할 수 있는 알파벳권 언어 사이의 번역기 예제에 비해 영어-한국어 데이터를 사용해서 번역기를 학습시키는 예제는 이상하리만큼 찾아보기 힘들었다. 왜 그런지 이유는 잘 모르겠지만 어쨌든 거의 없다. 그래서 영어-한국어 문장쌍이 들어있는 원시데이터를 사용해 T5 모델로 영어-한국어 번역기를 데모 수준 정도로 학습하는 예제를 만들어 블로그에 포스팅하면 많은 사람들에게 도움이 되지 않을까 해서 이 글을 적게 되었다.\n이 글에서는 트랜스포머에 대한 기초적인 내용은 다루지 않고 오직 데이터를 어떻게 준비하고, 어떻게 데이터를 모델에 입력하여 학습을 시키고, 마지막으로 어떻게 영어로 부터 한국어 번역 문장을 출력시키는가 하는 것에만 초점을 맞추었다. 그리고 코드를 복잡하게 만드는 그 어떤 테크닉도 사용하지 않는다. 오로지 가장 간단하게 한국어 번역기를 구축하는데만 집중할 것이다. 사실 이 글의 대부분 내용은 허깅페이스 도움말에 있는 것을 정리한것에 지나지 않는다. 하지만 입문자나 이제 막 트랜스포머를 이용해서 한국어 번역기를 만들고자 하는 사람들은 허깅페이스 도움말을 보고 이 내용을 모두 정리하기 쉽지 않은 것이 사실이어서 이글이 꽤 도움이 되리라 생각한다.\n\n\n\n\n먼저 허깅페이스의 트랜스포머스 라이브러리를 임포트 한다. 본인 컴퓨터에 허깅페이스 트랜스포머스 라이브러리가 설치되어 있지 않다면 다음 명령어로 설치한다.\n\n!pip install transformers\n\n그 다음 해야할 일은 데이터 셋을 다운받는 것이다. 다음 명령을 실행해서 허깅페이스 Datasets 라이브러리를 설치한다.\n\n!pip install datasets\n\n모두 설치가 완료되었다면 데이터 셋을 다운받아야 한다. 먼저 허깅페이스 사이트에 접속해서 상단 메뉴에 Datasets를 클릭하고 아래 그림처럼 검색 조건을 다음처럼 맞추면\n\n좌측 작은 메뉴에서 Languages를 선택한다.\nLanguages 하단에 보이는 여러 언어중에 Korean을 선택한다.\n다시 우측 검색 필터 창에 en을 적는다.\n\n데이터 셋 네 개가 보이는데 bongsoo/news_talk_en_ko를 사용하도록 하겠다.\n\nbongsoo/news_talk_en_ko를 클릭해서 나오는 화면에서 Files and Versions를 클릭하면 tsv 파일이 보이는데 이 파일에는 영어 문장과 한국어 문장이 한줄에 탭 문자로 구분되어 적혀있다. 로컬 디스크이 이 파일을 다운받고 파일을 읽어보면 다음처럼 확인된다.\n\n!head -5 news_talk_en_ko_train_130000.tsv\n\nSkinner's reward is mostly eye-watering.    스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\nEven some problems can be predicted.    심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\nOnly God will exactly know why. 오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\nBusinesses should not overlook China's dispute. 중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\nSlow-beating songs often float over time.   박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n데이터 파일은 아주 단순한 형태인 것을 알 수 있다. 직접 tsv파일을 다운받아서 사용해도 되나 허깅페이스 허브로 부터 바로 다운받아 사용하는 편이 더 편하다. 다음 명령으로 다운받을 수 있다.\n\n# 데이터 셋을 다운받을 함수를 임포트 한다.\nfrom datasets import load_dataset\n\n\n# 좀 전에 알아본 체크포인트를 사용해서 데이터를 받아온다.\nen_ko = load_dataset(\"bongsoo/news_talk_en_ko\")\n\nUsing custom data configuration bongsoo--news_talk_en_ko-e7f00bc8f76f18d5\nFound cached dataset csv (/home/metamath/.cache/huggingface/datasets/bongsoo___csv/bongsoo--news_talk_en_ko-e7f00bc8f76f18d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n\n\n\n\n\n이제 데이터 객체를 확인해보면 DatasetDict라는 것을 알 수 있고 안에 train 키만 있는 것이 확인된다.\n\nen_ko\n\nDatasetDict({\n    train: Dataset({\n        features: [\"Skinner's reward is mostly eye-watering.\", '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.'],\n        num_rows: 1299999\n    })\n})\n\n\ntrain키에 Dataset 객체가 하나 있는데 features가 첫번째 데이터로 되어있고 행수는 1299999개인 것을 보아 데이터 파일에 컬럼명이 적혀있는 헤더라인이 없어서 첫줄을 헤더로 읽은것 같다. 첫줄을 데이터로 다시 집어 넣고 컬럼명은 en, ko로 설정하기 위해 데이터 셋을 pandas로 읽어드린다.\n\nimport pandas as pd\n\n\n# 허깅페이스 데이터셋을 판다스 포맷으로 세팅\nen_ko.set_format(type=\"pandas\")\n\n\n# 'train'키의 모든 행을 DataFrame df에 할당\ndf = en_ko[\"train\"][:]\n\n# 잘 담겼는지 확인한다.\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Skinner's reward is mostly eye-watering.\n      스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n    \n  \n  \n    \n      0\n      Even some problems can be predicted.\n      심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n    \n    \n      1\n      Only God will exactly know why.\n      오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n    \n    \n      2\n      Businesses should not overlook China's dispute.\n      중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n    \n    \n      3\n      Slow-beating songs often float over time.\n      박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n    \n    \n      4\n      I can't even consider uninsured treatments.\n      보험 처리가 안 되는 비급여 시술은 엄두도 못 낸다.\n    \n  \n\n\n\n\n예상처럼 첫 줄이 헤더가 되었으니 이를 수정한 DataFrame을 만든다.\n\nexample_0 = list(df.columns)\nexample_0\n\n[\"Skinner's reward is mostly eye-watering.\",\n '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.']\n\n\n\nexample_0_df = pd.DataFrame({col: [value] for col, value in zip(('en', 'ko'), example_0)})\n\n\ndf.columns = ('en', 'ko')\n\n\nen_ko_df = pd.concat([example_0_df, df],).reset_index(drop=True)\nen_ko_df.head()\n\n\n\n\n\n  \n    \n      \n      en\n      ko\n    \n  \n  \n    \n      0\n      Skinner's reward is mostly eye-watering.\n      스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n    \n    \n      1\n      Even some problems can be predicted.\n      심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n    \n    \n      2\n      Only God will exactly know why.\n      오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n    \n    \n      3\n      Businesses should not overlook China's dispute.\n      중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n    \n    \n      4\n      Slow-beating songs often float over time.\n      박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n    \n  \n\n\n\n\n이렇게 데이터 셋을 DataFrame으로 만들었다. 이제 이 en_ko_df로 부터 다시 허깅페이스 데이터 셋을 생성하자.\n\nfrom datasets import Dataset\n\n\ndataset = Dataset.from_pandas(en_ko_df)\n\n\ndataset\n\nDataset({\n    features: ['en', 'ko'],\n    num_rows: 1300000\n})\n\n\n다시 데이터 셋을 확인해보면 features가 제대로 표시되고 샘플 수도 1300000개 인것을 확인할 수 있다.\n이렇게 만들어진 DataFrame으로 부터 데이터 셋이 잘 초기화되는 것을 확인했으니 en_ko_df를 세조각으로 쪼개서 tsv파일로 저장하자.\n\n# 각 데이터 셋의 샘플수를 정한다.\nnum_train = 1200000\nnum_valid = 90000\nnum_test = 10000\n\n설정된 크기만큼 DataFrame을 자른다.\n\nen_ko_df_train = en_ko_df.iloc[:num_train]\n\n\nen_ko_df_valid = en_ko_df.iloc[num_train:num_train+num_valid]\n\n\nen_ko_df_test = en_ko_df.iloc[-num_test:]\n\n다시 tsv파일로 저장한다.\n\nen_ko_df_train.to_csv(\"train.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_valid.to_csv(\"valid.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_test.to_csv(\"test.tsv\", sep='\\t', index=False)\n\n이렇게 tsv파일 세개로 데이터를 정리했다. 이제 필요할때 이 파일을 읽어 허깅페이스 데이터셋을 만들 수 있다.\n아래처럼 스플릿을 정의한 사전을 load_dataset에 넘기면 된다. 이때 delimiter를 탭 문자로 지정해야 한다.\n\ndata_files = {\"train\": \"train.tsv\", \"valid\": \"valid.tsv\", \"test\": \"test.tsv\"}\n\n\ndataset =  load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n\nUsing custom data configuration default-d166a7cf4252597d\n\n\nDownloading and preparing dataset csv/default to /home/metamath/.cache/huggingface/datasets/csv/default-d166a7cf4252597d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset csv downloaded and prepared to /home/metamath/.cache/huggingface/datasets/csv/default-d166a7cf4252597d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n\n\n\n\n\n제대로 로딩되었는지 dataset을 확인해보자.\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 10000\n    })\n})\n\n\nDatasetDict에 train, valid, test 키로 120만 문장, 9만 문장, 1만 문장이 저장된 것을 확인할 수 있다.\n이 데이터 셋에서 개별 샘플에 대한 접근은 [split][feature][row num] 형태로 가능하다.\n\n# train 스플릿에서 영어 3개와 한국어 3개 샘플을 가져온다.\nprint(dataset['train']['en'][:3], dataset['train']['ko'][:3])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n그런데 feature와 row num은 순서를 바꿔서 사용할 수 도 있다.\n\nprint(dataset['train'][:3]['en'], dataset['train'][:3]['ko'])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n데이터를 어떻게 조회하는지는 데이터 구성 방식에 따라 조금씩 다르므로 데이터 셋을 보고 몇번 해보면 금방 접근법을 알 수 있다.\n\n\n\n데이터 셋 준비를 마쳤으니 학습할 차례이다. 허깅페이스에서 제공하는 필요 클래스를 임포트 한다.\n먼저 선학습 모델을 사용하기 위한 클래스를 임포트 한다. AutoTokenizer는 선학습된 모델이 사용한 토크나이저를 읽기 위해 필요하며 AutoModelForSeq2SeqLM은 시퀀스 투 스퀀스 방식으로 작동하는 선학습된 모델을 불러 올 때 마지막에 분류기 헤드를 붙여서 모델을 로딩하기 위해 사용한다.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n2023-02-28 20:50:48.785319: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-28 20:50:48.886805: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-02-28 20:50:48.903288: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-02-28 20:50:49.229160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-02-28 20:50:49.229214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-02-28 20:50:49.229217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n\n다음은 데이터 콜레이터를 임포트한다. 시쿼스 투 시퀀스 학습 과정은 인코더 입력 시퀀스, 디코더 입력 시퀀스, 디코더 출력 시퀀스를 필요로 하는데 미니배치로 부터 이를 적절히 정리해서 모델에 입력하는 작업이 필요하다. 예를 들면 미니 배치 내에 있는 인코더 입력 시퀀스의 길이를 맞춘다든지 디코더 입력시퀀스를 오른쪽으로 한칸 쉬프트시켜 디코더 출력 시퀀스를 만드는 작업등이 콜레이터에서 일어나는 작업인데 이런 작업을 DataCollatorForSeq2Seq가 자동으로 처리하게 된다.\n\nfrom transformers import DataCollatorForSeq2Seq\n\n그리고 학습에 필요한 클래스를 임포트 한다. 학습에 필요한 설정을 Seq2SeqTrainingArguments에 정의하고 실제 학습은 Seq2SeqTrainer로 하게 된다. Seq2SeqTrainer는 generate()함수를 제공한다.\n\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n허깅페이스 라이브러리로는 마지막으로 데이터 셋을 로딩하는 함수와 번역 결과를 측정할 함수를 로딩한다.\n\nfrom datasets import load_dataset, load_metric\n\n그외 필요한 각종 라이브러리를 임포트 한다.\n\nimport numpy as np\nimport torch\nimport multiprocessing\n\n허깅페이스에서 파이토치 기반 구현을 사용하므로 gpu가 있다면 device를 세팅한다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\n미리 학습된 모델의 체크포인트를 세팅한다. 여기서 사용할 모델은 한국어와 영어에 미리 학습된 KE-T5모델을 사용한다. T5모델은 트랜스포머의 인코더, 디코더 구조를 모두 사용하는 모델로 번역기를 만들 때 사용할 수 있는 모델이다. 아래처럼 모델 체크 포인트와 T5 모델에 입력될 최대 토큰 길이를 설정한다.\n\nmodel_ckpt = \"KETI-AIR/ke-t5-base\"\nmax_token_length = 64\n\n\n\n\n먼저 모델 체크 포인트를 사용하여 KE-T5 모델이 학습할때 함께 사용한 토크나이저를 불러온다. 허깅페이스 트랜스포머스 라이브러리를 사용할 때 가장 핵심이 되며 익숙해지기 쉽지 않은 부분이 이 토크나이저라고 개인적으로 생각한다.\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n\n토크나이저를 로딩할때 sentencepiece가 없다고 에러가 나면 아래 명령으로 토크나이저가 사용하는 라이브러리를 설치하고 다시 로딩한다.\n!pip install sentencepiece\n토크나이저를 불러왔으니 현재 사용하는 데이터셋에서 샘플을 가져와 토크나이징해보는 것이 좋을 것이다. 학습 세트에서 10번 샘플을 가지고 실험해보자. 먼저 10번 샘플을 뿌려보고\n\ndataset['train'][10]['en'], dataset['train'][10]['ko']\n\n('Any academic achievement requires constant repetition.',\n '어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.')\n\n\n토크나이저에 각 문장을 입력하고 토큰화된 상태로 돌려 받는다.\n\ntokenized_sample_en = tokenizer(dataset['train'][10]['en'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_en\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenized_sample_ko = tokenizer(dataset['train'][10]['ko'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_ko\n\n{'input_ids': [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n문장에 토큰으로 쪼개지고 각 토큰이 숫자로 변환된 것을 볼 수 있다. 이렇게 숫자화된 토큰을 input_ids로 반환하고 추가로 트랜스포머 인코더, 디코더에 쓰일 패딩 마스크도 함께 attention_mask로 돌려준다. 마스크가 모두 1인 이유는 샘플이 하나밖에 없어서 이다. 샘플 몇개를 더 실험해보면\n\ntokenizer(dataset['train'][:3]['en'], \n          max_length=max_token_length, \n          padding=True, truncation=True)\n\n{'input_ids': [[388, 6809, 2952, 17, 8, 32204, 43, 8023, 6687, 28, 9495, 91, 3, 1], [4014, 322, 3170, 147, 67, 23274, 3, 1, 0, 0, 0, 0, 0, 0], [11783, 4412, 96, 6556, 709, 1632, 3, 1, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}\n\n\n미니배치에 있는 샘플의 최대길이메 맞춰서 패딩되는 모습을 확인할 수 있다. 실제로 어떻게 토큰화 되었는지 확인해보자.\n\npd.DataFrame(\n    [\n        tokenized_sample_en['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_en['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      ids\n      13941\n      10114\n      25542\n      9361\n      20526\n      742\n      32268\n      12520\n      3\n      1\n    \n    \n      tokens\n      ▁Any\n      ▁academic\n      ▁achievement\n      ▁requires\n      ▁constant\n      ▁re\n      pet\n      ition\n      .\n      </s>\n    \n  \n\n\n\n\n\npd.DataFrame(\n    [\n        tokenized_sample_ko['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_ko['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n    \n  \n  \n    \n      ids\n      404\n      12663\n      15\n      10775\n      2334\n      6\n      15757\n      21\n      29819\n      1736\n      26778\n      4342\n      15\n      1701\n      3\n      1\n    \n    \n      tokens\n      ▁어떤\n      ▁학문\n      이\n      든지\n      ▁일정\n      의\n      ▁성취\n      를\n      ▁이루기\n      ▁위해서는\n      ▁끊임없는\n      ▁반복\n      이\n      ▁필요하다\n      .\n      </s>\n    \n  \n\n\n\n\nKE-T5를 학습할때 학습된 규칙대로 토큰화가 진행된다. 영어에서 repetition은 re, pet, ition으로 쪼개진 것을 볼 수 있고, 한국어에서 성취를은 성취, 를로 쪼개지고 학문이든지는 학문, 이, 든지로 쪼개진것을 볼 수 있다. 토큰 앞에 _표시는 이 토큰 앞에는 공백이 있어야 한다는 의미다. 그리고 마지막에 엔드 토큰인 </s>가 항상 붙게 되는 것도 확인할 수 있다.\n이제 앞서 tsv파일로 부터 로딩한 dataset내의 문장을 모두 토크나이저를 사용해서 숫자로 바꾸는 작업을 해야 한다. 즉 문자로된 문장을 숫자로 바꿔 특성화 해야 한다. dataset.map()함수에 각 샘플을 토큰화 하는 함수를 만들어 전달하면 map()이 모든 샘플에 대해 전달받은 함수를 적용하게 되는데 함수는 이렇게 작성하면 된다.\n\ndef convert_examples_to_features(examples):\n    ###########################################################################\n    # with 쓰는 옛날 방식\n    # input_encodings = tokenizer(examples['en'], \n    #                             max_length=max_token_length, truncation=True)\n    \n    # Setup the tokenizer for targets\n    # with tokenizer.as_target_tokenizer():\n    # target_encodings = tokenizer(text_target=examples['ko'], \n    #                             max_length=max_token_length, truncation=True)\n    #\n    #\n    # return {\n    #     \"input_ids\": input_encodings[\"input_ids\"],\n    #     \"attention_mask\": input_encodings[\"attention_mask\"],\n    #     \"labels\": target_encodings[\"input_ids\"]\n    # }\n    \n    # 그런데 이렇게 하면 인풋하고 한번에 처리 가능함.\n    model_inputs = tokenizer(examples['en'],\n                             text_target=examples['ko'], \n                             max_length=max_token_length, truncation=True)\n    \n    return model_inputs\n\nconvert_examples_to_features()가 하고 싶은 일은 dataset에 있는 “어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.”라는 샘플 문장을 [404,12663,15,10775,2334,6,15757,21,29819,1736,26778,4342,15,1701,3,1]라는 정수로 바꾸는 것이다. convert_examples_to_features()가 dataset에 적용될 때 넘겨 받는 examples는 다음과 같이 넘어 온다.\nexamples= {'en':['sent1', 'sent2', ... , 'sent1000'], # 이건 문장 1000개짜리 리스트\n           'ko':['sent1', 'sent2', ... , 'sent1000']}\n기본으로 미니 배치 사이즈는 1000으로 세팅되어 있다.(함수 기본인자는 여기서 확인 가능)\n미니 배치로 넘어온 문장 샘플을 영어 문장과 한국어 문장을 각각 인풋과 타겟으로 토큰화하고 이로 부터 input_ids, attention_mask, labels로 묶어 리턴하는 방식이 예전에 쓰던 방식으로 함수 위쪽에 주석처리 되어 있다. 타겟 문장을 토큰화 할 때 타겟에서 필요로 하는 특수 토큰을 추가하는 경우 이를 처리하기위해 타겟 토큰 토큰화 때는 with tokenizer.as_target_tokenizer():라는 컨텍스트 매니저를 사용했는데 최근 업데이트에서는 그냥 tokenizer에 text_target인자에 타겟 문장을 넣어서 한번에 다 처리할 수 있다. 이렇게 model_inputs을 반환하면 dataset에 있던 각 레코드 마다 en, ko 특성에 추가로 input_ids, attention_mask, labels 특성이 더 추가 되게 된다. 사실 en, ko 특성은 더이상 필요없기 때문에 convert_examples_to_features()를 적요할 때 없애라는 인자를 세팅한다. 바로 dataset에 함수를 적용해보자.\n그냥 해도되나 좀 더 빠르게 하기 위해 num_proc 인자에 스레드 개수를 지정한다.\n\nNUM_CPU = multiprocessing.cpu_count() // 2\nNUM_CPU\n\n10\n\n\n그리고 remove_columns 인자에 기존 특성 이름인 en, ko를 전달해서 기존 특성은 제거하게 한다. 이 특성이 있으면 이후 콜레이터가 샘플들을 미니 배치로 묶을 때 패딩처리를 못하게 된다.\n\ntokenized_datasets = dataset.map(convert_examples_to_features, \n                                 batched=True, \n                                 # 이걸 쓰지 않으면 원 데이터 'en', 'ko'가 남아서\n                                 # 아래서 콜레이터가 패딩을 못해서 에러남\n                                 remove_columns=dataset[\"train\"].column_names,\n                                 num_proc=NUM_CPU) \n\n                    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconvert_examples_to_features()이 dataset의 모든 샘플에 다 적용되고 나면 tokenized_datasets는 다음처럼 된다.\n\ntokenized_datasets\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 10000\n    })\n})\n\n\n기존에 있던 특성 en, ko는 사라졌고 en은 input_ids와 attention_mask로 ko는 labels로 바뀐것을 확인할 수 있다. 예를 들어 학습 세트에 10번 데이터를 보면 다음처럼 다 숫자라 바뀌게 된것이다.\n\ntokenized_datasets['train'][10]\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'labels': [404,\n  12663,\n  15,\n  10775,\n  2334,\n  6,\n  15757,\n  21,\n  29819,\n  1736,\n  26778,\n  4342,\n  15,\n  1701,\n  3,\n  1]}\n\n\n토크나이저를 써서 숫자로 부터 토큰화 해보면 다음과 같다.\n\nprint( '원 데이터    :', dataset['train'][10]['en'] )\nprint( '처리 후 데이터:', tokenized_datasets['train'][10]['input_ids'] )\nprint( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['input_ids']) )\n\nprint('\\n')\nprint( '원 데이터    :', dataset['train'][10]['ko'] )\nprint( '처리 후 데이터:', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['labels']) )\nprint( '토큰화       :', tokenized_datasets['train'][10]['labels'] )\n\n원 데이터    : Any academic achievement requires constant repetition.\n처리 후 데이터: [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1]\n토큰화       : ['▁Any', '▁academic', '▁achievement', '▁requires', '▁constant', '▁re', 'pet', 'ition', '.', '</s>']\n\n\n원 데이터    : 어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.\n처리 후 데이터: ['▁어떤', '▁학문', '이', '든지', '▁일정', '의', '▁성취', '를', '▁이루기', '▁위해서는', '▁끊임없는', '▁반복', '이', '▁필요하다', '.', '</s>']\n토큰화       : [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1]\n\n\n데이터 특성화를 모두 마쳤으므로 이제 모델을 로딩하자. AutoModelForSeq2SeqLM를 사용해서 선학습 모델을 불러오면 선학습된 T5모델 마지막에 파인튜닝할 수 있는 분류 헤드를 붙인 모델을 반환한다.\n\n\n\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n위처럼 모델을 로딩하고 모델 출력 시켜보면 T5 모델 레이어가 매우 길게 출력되는데 제일 마지막 부분에 다음과 같이 분류 헤드가 붙어 있는 것을 확인할 수 있다. 헤드를 보면 모델에서 출력하는 벡터는 768차원이고 이를 단어장 사이즈인 64128로 변환시키고 있는 것을 알 수 있다.\n(lm_head): Linear(in_features=768, out_features=64128, bias=False)\n이렇게 생성된 model은 인코더-디코더 구조를 가지는 트랜스포머이므로 이 모델을 포워딩 하려면 인코더 인풋과 디코더 인풋을 넣어줘야 한다. 모델을 만들고 가장 먼저해야되는 작업은 포워딩 테스트라고 개인적으로 생각한다. 임의의 입력을 넣고 출력이 의도대로 나오는지 확인하는 것이다. 이런 작업은 직접 만든 모델이 아닐 수록 중요한데 이렇게 해야지 모델이 제대로 작동하는지 또 어떤 구조로 되어 있는지 쉽게 이해할 수 있기 때문이다. 포워드 테스트를 하기위해 간단한 영어문장으로 예제를 준비한다.\n\nencoder_inputs = tokenizer(\n    [\"Studies have been shown that owning a dog is good for you\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\ndecoder_targets = tokenizer(\n    [\"개를 키우는 것이 건강에 좋다는 연구 결과가 있습니다.\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\n영어 문장은 인코더의 입력이 되고 한국어 문장은 디코더의 타겟이 된다. 아래처럼 모두 숫자로 변환되어 있다.\n\nprint( encoder_inputs )\nprint( decoder_targets )\n\ntensor([[24611,    84,   166,  8135,    38,   847,    91,    16,  8146,    43,\n           667,    40,   106,     1]], device='cuda:0')\ntensor([[15833, 12236,   179, 16120, 28117,  1007,  3883,   327,     3,     1]],\n       device='cuda:0')\n\n\n이제 디코더 입력을 만들기위해 model._shift_right를 사용해 디코더 출력을 오른쪽으로 쉬프트 시킨다.\n\ndecoder_inputs = model._shift_right(decoder_targets)\n\nshifted와 decoder_inputs이 어떻게 다른지 비교해보면\n\npd.DataFrame(\n    [\n        tokenizer.convert_ids_to_tokens(decoder_targets[0]),\n        tokenizer.convert_ids_to_tokens(decoder_inputs[0])\n    ],\n    index=('decoder target', 'decoder input')\n)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      decoder target\n      ▁개를\n      ▁키우는\n      ▁것이\n      ▁건강에\n      ▁좋다는\n      ▁연구\n      ▁결과가\n      ▁있습니다\n      .\n      </s>\n    \n    \n      decoder input\n      <pad>\n      ▁개를\n      ▁키우는\n      ▁것이\n      ▁건강에\n      ▁좋다는\n      ▁연구\n      ▁결과가\n      ▁있습니다\n      .\n    \n  \n\n\n\n\n위처럼 오른쪽으로 쉬프트된 디코더 입력은 <pad> 토큰이 추가되었다. 이렇게 출력으로 쓰이는 문장을 오른쪽으로 쉬프트시켜 티처포싱Teacher forcing을 진행하게 된다. 다음처럼 model에 인코더 입력, 디코더 입력, 디코더 타겟을 입력하고 포워드 시킨다.\n\n# forward pass\noutputs = model(input_ids=encoder_inputs, \n                decoder_input_ids=decoder_inputs, \n                labels=decoder_targets)\n\nmodel의 outputs에는 다음과 같은 키가 있다.\n\noutputs.keys()\n\nodict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])\n\n\n손실함수 값을 다음처럼 확인할 수 있고 grad_fn이 있기 때문에 output.loss를 백워드 시킬 수 있다.\n\noutputs.loss\n\ntensor(87.8185, device='cuda:0', grad_fn=<NllLossBackward0>)\n\n\n인코더의 마지막 상태는 (1, 14, 768)이다. 각 숫자는 순서대로 샘플 수, 스탭 수, 모델 사이즈를 나타낸다. 즉 인코더로 들어가는 14개 토큰이 각각 768차원 벡터로 인코딩되었다.\n\noutputs['encoder_last_hidden_state'].shape\n\ntorch.Size([1, 14, 768])\n\n\nlogit은 디코더 입력 토큰 10개에 대한 그 다음 토큰 예측 10개를 담고있다. 샘플 한개에 대해서 10개 토큰에 대해서 64128개 단어에 대한 확률값이 들어 있다.\n\noutputs['logits'].shape\n\ntorch.Size([1, 10, 64128])\n\n\nlogit에 argmax를 씌워서 토큰화시켜보면 다음과 같다.\n\ntokenizer.convert_ids_to_tokens( torch.argmax(outputs['logits'][0], axis=1).cpu().numpy() )\n\n['큐브', '큐브', '▁비일비재', '▁비일비재', '▁베네', '▁비일비재', '▁베네', '▁베네', '큐브', '큐브']\n\n\n마지막 헤더가 학습이 되지 않았기 때문에 적절한 아웃풋이 나오지 않지만 입력과 출력의 텐서 모양을 보면 포워드 패스가 제대로 작동한다는 것을 알 수 있다.\n\n\n\n\n\n\nBLEU 점수는 번역기가 생성한 문장이 레퍼런스(정답이라는 표현을 사용하지 않는 이유는 제대로 된 번역 문장이 오직 하나가 아니기 때문)문장과 얼마나 비슷한지 측정하는 점수라고 생각하면 된다. 단 같은 단어가 반복된다든지 레퍼런스 문장보다 너무 짧은 문장을 생성한다든지 하면 패널티를 부여 한다. 그렇기 때문에 길이가 최대한 비슷하고 다양한 단어를 사용하면서 생성된 문장의 단어가 레퍼런스 단어에 많이 보여야 높은 점수를 얻게 된다.\nBLEU를 계산하기 위해 다음 명령어로 허깅페이스 evaluate 라이브러리를 설치하고 sacrebleu라이브러리도 함께 설치하자.\n\n!pip install evaluate\n\n\n!pip install sacrebleu\n\n\nimport evaluate\n\nmetric = evaluate.load(\"sacrebleu\")\n\n\npredictions = [\n    \"저는 깊이 있는 학습을 좋아해요.\",\n    \"딥러닝 틀이 잘 개발되기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 11.532935342795522,\n 'counts': [12, 3, 1, 0],\n 'totals': [21, 19, 17, 15],\n 'precisions': [57.142857142857146,\n  15.789473684210526,\n  5.882352941176471,\n  3.3333333333333335],\n 'bp': 1.0,\n 'sys_len': 21,\n 'ref_len': 21}"
  }
]