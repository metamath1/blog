[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "이 블로그는 주로 최적화, 머신러닝 따위의 내용을 다루고 있다.\n개인적으로 공부한 내용을 주로 다루며 혹시나 다른 분들께 도움이 될까 싶어 가급적 문서는 친절하고 자세하게 적고 있다.\n글은 주로 jupyter notebook으로 작성되며 노트북 파일은 ml-simple-works에 공개 되어 있다.\n2022년까지 내용은 https://metamath1.github.io 에서 볼 수 있다.\n혹시나 내용의 오류 지적이나 내용에 대한 문의 사항은 metamath@gmail.com으로…\n저작권 관련 사항\n본 블로그에 내용은 얼마든지 인용하고 사용해도 좋으나 출처는 꼭 밝혀 주시면 감사하겠다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML simple works",
    "section": "",
    "text": "A Gentle Introduction to Diffusion Model: Part 2-2 DDPM Hands-on with Pytorch\n\n\n\n\n\n\n\nai\n\n\nmachine learning\n\n\ngenerative model\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to Diffusion Model: Part 2-1 DDPM Hands-on with TensorFlow\n\n\n\n\n\n\n\nai\n\n\nmachine learning\n\n\ngenerative model\n\n\ntensorflow\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to Diffusion Model: Part 1 - DDPM\n\n\n\n\n\n\n\nai\n\n\nmachine learning\n\n\ngenerative model\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to Creating an English-to-Korean translator with Transformers\n\n\n\n\n\n\n\nai\n\n\nT5\n\n\ntransformers\n\n\nhugging face\n\n\n번역기\n\n\n한국어 번역기\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to Gradient Boosting\n\n\n\n\n\n\n\nai\n\n\nmachine learning\n\n\nboosting\n\n\ngradient boosting\n\n\nensemble\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2022\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\n  \n\n\n\n\nA Step by Step Introduction to EM Algorithm\n\n\n\n\n\n\n\nai\n\n\nmachine learning\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\nJo, Joonu (metamath@gmail.com)\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html",
    "href": "posts/gradientboost/gradient_boosting.html",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nimport graphviz\nfrom sklearn.tree import export_graphviz\nfrom imageio.v2 import imread\n\nimport plotly.graph_objects as go # colab\n# from plotly import graph_objs as go # local, old version plotly\n# import plotly.io as pio\n# pio.renderers.default = \"notebook\"\n\ncm2 = ListedColormap(['C1', 'C2'])\ncm3 = ListedColormap(['C1', 'C2', 'C3'])\ncm8 = ListedColormap(['#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087', '#f95d6a', \n                      '#ff7c43', '#ffa600'])\nimage_dpi=150"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#들어가며",
    "href": "posts/gradientboost/gradient_boosting.html#들어가며",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "들어가며",
    "text": "들어가며\n부스팅 알고리즘은 약한 학습기weak learner를 순차적으로 학습시키고 개별 학습기의 예측을 모두 더해 최종 예측을 만들어내는 앙상블 메소드의 한 종류입니다. 그 중 그래디언트 부스팅은 강력한 성능으로 가장 많이 애용되는 알고리즘입니다.\n인기가 많은 알고리즘이다 보니 많은 머신러닝 교재에서도 꼭 등장하는 편입니다. 주로 이전 단계 학습기가 만들어낸 잔차residual을 타겟으로 개별 학습기를 순차적으로 학습하는 방법을 보여주면서 직관적으로 알고리즘을 설명합니다. 하지만 이런 방식으로는 왜 ’그래디언트’라는 단어가 등장하는지 잘 설명되지 않습니다. 그리고 어떻게 어떻게 설명한다 하더라도 주로 회귀문제에 국한하여 설명하는 것이 대부분입니다. 이렇게 많은 설명이 피상적인 수준에 머무르는 이유는 이 알고리즘을 제대로 설명하기 위해서 좀 복잡한 최적화 방법을 이야기해야하기 때문입니다.\n하지만 결론적으로 이야기하면 회귀문제를 중심으로 간단하게 그래디언트 부스팅을 이해하면 실용적으로 알고리즘을 활용하기에 충분하다 할 수 있습니다. 왜냐하면 분류문제가 된다하더라도 회귀문제의 논리가 동일하게 적용되기 때문입니다. 실제로 회귀에 사용되는 오차제곱합 손실을 사용하는 방식을 그대로 분류 문제에 적용해도 잘 작동합니다. 굳이 분류문제를 예로 들지 않아도 크게 달라지는건 없습니다. 이런 이유로 우선 회귀문제를 중심으로 최대한 간단하게 그래디언트 부스팅을 이해한 다음 분류문제에 대한 구체적인 이야기를 이어가도록 하겠습니다.\n회귀문제를 설명하는 좋은 예제는 다음과 같습니다.1\n\n\n아래 예제는 핸즈온 머신러닝에 나온 예를 참고하였습니다.\n\n아래 링크를 클릭해서 코랩에서 직접 실행하면서 글을 읽을 수 있습니다.\n\n이 글은 국가수리과학연구소 초청세미나 발표 목적으로 작성되었으며 발표 슬라이드는 여기에서 볼 수 있다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#간단한-회귀문제-예",
    "href": "posts/gradientboost/gradient_boosting.html#간단한-회귀문제-예",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "간단한 회귀문제 예",
    "text": "간단한 회귀문제 예\n\n# 2차식으로 만든 데이터셋 + 잡음\nnp.random.seed(42)\nX_reg = np.random.rand(100, 1) - 0.5\ny_reg = 3*X_reg[:,0]**2 + 0.05 * np.random.randn(100)\n\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k')\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step1.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위와 같은 데이터에 결정회귀트리를 적합시키겠습니다. 처음에는 타겟의 평균만 출력하는 더미 학습기로 출발합니다.\n\n# 개별학습기 로드\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.dummy import DummyRegressor\n\n\n# 아주 약한 0번째 학습기를 한번 학습시킨다.\nh0 = DummyRegressor(strategy=\"mean\").fit(X_reg, y_reg)\n\n# 예측하고 잔차를 구한다.\ny_h0 = h0.predict(X_reg)\nr0 = y_reg - y_h0\n\n# 결과를 그림으로 확인\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True, dpi=100)\n\nax[0].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k')\nax[0].plot(x, h0.predict(x.reshape(-1,1)), color='C2')\nax[0].set_title('h0(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y')\nax[0].spines[\"right\"].set_visible(False)\nax[0].spines[\"top\"].set_visible(False)\n\nax[1].scatter(X_reg, r0, marker='o', color='C3', edgecolor='k')\nax[1].set_title('Residual')\nax[1].set_xlabel('x')\nax[1].set_ylabel('Residual_0')\nax[1].spines[\"right\"].set_visible(False)\nax[1].spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h0.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위 그래프에서 왼쪽 그림은 주어진 데이터와 초기 더미 학습기 \\(h_0(x)\\)의 출력을 나타내고 오른쪽 그림은 더미 학습기가 만든 출력으로부터 잔차residual을 계산하여 그린 것입니다. 두번째 학습기는 첫번째 학습기가 만든 이 오차를 타겟으로 학습하게 됩니다. 계속 이런식으로 반복합니다.\n\n# 아주 약한 1번째 학습기를 한번 학습시킨다.\n# X_reg에 대한 타겟을 이번에는 r0로 설정해서 학습한다.\n# 또 개별 학습기는 max_depth=2로 설정하여 그리 깊지 않은 트리가 생성되게 한다.\nh1 = DecisionTreeRegressor(max_depth=2).fit(X_reg, r0)\n\n# 예측하고 잔차를 구한다.\ny_h1 = h1.predict(X_reg)\nr1 = r0 - y_h1\n\n# 결과를 그림으로 확인\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True, dpi=100)\n\nax[0].scatter(X_reg, r0 , marker='o', color='C3', edgecolor='k')\nax[0].plot(x, h1.predict(x.reshape(-1,1)), color='C2')\nax[0].set_title('h1(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('Residual_0')\nax[0].spines[\"right\"].set_visible(False)\nax[0].spines[\"top\"].set_visible(False)\n\nax[1].scatter(X_reg, r1, marker='o', color='C4', edgecolor='k')\nax[1].set_title('Residual')\nax[1].set_xlabel('x')\nax[1].set_ylabel('Residual_1')\nax[1].spines[\"right\"].set_visible(False)\nax[1].spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위 그림에서 왼쪽 그림은 데이터로 이전 그림의 오른쪽에 그려진 잔차를 그리고 이에 대한 \\(h_1(x)\\)의 출력을 나타낸 것입니다. 오른쪽은 이렇게 \\(h_1(x)\\)의 출력으로 부터 다시 에러를 계산하여 그린 것입니다. 한번 더 반복하겠습니다.\n\n# 아주 약한 2번째 학습기를 한번 학습시킨다.\nh2 = DecisionTreeRegressor(max_depth=2).fit(X_reg, r1)\n\n# 예측하고 잔차를 구한다.\ny_h2 = h2.predict(X_reg)\nr2 = r1 - y_h2\n\n# 결과를 그림으로 확인\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True, dpi=100)\n\nax[0].scatter(X_reg, r1 , marker='o', color='C4', edgecolor='k')\nax[0].plot(x, h2.predict(x), color='C2')\nax[0].set_title('h2(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('Residual_1')\nax[0].spines[\"right\"].set_visible(False)\nax[0].spines[\"top\"].set_visible(False)\n\nax[1].scatter(X_reg, r2, marker='o', color='C5', edgecolor='k')\nax[1].set_title('Residual')\nax[1].set_xlabel('x')\nax[1].set_ylabel('Residual_2')\nax[1].spines[\"right\"].set_visible(False)\nax[1].spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h2.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n이제 세 학습기에 \\([-0.5,0.5]\\)를 입력하여 출력을 구해서 모두 더해 최종 출력을 만들어 냅니다. 이렇게 약한 학습기 세 개를 더한 최종 예측이 얼마나 잘 맞는지 확인해보겠습니다.\n\n# 학습기 0번, 1번, 2번을 다 더한다.\npred = ( h0.predict(x) + h1.predict(x) + h2.predict(x) )\n\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k')\nax.plot(x, pred, color='C2')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title(\"h0(x)+h1(x)+h2(x)\")\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\n\nplt.show()\n\n# fig.savefig(\"first_step_h0h1h2.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n데이터의 전체 구조를 잘 따라가는 것을 확인할 수 있습니다.\n이런 방식은 0번 학습기가 틀린만큼 1번 학습기가 예측하여 더해주고 1번 학습기 조차 틀린 부분을 2번 학습기가 보완해주는 방식으로 진행합니다. 이런 방식으로 결과가 좋아진다는 것을 직관적으로 이해할 수 있는데 여기까지 설명으로는 그래디언트가 어느 장면에서 등장하는지 알 수 없습니다.\n많은 공개 강의나 블로그 글에서 이 후 그래디언트가 등장하는 장면을 간략하게 설명하는데 이 글에서는 조금 더 자세하게 이야기 해보도록 하겠습니다.\n참고로 앞으로 나오는 식번호에 (1) 모양으로 붙은 식번호는 그래디언트 부스팅 원논문의 식번호를 그대로 쓴 것이며 이 글에서 사용하는 식번호는 ([1])식으로 붙였습니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#매개변수-서치",
    "href": "posts/gradientboost/gradient_boosting.html#매개변수-서치",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "매개변수 서치",
    "text": "매개변수 서치\n벡터 변수 \\(\\mathbf{x}\\)와 그에 대응되는 타겟값 \\(y\\)가 있을 때 지도학습의 목표는 입력 \\(\\mathbf{x}\\)를 적절히 출력 \\(y\\)로 매핑하는 함수 \\(F(\\mathbf{x})\\)를 찾는 것입니다. 그렇게 찾아진 최적의 함수를 \\(F^{*}(\\mathbf{x})\\)라하면 이는 개념적으로 다음처럼 쓸 수 있습니다.\n\\[\nF^{*} = \\underset{F}{\\text{argmin}} \\, \\mathbb{E}_{y,\\mathbf{x}} \\left[ L(y, F(\\mathbf{x}) \\right] \\tag{1}\n\\]\n위 식에서 \\(L\\)은 \\(y\\)와 \\(F\\)의 차이를 설명하는 적절한 손실함수loss function입니다. 그러면 식(1)은 이 함수가 정의된 변수들의 존재하는 모든 경우에 대해 평균을 낸 것이 되고 이를 최소화하는 \\(F(\\mathbf{x})\\)를 찾는 것이 됩니다.\n보통의 경우 \\(F(\\mathbf{x})\\)를 특정한 형태의 함수로 제한하고 그 제한된 범위에서 함수를 결정하는 매개변수parmeter \\(\\mathbf{P}\\)를 찾게 됩니다. 다시말해 \\(F(\\mathbf{x};\\mathbf{P})\\)에서 \\(\\mathbf{P}\\)를 결정하는 문제를 다루게 됩니다. 예를들어 \\(F(\\mathbf{x};\\mathbf{P})\\)가 \\(p\\)차 다항식이라면 매개변수는 \\(p+1\\)개의 개수가 될것입니다. 또는 \\(F(\\mathbf{x};\\mathbf{P})\\)가 인공신경망이라면 신경망의 형태(네트워크 구조)를 고정하고 신경망의 매개변수를 결정하는 것이 될 것입니다.\n그래디언트 부스팅에서는 함수 \\(F(\\mathbf{x};\\mathbf{P})\\)를 다음처럼 매개변수 \\(\\mathbf{a}\\)에 의해 결정되는 \\(M\\)개의 일반적인 함수 \\(h\\)의 합으로 이뤄지는 함수라고 설정합니다.\n\\[\nF\\left(\\mathbf{x}; \\{\\beta_m, \\mathbf{a}_m\\}^M_{m=1}\\right) = \\sum_{m=1}^M \\beta_m h (\\mathbf{x}; \\mathbf{a}_m)\n\\]\n이렇게 정의된 문제에서 매개변수 공간을 서치하기 위해 일반적인 최적화 과정을 도입할 수 있습니다. 함수 \\(F(\\mathbf{x})\\)를 고정하고 매개변수를 최적화 할 것이기 때문에 매개 변수에 대한 손실함수를 다음처럼 적으면\n\\[\n\\Phi(\\mathbf{P}) = \\mathbb{E}_{y, \\mathbf{x}} \\left[ L(y, F(\\mathbf{x}; \\mathbf{P}) \\right]\n\\]\n최적의 매개변수는 다음처럼 찾아야 하고\n\\[\n\\mathbf{P}^* = \\underset{\\mathbf{P}}{\\text{argmin}} \\, \\Phi (\\mathbf{P}) \\tag{3}\n\\]\n이렇게 결정된 \\(\\mathbf{P}^*\\)를 이용해 다음처럼 \\(F^* (\\mathbf{x})\\)를 구할 수 있습니다.\n\\[\nF^{*} (\\mathbf{x}) = F(\\mathbf{x}; \\mathbf{P}^{*} )\n\\]\n위 식에서 \\(\\mathbf{P}^{*}\\)를 결정하기 위해 식(3)을 최적화 과정을 통해 풀어야 하는데 \\(M+1\\)번 단계를 거쳐 결정된 \\(\\mathbf{P}^{*}\\)를 다음처럼 쓸 수 있습니다.\n\\[\n\\mathbf{P}^{*} = \\sum_{m=0}^{M} \\mathbf{p}_m \\tag{4}\n\\]\n식(4)에서 \\(\\mathbf{p}_m\\)은 각 최적화 단계에서 결정되는 업데이트 벡터입니다. 그래서 \\(\\mathbf{p}_m\\)은 최적화 수법에 따라 정의될텐데 여기서는 가장 간단한 최속강하법steepest descent method를 고려합니다.\n\n최속강하법\n최속강하법은 최적화 알고리즘에가 가장 간단한 형태이나 최근 딥러닝에서 많이 사용하는 확률적 경사하강법과 거의 동일한 알고리즘입니다. 방법의 핵심은 주어진 목적함수(손실함수)를 설계변수(여기서는 매개변수)로 미분한 그래디언트의 반대방향으로 설계변수를 업데이트하는 것입니다.\n앞선 절에서 정의한 손실함수 \\(\\Phi(\\mathbf{P})\\)에 대한 매개변수 \\(\\mathbf{P}\\)에 대한 단계 \\(m\\)에서 그래디언트는 다음과 같습니다.\n\\[\n\\mathbf{g}_m = \\{g_{jm}\\} =\\left\\{ \\left[  \\frac{\\partial \\, \\Phi(\\mathbf{P})}{\\partial P_j} \\right]_{\\mathbf{P}=\\mathbf{P}_{m-1}} \\right\\}\n\\]\n위 식은 매개변수 공간에서 \\(\\mathbf{P}_{m-1}\\)의 위치에서 그래디언트를 나타냅니다. 그래디언트가 구체적인 벡터로 정의되려면 편도함수들에 현재 그래디언트가 정의되는 위치를 대입해야 한다는 사실을 상기합시다. 이를 나타내는 표현이 대괄호 아랫쪽에 \\(\\mathbf{P}=\\mathbf{P}_{m-1}\\)입니다. 그리고 \\(\\mathbf{P}_{m-1}\\)은 단계 \\(m-1\\)까지 \\(\\mathbf{p}_m\\)들이 누적된 벡터입니다.\n\\[\n\\mathbf{P}_{m-1} = \\sum_{i=0}^{m-1} \\mathbf{p}_i\n\\]\n각 단계에서 \\(\\mathbf{p}_m\\)은 다음처럼 그래디언트 \\(\\mathbf{g}_m\\)의 반대방향으로 \\(\\rho_m\\)만큼 이동한 벡터입니다.\n\\[\n\\mathbf{p}_m = -\\rho_m \\mathbf{g}_m\n\\]\n\\(\\rho_m\\)은 선탐색line search라는 과정으로 구할 수 있으며 다음과 같습니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}}\\, \\Phi(\\mathbf{P}_{m-1} - \\rho \\, \\mathbf{g}_m) \\tag{5}\n\\]\n이상의 내용은 조금 추상적으로 느껴질 수 있지만 기호가 좀 복잡할 뿐 보통 매개변수를 최적화하는 일반적인 내용에 지나지 않습니다. 그래디언트 부스팅이 복잡하게 느껴지는 이유는 이 같은 매개변수 최적화와 달리 함수공간에서 함수를 탐색하고자 하기 때문입니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#함수-서치",
    "href": "posts/gradientboost/gradient_boosting.html#함수-서치",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "함수 서치",
    "text": "함수 서치\n입력변수 \\(\\mathbf{x}\\)와 출력변수 \\(y\\)의 결합확률분포 상에서 손실함수 \\(L(y, F(\\mathbf{x}))\\)의 기대값은 \\(\\mathbb{E}_{y, \\mathbf{x}}\\left[ L(y, F(\\mathbf{x}))\\right]\\)이며 이를 최소화하는 ’매개변수’로써의 함수 \\(F(\\mathbf{x})\\)를 생각해볼 수 있습니다. 이를 좀 더 명시적으로 표시하기 위해 다음처럼 \\(F\\)를 변수로 하는 범함수functional로 쓰기로 합시다.\n\\[\n\\Phi(F) = \\mathbb{E}_{y, \\mathbf{x}}\\left[ L(y, F(\\mathbf{x}))\\right]\n\\]\n기댓값의 정의에 의해\n\\[\n\\begin{aligned}\n\\Phi(F) &= \\int_{\\mathbf{x}} \\int_{y} L(y, F(\\mathbf{x})) f_{XY}(\\mathbf{x}, y) \\, dy \\, d\\mathbf{x} \\\\[5pt]\n&= \\int_{\\mathbf{x}} \\int_{y} L(y, F(\\mathbf{x})) f_{Y \\mid X}(y \\mid \\mathbf{x}) \\, dy \\, f_X (\\mathbf{x}) \\, d \\mathbf{x} \\\\[5pt]\n&= \\mathbb{E}_{\\mathbf{x}} \\left[ \\,\\mathbb{E}_{y} [\\,L(y,F(\\mathbf{x}))\\mid \\mathbf{x}\\,] \\, \\right]\n\\end{aligned}\n\\]\n로 쓰고 위 식에서\n\\[\n\\phi(F(\\mathbf{x}))= \\mathbb{E}_{y} [\\,L(y,F(\\mathbf{x}))\\mid \\mathbf{x}\\,]\n\\]\n로 두면 \\(\\phi(F(\\mathbf{x}))\\)는 주어진 \\(\\mathbf{x}\\)에 대해서 함수 \\(F\\)를 바꿔치기 하면 함수 값이 달라지는 함수가 됩니다. \\(\\phi(F(\\mathbf{x}))\\)를 최소화하는 무수히 많은 후보 함수 \\(F\\)에서 최적의 함수를 찾기 위해 앞서 이야기한 최속강하법을 함수 공간에 적용하는 것을 생각해봅시다.\n식(4)에서 각 최적화 단계에서 구한 매개변수 벡터 \\(\\mathbf{p}_m\\)을 모두 더해 최적의 매개변수 벡터를 업데이트 했습니다. 이렇게 최적화 단계는 보통 유한한 길이를 가지는 벡터변수에 대해 그래디언트를 구하고 그것에 마이너스를 곱해 최적화 하고자 하는 변수를 업데이트 합니다. 하지만 지금은 \\(\\phi(F(\\mathbf{x}))\\)를 최소화하는 변수가 함수입니다. 따라서 최적화되는 변수는 요소 개수가 무한대인 벡터 변수라고 생각할 수 있습니다. 즉 함수 자체가 “매개변수”인 것입니다. 그러면 각 최적화 단계에서 찾아지는 벡터는 그 자체로 함수가 되며 그 함수들을 \\(f_m(\\mathbf{x})\\)라 하면 이를 모두 더한\n\\[\nF^* (\\mathbf{x}) = \\sum_{m=0}^M f_m (\\mathbf{x})\n\\]\n가 최적해가 되며 여기서 매 최적화 단계에서 \\(f_m(\\mathbf{x})\\)는\n\\[\nf_m(\\mathbf{x}) = - \\rho_m g_m(\\mathbf{x}) \\tag{6}\n\\]\n입니다. 식(6)은 최속강하법 매단계에서 구해지는 마이너스 그래디언트가 \\(\\mathbf{x}\\)의 함수인 \\(g_m(\\mathbf{x})\\)임을 의미합니다.\n\\(m\\)번째 단계에서 그래디언트 \\(g_m(\\mathbf{x})\\)은 \\(F(\\mathbf{x})\\)를 매개변수로 보고 다음처럼 구할 수 있습니다.\n\\[\ng_m(\\mathbf{x}) = \\left[ \\frac{\\partial \\, \\phi(F(\\mathbf{x}))}{\\partial \\, F(\\mathbf{x})} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})} = \\left[ \\frac{\\partial \\, \\mathbb{E}_{y} [\\,L(y,F(\\mathbf{x}))\\mid \\mathbf{x}\\,]   }{\\partial F(\\mathbf{x})} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})}\n\\]\n여기서 \\(F_{m-1}(\\mathbf{x})\\)은 다음처럼 개별 단계에서 찾아진 \\(f_i(\\mathbf{x})\\)를 0단계부터 \\(m-1\\)단계까지 더한\n\\[\nF_{m-1}(\\mathbf{x}) = \\sum_{i=0}^{m-1} f_i(\\mathbf{x})\n\\]\n입니다. 위 \\(g_m(\\mathbf{x})\\)을 구하는 과정을 보면 \\(F\\)로 미분하고 구체적인 그래디언트를 구하기 위해 \\(F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})\\)처럼 이전 단계에서 구해진 함수를 대입하는 것을 확인할 수 있습니다. 즉 그래디언트를 구하는 위치가 \\(F_{m-1}(\\mathbf{x})\\)인 것입니다. 그리고 적분과 미분의 순서를 바꾸면\n\\[\ng_m(\\mathbf{x}) = \\mathbb{E}_{y} \\left[ \\frac{\\partial \\, L(y,F(\\mathbf{x})) }{\\partial F(\\mathbf{x})} \\mid \\mathbf{x} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})} \\tag{7}\n\\]\n손실함수를 함수 형태인 매개변수로 편미분하고 주어진 \\(\\mathbf{x}\\)에 어떤 \\(y\\)가 매치 될지 모르니 \\(f_{Y \\mid X}(y \\mid \\mathbf{x})\\)상에서 \\(y\\)에 대해서 평균을 규해서 최종적인 함수를 구해내게 됩니다. 이 함수가 바로 매개변수 \\(F(\\mathbf{x})\\)에 대한 그래디언트가 됩니다. 그래디언트를 구했으므로 식(6)에 있는 그래디언트 \\(g_m(\\mathbf{x})\\) 방향으로 손실함수의 기댓값을 최소화하는 스탭사이즈 \\(\\rho_m\\)을 찾게 됩니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}} \\, \\mathbb{E}_{y, \\mathbf{x}}\\left[ L(\\, y, F_{m-1}(\\mathbf{x}) - \\rho \\, g_m(\\mathbf{x})\\, )\\right] \\tag{8}\n\\]\n\n유한개 데이터에 대해\n지금까지 이야기한 함수를 바로 찾는 방법은 유한개의 데이터 \\(N\\)개만 가진 현실적인 경우에 적용할 수 없습니다. \\(L(y, F)\\)을 \\(F\\)로 미분해서 \\(y\\)에 대해 적분해야지 함수로써의 그래디언트가 구해지게 되는데 이 적분을 수행할 수 가 없습니다. \\(f_{XY}\\)와 \\(f_{Y \\mid X}\\)를 모르고 그냥 \\(f_{XY}\\)에서 샘플링된 데이터 \\(N\\)개만 가지고 있으니 그래디언트를 함수 형태로 구할 수 없습니다.\n때문에 다음처럼 주어진 데이터 포인트에 대해서 그래디언트 함수의 함수값 \\(N\\)개만 구할 수 있습니다.\n데이터 \\(\\mathcal{D}=\\{\\mathbf{x}_i, y_i\\}_{i=1}^N\\)가 주어져 있을 때 식(7)로 주어지는 그래디언트의 마이너스곱한 값을 \\(\\mathbf{x}_i\\), \\(y_i\\)에 대해서 근사해보면\n\\[\n-g_m (\\mathbf{x}_i) = - \\left[ \\frac{\\partial \\, L(y_i, F(\\mathbf{x}_i)) }{\\partial F(\\mathbf{x}_i)} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})}\n\\]\n로 구할 수 있습니다. 이렇게 모두 \\(-g_m(\\mathbf{x}_i)\\)를 \\(N\\)개 구할 수 있는데 이 값들은 \\(\\mathbf{x}_i\\)에 대해서 주어진 \\(y_i\\)에 대해서만 구한 값들이라 식(7)에서 구해지는 함수 \\(g_m(\\mathbf{x})\\)에서의 함수값과 정확히 동일한 값이 아닙니다. 이런 이유로 \\(g_m (\\mathbf{x}_i)\\)를 식(7)에 대한 pseudo-response라 합니다.\n우리에게 필요한것은 모든 \\(\\mathbf{x}\\)에 대하서 값을 계산해주는 그래디언트 함수가 필요하므로 할 수 있는 최선은 구해진 pseudo-response를 피팅하여 함수 \\(g_m(\\mathbf{x})\\)를 재구성하는 것입니다.\n매개변수 \\(\\mathbf{a}\\)에 의해 정의되는 함수 \\(h(\\mathbf{x};\\mathbf{a})\\)을 준비해서 \\(-g_m(\\mathbf{x}_i)\\)들을 피팅합니다. 이 \\(h(\\mathbf{x};\\mathbf{a})\\)을 약한학습기weak learner 또는 기본학습기base leaner라고 합니다. 약한학습기는 다음과 같은 least square 손실을 사용해서 학습합니다.\n\\[\n\\mathbf{a}_m = \\underset{\\mathbf{a}}{\\text{argmin}} \\sum_{i=1}^N (-g_m(\\mathbf{x}_i) - h(\\mathbf{x}_i;\\mathbf{a}))^2 \\tag{11}\n\\]\n식(11)에서 \\(h(\\mathbf{x}_i;\\mathbf{a})\\)는 실제 구현에서 특정 머신러닝 모델이 되고 식(11) 자체는 이 약한학습기를 학습시키는 과정이 됩니다. 이렇게 학습한 매개변수 \\(\\mathbf{a}_m\\)을 사용해서 그래디언트 \\(h(x;\\mathbf{a}_m)\\)를 구성했으면 다음처럼 선탐색해서 스탭사이즈를 구합니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}} \\sum_{i=1}^N L(y_i, F_{m-1}(\\mathbf{x}_i) + \\rho h(\\mathbf{x}_i;\\mathbf{a}_m)) \\tag{12}\n\\]\n최종적으로 다음처럼 함수를 업데이트 할 수 있습니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m h(\\mathbf{x};\\mathbf{a}_m)\n\\]\n이상의 과정에서 중요한 점은 이 방법은 \\(h(\\mathbf{x}_i;\\mathbf{a})\\)이 어떤 모델이든 상관이 없다는 것입니다. 즉, 이 방법은 매우 일반적인 방법론입니다. 사이킷런 구현은 결정트리로 되어 있지만 기타 다른 모델에 대해서도 잘 작동하는 것을 이후 코딩 실험으로 확인해보도로고 하겠습니다.2\n결과적으로 그래디언트 \\(h(\\mathbf{x};\\mathbf{a}_m)\\)에 의해서 모델이 업데이트 되는 식이 되었고 그래서 이 알고리즘의 이름이 그래디언트 부스팅이 됩니다.\n\n\n분류문제에서도 이런 성질은 그대로 유지되지만 TreeBoost라는 이름으로 수정된 방식을 위해서는 꼭 결정트리를 사용해야 하는 것도 ’TreeBoost’절에서 확인해보겠습니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#그래디언트-부스팅-알고리즘",
    "href": "posts/gradientboost/gradient_boosting.html#그래디언트-부스팅-알고리즘",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "그래디언트 부스팅 알고리즘",
    "text": "그래디언트 부스팅 알고리즘\n이제 위에서 이야기한 내용을 구체적인 단계로 적용시켜 보겠습니다. 먼저 \\(F_0(\\mathbf{x})\\)를 정의 합니다.\n\n\n\n최초 학습기 \\(F_0(x)\\)를 다음을 만족하는 상수 \\(\\rho_0\\)로 설정합니다.\n\n\n\\[\nF_0(x) = \\underset{\\rho}{\\text{argmin}} \\sum_{i=1}^n L(y_i, \\rho) \\tag{step 1}\n\\]\n구체적으로 \\(\\rho_0\\)를 결정하는 과정은 이후 다시 알아보겠습니다. (step 1)에서 정한 학습기는 어떤 입력이 들어와도 무조건 상수 \\(\\rho_0\\)를 출력하는 더미 학습기입니다. 그래서 앞 예제에서 h_0로 DummyRegressor를 사용했습니다.\n - 2) 피팅할 pseudo-response를 다음처럼 계산합니다.\n\\[\n\\tilde{y}_i = - \\left[ \\frac{\\partial \\, L(y_i, F(\\mathbf{x}_i)) }{\\partial F(\\mathbf{x}_i)} \\right]_{F(\\mathbf{x}) = F_{m-1}({\\mathbf{x}})} \\tag{step 2}\n\\]\n - 3) 앞서 기술한대로 어떤 손실함수를 쓰던지 상관없이 약한 학습기 \\(h(\\mathbf{x};\\mathbf{a}_m)\\)를 \\(\\{\\mathbf{x}_i, \\tilde{y}_i\\}_{i=1}^N\\)에 대해서 학습시킵니다. 이 단계에서 \\(h(\\mathbf{x};\\mathbf{a}_m)\\)는 어떤 모델이라도 상관없지만 주로 결정트리를 사용합니다. 아래 코딩 예제에서 결정트리 이와의 모델로도 직접 피팅해보도록 하겠습니다.\n\\[\n\\mathbf{a}_m = \\underset{\\mathbf{a}}{\\text{argmin}} \\sum_{i=1}^N (-g_m(\\mathbf{x}_i) - h(\\mathbf{x}_i;\\mathbf{a}))^2 \\tag{step 3}\n\\]\n - 4) step 3에을 구해진 그래디언트 방향으로 스탭사이즈 \\(\\rho_m\\)을 구합니다. 보통 선탐색하지 않고 고정된 학습률을 사용하기 때문에 이 단계는 적절한 고정 숫자 \\(\\rho_m\\)으로 대체될 수 있습니다. 이후 코딩 실습에서 실제 선탐색도 해보도록 하겠습니다. 스탭사이즈가 결정되었다면 step 2로 돌아가서 반복합니다.\n\\[\n\\rho_m = \\underset{\\rho}{\\text{argmin}} \\sum_{i=1}^N L(y_i, F_{m-1}(\\mathbf{x}_i) + \\rho h(\\mathbf{x}_i;\\mathbf{a}_m)) \\tag{step 4}\n\\]\n - 5) step 3, 4에서 구한 그래디언트와 스탭사이즈로 다음처럼 업데이트 합니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m h(\\mathbf{x};\\mathbf{a}_m) \\tag{step 5}\n\\]\n\n이제 $ L(y, F())$를 다음처럼 구체적으로 squared loss로 두면\n\\[\nL(y, F(\\mathbf{x}))=\\frac{1}{2}  (y- F(\\mathbf{x}))^2\n\\]\n\\(F_{m-1}(\\mathbf{x})\\)에서 \\(F(\\mathbf{x})\\)에 대한 미분은 다음과 같이 됩니다.\n\\[\n\\left[ \\frac{\\partial }{\\partial F(\\mathbf{x})} \\left( \\frac{1}{2}  (y_i- F(\\mathbf{x}_i))^2 \\right) \\right]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} =  F_{m-1}(\\mathbf{x}_i) - y_i\n\\]\n마이너스 그래디언트를 만들기 위해 마이너스를 곱하면\n\\[\n\\tilde{y}_i =  y_i - F_{m-1}(\\mathbf{x}_i)\n\\]\n가 되는데 결국 그래디언트의 함수값이 이전 함수의 결과와 정답값의 차이가 되기 때문에 이를 pseudo-residual이라고도 하게 됩니다.\n이렇게 손실함수를 오차제곱합으로 정의했기 때문에 회귀문제에만 가능할 것 같지만 \\(y_i\\)를 0 또는 1인 타겟으로 두고 분류문제에 적용해도 큰 문제없이 적용 가능함을 잠시 후 코드로 확인해보겠습니다.\n하지만 분류문제에 오차제곱합 손실을 쓰는 것이 그렇게 바람직한 상황은 아닌 것은 분명합니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#직접-구현-1",
    "href": "posts/gradientboost/gradient_boosting.html#직접-구현-1",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "직접 구현 1",
    "text": "직접 구현 1\n지금끼지 이야기한 내용을 코드로 구현해서 작동을 확인해보겠습니다. 아래 코드는 최적화 과정중 선탐색을 하기 위해 황금분할 탐색법을 구현한 보조 코드입니다. 사이킷 런의 그래디언트 부스팅은 선탐색을 하지 않기 때문에 필수적인 코드는 아닙니다.\n\ndef gss(f_alpha, delta=1.0e-2, tol=1e-15):\n    '''\n    Line search function by golden section search\n    https://en.wikipedia.org/wiki/Golden-section_search and [arora]\n    \n    f_alpha: 1D objective function\n    delta  : Init. guess interval determining initial interval of uncertainty\n    tol    : stop criterion\n    '''\n    gr = (np.sqrt(5) + 1) / 2\n    \n    ########################################################################################\n    # ESTABLISH INITIAL DELTA\n    # 초기 delta를 잡는다.\n    # alpah = 0에서 값과 delta에서의 함수값을 계산하고 delta에서의 값이 크다면 delta를 줄인다.\n    ########################################################################################\n    AL = 0.\n    FL = f_alpha(AL)\n    AA = delta\n    FA = f_alpha(AA)\n    while  FL &lt; FA :\n        delta = 0.1*delta\n        AA = delta\n        FA = f_alpha(AA)\n    ########################################################################################\n    \n    ########################################################################################\n    # ESTABLISH INITIAL INTERVAL OF UNCERTAINTY\n    # delta를 사용하여 초기 불확정 구간을 설정한다.\n    # 결정된 구간을 [AL, AU] 로 두고 황금분할 탐색을 시작한다.\n    ########################################################################################\n    j = 1\n    AU = AA + delta * (gr**j)\n    FU = f_alpha(AU)\n    while FA &gt; FU :\n        AL = AA\n        AA = AU\n        FL = FA\n        FA = FU\n        \n        j += 1\n        AU = AA + delta * (gr**j)\n        FU = f_alpha(AU)\n\n    AB = AL + (AU - AL) / gr\n\n    FB = f_alpha(AB)\n    \n    while abs(AA - AB) &gt; tol:\n        if f_alpha(AA) &lt; f_alpha(AB):\n            AU = AB\n        else:\n            AL = AA\n\n        # we recompute both c and d here to avoid loss of precision \n        # which may lead to incorrect results or infinite loop\n        AA = AU - (AU - AL) / gr\n        AB = AL + (AU - AL) / gr\n\n    return (AU + AL) / 2\n\n구현은 두가지 함수로 이뤄져있습니다. train_gradient_boost()는 데이터와 약한학습기 h와 약한학습기 수 M과 학습률 lr을 넘겨 받아 이전 절에서 설명한 내용을 그대로 따라갑니다. 자세한 설명은 코드에 달았습니다.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.base import clone\n\n# 그래디언트부스팅 직접 만들기[+]\ndef train_gradient_boost(X, y, h, M=10, lr=0.1, loss=None):\n    \"\"\"\n    X: train samples (N,D), N: # of samples, D: the dimension of a sample\n    y: target\n    h: weak learner instance\n    M: # of week learner (except for the first dummy learner)\n    lr: float: learning_rate, 'linesearch': linesearch\n    loss: loss function whose args are predictions and ground truth\n    \"\"\"\n    loss_values = []\n\n    # 위 알고리즘에서처럼 상수로 첫번째 예측기를 정한다,  m=0\n    # step 1, F0 = argmin_{rho} sum_i L(y_i, rho)\n    # 타겟의 평균으로 \n    H = [ y.mean() ] \n    Fm_1 = np.ones(X.shape[0])*H[0]\n    steps = [None, ]\n\n    do_linesearch = True if type(lr) is str else False\n\n    # 학습\n    for m in range(1, M+1):# 최초 더미 예측기 제외 M개 트리 \n        # 각 단계의 학습기에서 만들어낸 로스 저장\n        # 여기서는 로스값을 저장할 목적으로 loss를 사용할 뿐\n        # 실제 계산을 위해 사용하는 것은 아님\n        # 이 코드에서 실제 loss의 미분은 squared loss로 하드코딩\n        if loss is not None:\n            loss_values.append( loss(y, Fm_1) )\n\n        # step 2 그래디언트의 값 구함 pseudo-residual\n        r_im = y - Fm_1 \n\n        # step 3 구해진 잔차 r_im에 대해서 학습기 hm을 학습\n        h_ = clone(h)\n        H.append( h_.fit(X, r_im) ) \n\n        # step 4 & 5\n        if do_linesearch:\n            def f_rho(rho):\n                return loss(y, Fm_1 + rho * H[-1].predict(X))\n\n            rho = gss(f_rho)\n            steps.append(rho)\n            \n            Fm_1 += steps[-1]*H[-1].predict(X)\n        else:\n            Fm_1 += lr*H[-1].predict(X)\n\n    if do_linesearch:\n        return {'learners':H, 'learning_rate':steps, 'loss_values':loss_values}\n    else:\n        return {'learners':H, 'learning_rate':lr, 'loss_values':loss_values}\n    \n\n# 예측하기[+]\ndef predict(X, gradient_boost):\n    \"\"\"\n    X: input, (N,D)\n    gradient_boost: model dict. that has been trained \n    \"\"\"\n    H = gradient_boost['learners']\n    lr = gradient_boost['learning_rate']\n\n    # 0번째 약한학습기는 모든 입력에 대해서 상수 H[0] 출력\n    F = np.ones(X.shape[0])*H[0]\n\n    # 1번째 약한 학습기부터 M번째 약한 학습기까지 결과 더하기\n    for m in range(1, len(H)):\n        # lr이 리스트로 구성되었으면 각 단계에서 선탐색을 한것이므로\n        # 각 단계마다 결정된 스탭사이즈를 사용\n        if hasattr(lr, '__iter__'):\n            F += lr[m]*H[m].predict(X)\n        # 그렇지 않으면 고정 러닝레이트를 사용    \n        else:\n            F += lr*H[m].predict(X)\n\n    return F"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#회귀",
    "href": "posts/gradientboost/gradient_boosting.html#회귀",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "회귀",
    "text": "회귀\n이제 직접 구현한 코드를 회귀 문제에 적용해보도록 하겠습니다.\n\nKNeighborsRegressor 이용\n당연한 이야기지만 단계별 학습기로 꼭 결정 트리를 사용할 필요없습니다. 여기서는 KNeighborsRegressor으로 먼저 시도해보겠습니다. 먼저 학습과정에서 줄어드는 손실을 계산하기 위해 간단한 squared loss 함수를 정의합니다.\n\nsquared_loss = lambda y, pred:  np.sum((y - pred)**2)\n\n\n# 개별 estimator는 꼭 결정트리가 아니어도 상관없음 \nfrom sklearn.neighbors import KNeighborsRegressor\n\nx = np.linspace(-0.5, 0.5, 1000).reshape(-1,1)\n\nh = KNeighborsRegressor(n_neighbors=20)\n\n# 선탐색하는 경우와 하지 않는 경우를 비교해 봅니다.\n# learning_rate = 'linesearch'\nlearning_rate = 0.1\ngb_reg_knn_mse = train_gradient_boost(X_reg, y_reg, h, M=50, \n                                  lr=learning_rate, loss=squared_loss)\n# 예측결과의 저장은 pred_{datsset}_{weak learner}_{loss}로 함\npred_X_reg_knn_mse = predict(X_reg, gb_reg_knn_mse)\npred_x_reg_knn_mse = predict(x, gb_reg_knn_mse)\n\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True)\n\nax[0].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[0].scatter(X_reg, pred_X_reg_knn_mse, marker='o', color='C2', edgecolor='k', label=\"pred\")\nax[0].legend()\n\nax[1].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[1].plot(x, pred_x_reg_knn_mse, color='C2', lw=3, label=\"pred\")\nax[1].legend()\n\nplt.show()\n\n# fig.savefig(\"reg_knn_mse_lrate.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n결과를 보면 그럴듯 하게 회귀된 것을 확인할 수 있습니다. 여기서 각 학습단계에 대한 손실도 그려보면\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_reg_knn_mse['loss_values'], '.-', color='C1')\nax.set_xlabel('iterations')\nax.set_ylabel('loss')\nplt.show()\n\n# fig.savefig(\"reg_knn_mse_lrate_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n단계별 손실함수 값도 안정적으로 줄어들고 있는 것을 확인할 수 있습니다.\n보통 개별 학습기로 결정 트리를 사용하고 사이킷-런도 내부 예측기가 결정트리로 구현되어 있으므로 결정트리로 다시 시도해보겠습니다.\n\n\nDecisionTreeRegressor 이용\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\n# learning_rate = 'linesearch'\nlearning_rate = 0.1\ngb_reg_tree_mse = train_gradient_boost(X_reg, y_reg, h, M=50, \n                                   lr=learning_rate, loss=squared_loss)\npred_X_reg_tree_mse = predict(X_reg, gb_reg_tree_mse)\npred_x_reg_tree_mse = predict(x, gb_reg_tree_mse)\n\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, sharey=True)\n\nax[0].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[0].scatter(X_reg, pred_X_reg_tree_mse, marker='o', color='C2', edgecolor='k', label=\"pred\")\nax[0].legend()\n\nax[1].scatter(X_reg, y_reg , marker='o', color='C1', edgecolor='k', label=\"train data\")\nax[1].plot(x, pred_x_reg_tree_mse, color='C2', lw=3, label=\"pred\")\nax[1].legend()\n\nplt.show()\n\n# fig.savefig(\"reg_tree_mse_lrate.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_reg_tree_mse['loss_values'], '.-', color='C1')\nax.set_xlabel('iterations')\nax.set_ylabel('loss')\nplt.show()\n\n# fig.savefig(\"reg_tree_mse_lrate_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n회귀 결과와 손실함수값을 확인해보면 문제없이 잘 작동하는 것 같습니다.\n이렇게 간단한 실험을 통해 약한 학습기는 knn, 트리 모델 모두 상관없이 잘 작동한다는 약한 학습기의 일반성을 확인해봤습니다.\n\n\nsklearn 사용\n지금까지 직접 코딩한 버전을 바닐라 버전이라 칭하고 사이킷런을 사용해서 동일한 문제를 풀어보겠습니다.\n사이킷런에서 제공하는 GradientBoostingRegressor를 사용합니다. 여기서 각 개별 약한 학습기를 따로 지정할 수 없고 무조건 결정트리가 사용되는데 트리 분기 기준으로 criterion='squared_error를 사용하여 트리의 작동방식을 검증하겠습니다.\n\n# 모델 로드\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# 동일 조건으로 모델 생성과 fit\nlearning_rate = 0.1\ngb_reg_sk = GradientBoostingRegressor(criterion='squared_error', \n                                 max_depth=2, n_estimators=50, \n                                 learning_rate=learning_rate)\ngb_reg_sk.fit(X_reg,y_reg)\n\nGradientBoostingRegressor(criterion='squared_error', max_depth=2,\n                          n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(criterion='squared_error', max_depth=2,\n                          n_estimators=50)\n\n\n\n# 예측\npred_x_reg_sk = gb_reg_sk.predict(x)\npred_x_reg_sk.shape\n\n(1000,)\n\n\n\n# 그림 확인\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.plot(X_reg, y_reg , '.', color='C1')\nax.plot(x, pred_x_reg_sk, color='C3', lw=3, label='sklearn')\nax.plot(x, pred_x_reg_tree_mse, '--', color='C2', label='vanilla gradient boost')\n\nax.legend()\nplt.show()\n\n# fig.savefig(\"reg_tree_mse_sklearn.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n사이킷런 예측과 직접 만든 예측을 비교해보면 동일한 것을 알 수 있습니다. 정확히 확인하게 위해 예측을 서로 비교해보면\n\nnp.sum((pred_x_reg_sk - pred_x_reg_tree_mse)**2)\n\n0.0\n\n\n0이 되어 두 예측 결과는 완전히 동일함을 알 수 있습니다.\n\n사이킷런 결과 분석\n이제 사이킷런이 만들어낸 첫번째 학습기와 두번째 학습기를 직접 확인하면서 내부적으로 어떻게 구현되었는지 확인해보도록 하겠습니다. 이미 확인한 결과를 통해 예상해보면 직접 만든 구현과 크게 다르지 않을 것 같습니다.\n사이킷런 그래디언트 부스팅 객체에는 gb_reg_sk.init_에 무조건 평균을 예측하는 DummyRegressor() 학습기 있고 gb_reg_sk.estimators_에 (M,1)인 어레이로 학습기가 순서대로 들어 있습니다. 그러니까 더미 포함 총 M+1개가 있는 것입니다.\n\n# 첫번째 트리의 예측\n\ny_reg_mean = y_reg.mean()\nprint(f\"초기 더미 예측:{np.unique(gb_reg_sk.init_.predict(X_reg))[0]:.6f}, \\\n학습세트 타겟평균:{y_reg_mean:.6f}\")\n\n# 더미 예측에 학습률만큼 곱한 첫번째 예측기의 예측을 더함\nh_1 = gb_reg_sk.estimators_[0,0]\npred_1 = gb_reg_sk.init_.predict(x) + learning_rate*h_1.predict(x)\n\n# 그림으로 확인\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.plot(X_reg, y_reg , '.', color='C1')\nax.plot(x, pred_1, color='C3', lw=3, label=r'$h_0 + \\eta h_1$')\n\nfor thr in h_1.tree_.threshold:\n    if X_reg.min() &lt; thr &lt; X_reg.max():\n        ax.vlines(thr, ymin=y_reg.min(), ymax=y_reg.max(), \n                  ls='--', color='C4', lw=2, alpha=0.5)\n\nax.legend()\nplt.show()\n\n# fig.savefig(\"gb_reg_sk_h0h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n초기 더미 예측:0.265458, 학습세트 타겟평균:0.265458\n\n\n\n\n\n위 그림은 사이킷런이 학습한 모델 gb_reg_sk의 더미 학습기와 첫번째 학습기가 만들어낸 예측 \\(h_0(x) + \\eta h_1(x)\\)을 그린 것입니다. 여기서 스탭사이즈는 고정 학습률을 사용했기 때문에 \\(\\eta\\)라 표현했습니다. 첫번째 트리 \\(h_1(x)\\)는 세번 분기한것을 알 수 있습니다. 이 트리를 직접 그려보면 다음과 같습니다.\n\n# h_1을 시각화\nfig = plt.figure(figsize=(10,5), dpi=100)\nax = plt.axes()\n\n# 트리 그림 그리기\ndot_data = export_graphviz(h_1, out_file=None, max_depth=2)\ngraph = graphviz.Source(dot_data, format=\"png\")\ntree_img = imread(graph.render())\nax.imshow(tree_img)\nax.axis('off')\n\nplt.show()\n\n# fig.savefig(\"gb_reg_sk_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n그림으로 표현된 첫번때 트리 h_1을 보고 각 노드에 출력값 squared_error와 value를 계산해보겠습니다.\n\n# 첫번째 트리의 타겟은 더미 예측기의 출력인 y_reg_mean과의 차이가 됨\nresidual_1  = y_reg - y_reg_mean\nprint(f\"root value={residual_1.mean():.3f}\")\n\n# 첫번째 분류에서 벨류값이 그냥 평균이냐?\ndepth1_first_node_idx = X_reg[:,0]&lt;=-0.36\ndepth1_first_node_target = residual_1[depth1_first_node_idx]\n\n# squared_error은 평균과 각 타겟 사이의 mse\nprint(\"첫번째 트리의 depth1 first node 결과\")\nprint(f\"squared_error={np.mean((depth1_first_node_target - depth1_first_node_target.mean())**2):.3f}\") \nprint(f\"value={depth1_first_node_target.mean():.3f}\") # 출력값은 그냥 평균\n\nroot value=0.000\n첫번째 트리의 depth1 first node 결과\nsquared_error=0.012\nvalue=0.309\n\n\n루트노드에 모인 샘플의 타겟은 더미 학습기 예측과 최초 타겟값의 차이, 잔차(residual)이며 이를 평균한 값이 루트노드의 value로 출력되었음을 알 수 있습니다. 깊이1에서 첫번째 노드에 나타난 value=0.309는 루트노드에 있는 샘플이 X[0]&lt;=-0.36이라는 조건을 통해 이 노드에 모인 샘플들의 타겟값 평균임을 확인할 수 있습니다. 깊이1에서 첫번째 노드에 나타난 squared_error=0.012는 앞서 구한 value값과 노드에 모인 샘플들의 타겟값 사이에서 계산되는 오차제곱합임을 알 수 있습니다.\n이제 h_1트리의 실제 출력을 담당하는 리프노드에서 값도 확인해보겠습니다.\n\ndepth2_first_node_idx = X_reg[:,0] &lt;= -0.43\n\ndepth2_first_node_target = residual_1[depth1_first_node_idx & depth2_first_node_idx]\n\nprint(\"첫번째 트리의 depth2 first node 결과\")\nprint(f\"squared_error: {np.mean((depth2_first_node_target - depth2_first_node_target.mean())**2):.3f}\") \nprint(f\"value: {depth2_first_node_target.mean():.3f}\") # 출력값은 그냥 평균\n\n첫번째 트리의 depth2 first node 결과\nsquared_error: 0.004\nvalue: 0.395\n\n\n깊이1의 첫번째 노드처럼 그냥 노드 샘플의 평균을 value로 출력하는 일반적인 회귀 트리임을 확인할 수 있습니다. 이렇게 그래디언트 부스팅이 회귀 문제에 적용되면 예측과 타겟의 잔차를 다시 타겟으로 하는 회귀 트리의 연속적인 모임이라는 것을 알 수 있습니다.\n앞서 복잡한 식을 통해 유도했던 과정이 그대로 적용됨을 실제 사이킷런 학습 결과를 분석하면서 확인할 수 있었습니다."
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#분류",
    "href": "posts/gradientboost/gradient_boosting.html#분류",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "분류",
    "text": "분류\n이제 위에서 만든 함수를 수정없이 그대로 이진분류 문제에 적용해보겠습니다. 이진분류 문제에 적용하게 되면 타겟은 0 또는 1이 되며 잔차는 0 또는 1과 예측값의 차이가 될 것입니다. 원칙적으로 이 문제에 대한 학습기는 0보다 작은값 또는 1보다 큰값을 출력하지 않아야 합니다. 현재 작성된 함수에 그런 제약조건을 고려하지 않아서 1보다 큰값, 0보다 작은 값이 출력될 수 있지만 그것과 별개로 손실을 줄이도록 만들어졌으므로 전반적으로는 잘 작동할 것으로 예상됩니다.\n두가지 예제를 통해 확인해보겠습니다.\n\n첫번째 예제\n첫번째 예제는 사이킷런의 make_gaussian_quantiles 함수를 통해 생성합니다.\n\n# https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\n\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(\n    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1\n)\nX2, y2 = make_gaussian_quantiles(\n    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1\n)\nX_clf1 = np.concatenate((X1, X2))\ny_clf1 = np.concatenate((y1, -y2 + 1))\n\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X_clf1[y_clf1==0][:, 0], X_clf1[y_clf1==0][:, 1], 'o', color='C1')\nax.plot(X_clf1[y_clf1==1][:, 0], X_clf1[y_clf1==1][:, 1], '^', color='C2')\nax.axis('tight')\nplt.show()\n\n\n\n\n앞서 만들어둔 함수 train_gradient_boost는 오차제곱합 손실함수에 대한 그래디언트를 계산하므로 분류 문제에서도 타겟 0, 1이 회귀해야할 실제 값입니다. 따라서 개별 학습기도 여전히 Regressor로 지정해야 합니다. 이런 이유로 그래디언트 부스팅 알고리즘의 다른 이름이 그래디언트 부스팅 회귀트리gradient boosting regression tree가 되는 것입니다.\n\nDecisionTreeRegressor 이용\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 0.2\ngb_clf1_tree_mse = train_gradient_boost(X_clf1, y_clf1, h, M=50, \n                                        lr=learning_rate, loss=squared_loss)\n\n\n실행 결과를 그림으로 확인해보자.\n\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_x_clf1_tree_mse = predict(X_grid, gb_clf1_tree_mse)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf1_tree_mse &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\n\nax.axis('tight')\n\nplt.show()\n\npred_x_clf1_tree_mse = Z.copy()\n\n# fig.savefig(\"gb_clf1_tree_mse.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n생각보다 나쁘지 않게 잘되는 것을 확인할 수 있습니다. 학습시키는 함수로부터 각 단계에서 발생한 로스값을 돌려받았으므로 그림으로 그려보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf1_tree_mse['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n# fig.savefig(\"gb_clf1_tree_mse_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n손실도 꾸준히 잘 감소하는 것을 확인할 수 있습니다. 재미삼아 kNN을 개별 예측기로 사용해서 실험 해보겠습니다.\n\n\nKNeighborsRegressor 이용\n\nh = KNeighborsRegressor(n_neighbors=10)\n\nlearning_rate = 0.1\ngb_clf1_knn_mse = train_gradient_boost(X_clf1, y_clf1, h, M=25, \n                                       lr=learning_rate, loss=squared_loss)\n\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_x_clf1_knn_mse = predict(X_grid, gb_clf1_knn_mse)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf1_knn_mse &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\n\nax.axis('tight')\n\nplt.show()\n\n# fig.savefig(\"gb_clf1_knn_mse.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\nkNN 결과도 나쁘지 않습니다. 손실도 그려보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf1_knn_mse['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n# fig.savefig(\"gb_clf1_knn_mse_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\nk값, 학습률, 개별 학습기 수 M을 적당히 잘 지정하면 나쁘지 않은 결과를 얻을 수 있습니다. 분류된 결과는 결정 트리를 기반으로 하는 경우와는 사뭇 다른 모습인것도 재미있는 부분입니다.\n이제 사이킷런으로 같은 데이터에 대해서 실행해보겠습니다.\n\n\nsklearn 사용\n사이킷런의 GradientBoostingClassifier를 사용합니다. 단 여기서는 어떤 이유로 인해 학습률을 1로 지정하도록 하겠습니다. 1로 두는 이유는 이 글 마지막에 이야기하도록 하겠습니다.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nlearning_rate = 1.0\n# 옵션 criterion 은 약한 학습기\ngb_clf1_sk = GradientBoostingClassifier(criterion='squared_error', \n                                        learning_rate=learning_rate, \n                                        n_estimators=50, max_depth=2)\n\n\ngb_clf1_sk.fit(X_clf1, y_clf1)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)\n\n\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nZ = gb_clf1_sk.predict(X_grid)\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\n\nax.axis('tight')\n\nplt.show()\n\npred_x_clf1_sk = Z.copy()\n\n# fig.savefig(\"gb_clf1_sk.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n결정트리를 개별 학습로 사용한 결과와 크게 다르지 않은 결과를 확인할 수 있습니다. 하지만 회귀의 경우처럼 완벽하게 동일하지는 않기 때문에 사이킷런 학습 과정은 앞서 설명한 그래디언트 부스팅의 과정과 약간 다를 것이란 것을 짐작할 수 있습니다.\n조금 더 간단한 두 번째 예제를 하나 더 실행하고 두 번째 예제를 가지고 분석을 해보도록 하겠습니다.\n\n\n\n두번째 예제\n\nfrom sklearn.datasets import make_moons\n\nX_clf2, y_clf2 = make_moons(n_samples=100, noise=0.25, random_state=3)\n\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X_clf2[y_clf2==0][:, 0], X_clf2[y_clf2==0][:, 1], 'o', color='C1')\nax.plot(X_clf2[y_clf2==1][:, 0], X_clf2[y_clf2==1][:, 1], '^', color='C2')\nax.axis('tight')\nplt.show()\n\n\n\n\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 0.2\ngb_clf2_tree_mse = train_gradient_boost(X_clf2, y_clf2, h, M=50, \n                                        lr=learning_rate, loss=squared_loss)\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_x_clf2_tree_mse = predict(X_grid, gb_clf2_tree_mse)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf2_tree_mse &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.axis('tight')\n\nplt.show()\n\npred_x_clf2_tree_mse = Z.copy()\n\n\n# fig.savefig(\"gb_clf2_tree_mse.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf2_tree_mse['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n\n# fig.savefig(\"gb_clf2_tree_mse_loss.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n이번 예제도 예측 결과나 손실이 줄어드는 과정이 썩 나쁘지 않습니다. 사이킷런으로 실행해서 결과를 비교해봅시다.\n\nsklearn 사용\n이전과 동일하게 실행하며 이번에도 학습률을 1로 두겠습니다.\n\nlearning_rate = 1.0\ngb_clf2_sk = GradientBoostingClassifier(criterion='squared_error', \n                                 learning_rate=learning_rate, \n                                  n_estimators=50, max_depth=2)\n\n\ngb_clf2_sk.fit(X_clf2, y_clf2)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=50)\n\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nZ = gb_clf2_sk.predict(X_grid)\nZ = Z.reshape(X1.shape)\n\nfig = plt.figure(figsize=(5,5), dpi=100)\nax = plt.axes()\n\nax.plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax.plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.axis('tight')\n\nplt.show()\n\npred_x_clf2_sk = Z.copy()\n\n# fig.savefig(\"gb_clf2_sk.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n역시 비슷한 결과를 얻을 수 있지만 완전히 동일하지 않은 것을 확인할 수 있습니다. 무엇이 다른지 확인하기 위해 회귀때 처럼 첫번째 트리를 분석해보기로 하겠습니다.\n\n\n사이킷런 결과 분석\n\n# 첫번째 트리의 예측\ny_mean = y.mean()\nprint(f\"초기 더미 예측:{np.unique(gb_clf2_sk.init_.predict(X))[0]:.6f}, 학습세트 타겟평균:{y_mean:.6f}\")\n\n초기 더미 예측:0.000000, 학습세트 타겟평균:0.500000\n\n\n초기 타겟의 평균은 0.5인데 더미 예측기가 타겟의 평균을 출력하지 않는것을 확인할 수 있습니다. 초기 더미 예측기가 0을 출력한 것은 타겟 평균 0.5를 확률값으로 보고 이에 대한 로짓값을 출력하기 때문입니다.\n\n# 위에 더미 예측기의 출력값이 0 이 나오는 이유는? \n# 예측기는 확률값을 출력하지 않고 로짓을 출력한다.\np = y_mean\nnp.log(p / (1-p))\n\n0.0\n\n\n이 사실로부터 분류문제에서 각 학습기는 로짓을 출력한다고 가정한 것을 알 수 있습니다. 마치 로지스틱 회귀의 선형함수 부분과 같은 역할을 한다고 생각할 수 있습니다. 로지스틱회귀에서도 선형식을 이용해서 [0,1]로 바운드되지 않는 로짓값을 출력하고 이를 로지스틱 시그모이드 함수에 입력하여 최종 출력을 분류문제에 적합한 확률로 바꾸는데 그래디언트 부스팅도 유사하게 작동하는 것입니다.\n개별 학습기가 확률을 바로 출력한다고 하면 [0,1]로 바운드된 값을 출력해야 하는데 이렇게 하기 위해서는 제약조건을 걸어야 합니다. 그것보다 출력을 로짓으로 가정하고 \\((-\\infty, \\infty)\\)로 언바운드된 값을 출력하게 하는 편이 더 간편합니다.\n이제 \\(F_m(\\mathbf{x})\\)가 로짓을 출력하는 함수이므로 로짓을 입력으로 받는 목적함수를 정의 해야 합니다.\n\n\n\nDeviance loss\n이진분류 문제에서 로그 가능도는 다음과 같습니다.\n\\[\n\\ell = y \\log(p) + (1-y) \\log(1 - p)\n\\]\n위 식에서 이 식에 입력되는 값이 확률임을 분명히 하기 위해 \\(p\\)로 표기 했습니다. 적당히 전개를 합니다.\n\\[\n\\begin{aligned}\n\\ell &= y \\log(p) + (1-y) \\log(1 - p) \\\\[5pt]\n&= y \\log(p) + \\log(1 - p) - y \\log(1 - p) \\\\[5pt]\n&= \\log(1 - p) + y ( \\log(p) - \\log(1 - p)) \\\\[5pt]\n&= \\log(1 - p) + y \\log \\left( \\frac{p}{1-p} \\right)\n\\end{aligned} \\tag{[1]}\n\\]\n한편 \\(p\\)와 로짓 \\(z\\)의 관계는 다음과 같으므로 (확률은 더해서 1이 된다는 것을 이용)\n\\[\np = \\frac{e^{z}}{1+e^{z}} \\implies 1-p = \\frac{1}{1+e^{z}}\n\\]\n두번째 결과에 로그를 취하면\n\\[\n\\log(1 - p) = \\log \\left( \\frac{1}{1 + e^{z}} \\right) =  -\\log (1+ e^{z}) \\tag{[2]}\n\\]\n이 결과를 [1]에 대입하면\n\\[\n\\ell =  -\\log (1+ e^{z}) + y \\log \\left( \\frac{p}{1 - p} \\right)  \n\\]\n여기서 \\(\\log \\left( \\frac{p}{1 - p} \\right) = z\\)이므로\n\\[\n\\ell =  y  z- \\log (1+ e^{z})\n\\]\n손실로 만들기위애 가능도에 마이너스를 곱하면\n\\[\nL(y, z) = -y  z + \\log (1+ e^{z})\n\\]\n그리고 가정에 의해\n\\[\nz = F(\\mathbf{x})\n\\]\n이므로 최종적으로 손실함수는 다음처럼 주어집니다.\n\\[\nL(y, F(\\mathbf{x})) = -y F(\\mathbf{x}) + \\log \\left( 1+ e^{F(\\mathbf{x})} \\right) \\tag{[3]}\n\\]\n이제 [3]를 \\(F(\\mathbf{x})\\)에 대해 미분해보면\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\, F(\\mathbf{x})} L(y, F(\\mathbf{x}))  &=  -y F(\\mathbf{x}) + \\log \\left( 1+ e^{F(\\mathbf{x})} \\right) \\\\\n&= -y + \\frac{e^{F(\\mathbf{x})}}{1+ e^{F(\\mathbf{x})}}\n\\end{aligned}\n\\]\n그래디언트를 구하기 위해 그래디언트가 정의되는 포인트를 \\(F_{m-1}(\\mathbf{x})\\)로 설정하면\n\\[\n\\left[ \\frac{\\partial}{\\partial \\, F(\\mathbf{x})} L(y, F(\\mathbf{x})) \\right]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} = -y + \\frac{e^{F_{m-1}(\\mathbf{x})}}{1+ e^{F_{m-1}(\\mathbf{x})}} = -y + p_{m-1}(\\mathbf{x})\n\\]\n가 됩니다.\n강하방향으로 만들기 위해 마이너스를 곱하면\n\\[\n-\\left[ \\frac{\\partial}{\\partial \\, F(\\mathbf{x})} L(y, F(\\mathbf{x})) \\right]_{F(\\mathbf{x})= F_{m-1}(\\mathbf{x}) } = y - p_{m-1}(\\mathbf{x})\n\\]\n따라서 pseudo-response는 다음과 같습니다.\n\\[\n-g_m(\\mathbf{x}_i) = y_i - p_{m-1}(\\mathbf{x}_i) \\tag{[4]}\n\\]\n이 식에서 \\(p_{m-1}\\)은 \\(F_{m-1}(\\mathbf{x})\\)가 만들어낸 출력 로짓을 확률로 변환한 값을 의미합니다. 그러고 보면 이번에도 역시 그래디언트의 유사값analogue이 확률의 잔차가 됨을 알 수 있습니다. 그래서 역시 이번에도 pseudo-residual이 됩니다.\n\\[\n\\left[ \\frac{\\partial}{\\partial \\, F(\\mathbf{x})} \\left( -y_i F(\\mathbf{x}_i) + \\log \\left(1+e^{F(\\mathbf{x}_i)} \\right) \\right) \\right]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} =  p_{m-1}(\\mathbf{x}_i) - y_i\n\\]\n\n\n직접구현 2\n위에서 유도한 새로운 손실함수에 대해서 코드를 작성했습니다. 뭔가 복잡하게 유도된 듯 하지만 결론은 식[4]이고 놀랍게도 식[4]는 기존 방식에서 잔차를 구하는 것과 동일합니다. 다른 점은 잔차를 계산하기전에 모델의 출력을 로짓에서 확률로 바꾸는 것 밖에 없습니다.\n\nlogit = lambda p: np.log(p / (1-p))\nproba = lambda z: 1 / (1+np.exp(-z))\n\n# 그래디언트부스팅 직접 만들기[+]\ndef train_gradient_boost_clf(X, y, h, M=10, lr=0.1, loss=None):\n    \"\"\"\n    X: train samples (N,D), N: # of samples, D: the dimension of a sample\n    y: target\n    h: weak learner instance\n    M: # of week learner (except for the first dummy learner)\n    lr: float: learning_rate, 'linesearch': linesearch\n    loss: loss function whose args are predictions and ground truth\n    \"\"\"\n    loss_values = []\n\n    # step 1\n    # 알고리즘에서처럼 상수로 첫번째 예측기를 정한다,  m=0\n    # 단 이번에는 평균에 대한 로짓을 함수값으로 가진다.\n    H = [ logit(y.mean()) ] \n    Fm_1 = np.ones(X.shape[0])*H[0]\n    steps = [None, ]\n\n    do_linesearch = True if type(lr) is str else False\n\n    # 학습\n    for m in range(1, M+1):# 최초 더미 예측기 제외 M개\n        # 각 단계의 학습기에서 만들어낸 로스 저장\n        # 여기서는 로스값을 저장할 목적으로 loss를 사용할 뿐\n        # 실제 계산을 위해 사용하는 것은 아님\n        # 이 코드에서 실제 loss의 미분은 squared loss로 하드코딩\n        if loss is not None:\n            loss_values.append( loss(y, Fm_1) )\n\n        # step 2 그래디언트의 값 N를 구함 pseudo-residual\n        # 위 유도 결과에 따라 잔차는 다시 확률로 바꿔서 계산\n        r_im = y - proba(Fm_1)\n        \n        # step 3 구해진 잔차 r_im에 대해서 학습기 hm을 학습\n        h_ = clone(h)\n        H.append( h_.fit(X, r_im) )\n\n        # step 4 & 5\n        if do_linesearch:\n            def f_rho(rho) :\n                return loss(y, Fm_1 + rho * H[-1].predict(X))\n\n            rho = gss(f_rho)\n            steps.append(rho)\n\n            Fm_1 += steps[-1]*H[-1].predict(X)\n        else:\n            Fm_1 += lr*H[-1].predict(X)\n    \n    if do_linesearch:\n        return {'learners':H, 'learning_rate':steps, 'loss_values':loss_values}\n    else:\n        return {'learners':H, 'learning_rate':lr, 'loss_values':loss_values}\n\n# 예측하기[+]\ndef predict_clf(X, gradient_boost):\n    \"\"\"\n    X: input, (N,D)\n    gradient_boost: model dict. that has been trained \n    \"\"\"\n    \n    H = gradient_boost['learners']\n    lr = gradient_boost['learning_rate']\n\n    # 0번째 약한학습기는 모든 입력에 대해서 상수 H[0] 출력\n    F = np.ones(X.shape[0])*H[0]\n\n    # 여기서 계산되는 함수값은 로짓이다.\n    for m in range(1, len(H)):\n        # lr이 리스트로 구성되었으면 각 단계에서 선탐색을 한것이므로\n        # 각 단계마다 결정된 스탭사이즈를 사용\n        if hasattr(lr, '__iter__'):\n            F += lr[m]*H[m].predict(X)\n        # 그렇지 않으면 고정 러닝레이트를 사용    \n        else:\n            F += lr*H[m].predict(X)\n\n    # 리턴하기 전에 확률값으로 바꾼다.\n    pred = proba(F)\n\n    return pred\n\n다시 코딩하긴 했지만 달라진 부분은 결국 손실함수가 바뀐 부분밖에 없습니다. 이전처럼 부스팅 과정에서 손실함수 값을 가져오기 위해 앞서 유도한 deviance loss를 준비합니다.\n\ndeviance_loss = lambda y, pred:  np.sum( -y*pred + np.logaddexp(0, pred) )\n\n그런데 방금 코딩한 손실함수를 사이킷런 구현에서 살펴보면 다음처럼 되어 있습니다.\n\nhttps://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/ensemble/_gb_losses.py#L633\n\n-2*np.mean( y*pred - np.logaddexp(0, pred) ) \n사이킥런에서는 sum() 대신 mean()을 쓰고 앞에 2가 곱해져 있습니다. mean()을 쓴것은 별로 문제가 안되는데 앞에 2가 곱해진 것은 좀 이해하기 어렵습니다. 2가 곱해진 이유는 로그가능도가 점근적으로 카이제곱분포를 따르도록 만들기 위함이라고 합니다. 물론 여기서 이야기하고 있는 내용은 통계학에서 가설검정과는 상관없는 내용이므로 2가 곱해전 것은 손실이 2배가 된다는 것 말고는 아무 의미도 없습니다.\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\n# learning_rate= 'linesearch'\ngb_clf1_tree_dev = train_gradient_boost_clf(X_clf1, y_clf1, h, M=50, \n                                      lr=learning_rate, loss=deviance_loss)\n\npred_X_clf1_tree_dev = predict_clf(X_clf1, gb_clf1_tree_dev)\n\n만들어진 모델을 학습데이터에 대해서 예측하여 몇개나 틀리는지 확인해보겠습니다.\n아래 셀을 실행하면 결과가 나오는데 위 셀에서 선탐색을 한 경우와 안한 경우를 비교해보면 선탐색을 한 경우 모든 데이터를 다 맞추는 것을 확인할 수 있습니다. 선탐색을 하지 않으면 42개는 틀리게 됩니다.\n\nnp.sum( (pred_X_clf1_tree_dev &gt;= 0.5).astype(int) != y_clf1 )\n\n42\n\n\n이제 이전 결과와 이번 결과를 비교 해봅시다.\n\n# 그림으로 확인\nX = X_clf1\ny = y_clf1\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nfig, ax = plt.subplots(figsize=(15,4), nrows=1, ncols=3)\n\n# squared loss로 한것\n# Z = np.zeros(X_grid.shape[0])\n# Z[pred_x_clf1_tree_mse &gt;= 0.5] = 1.\n# Z = Z.reshape(X1.shape)\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf1_tree_mse, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('ours, squared loss(lr=0.2)')\n\n#sklearn\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, pred_x_clf1_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('sklearn, deviance loss(lr=1.0)')\n\n# deviance loss\npred_x_clf1_tree_dev = predict_clf(X_grid, gb_clf1_tree_dev)\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf1_tree_dev &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\nax[2].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[2].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[2].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[2].set_xlim(x_min, x_max)\nax[2].set_ylim(y_min, y_max)\nax[2].set_title(f'ours, deviance loss(lr={learning_rate})')\n\nplt.show()\n\npred_x_clf1_tree_dev = Z.copy()\n\n# fig.savefig(\"dv_clf1_test.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위 그림은 분류 1번 데이터에 대해서 squared loss와 deviance loss를 적용한 것을 비교한 그림입니다. 가운데 그림은 사이킷런으로 실행한 결과입니다. 세번째 그림이 첫번째 그림보다 가운데 그림과 조금 더 비슷하게 보입니다. 하지만 손실을 deviance loss로 바꾸고 학습률도 동일하게 두었는데도 사이킷런 결과와 완전히 일치하지 않습니다. 학습 과정중에 손실이 어떻게 줄어드는 지도 확인해보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf1_tree_dev['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n\n\n\n손실도 안정적으로 줄어들고 있습니다. 사이킷런 구현에 다른 뭔가가 있는 듯 해 보입니다. 계속해서 두 번째 예제에 대해서도 실험해봅시다.\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\n# learning_rate = 'linesearch'\nlearning_rate = 1.0\ngb_clf2_tree_dev = train_gradient_boost_clf(X_clf2, y_clf2, h, M=50, \n                                      lr=learning_rate, loss=deviance_loss)\npred_X_clf2_tree_dev = predict_clf(X_clf2, gb_clf2_tree_dev)\n\n\nnp.sum( (pred_X_clf2_tree_dev &gt;= 0.5).astype(int) != y_clf2 )\n\n0\n\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nfig, ax = plt.subplots(figsize=(15,4), nrows=1, ncols=3)\n\n# squared loss로 한것\n# Z = Z.reshape(X1.shape)\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf2_tree_mse, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('ours, squared loss(lr=0.2)')\n\n#sklearn\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, pred_x_clf2_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('sklearn, deviance loss(lr=1.0)')\n\n# deviance loss\npred_x_clf2_tree_dev = predict_clf(X_grid, gb_clf2_tree_dev)\nZ = np.zeros(X_grid.shape[0])\nZ[pred_x_clf2_tree_dev &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\nax[2].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[2].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[2].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[2].set_xlim(x_min, x_max)\nax[2].set_ylim(y_min, y_max)\nax[2].set_title(f'ours, deviance loss(lr={learning_rate})')\n\nplt.show()\n\n# fig.savefig(\"dv_clf2_test.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n여기서도 예제1과 마찬가지로 deviance loss를 쓴 경우 사이킷런과 좀 더 닮은 더 안정적인 결정 영역을 만드는 것처럼 보입니다. 하지만 이번 예도 아직 sklearn의 결과와는 약간 차이가 있습니다. 손실이 안정적으로 떨어지지 않아서 그런지 단계별 손실값도 다시 한번 그려보겠습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(gb_clf2_tree_dev['loss_values'], '.-', color='C1')\n\nax.set_xlabel('iteration')\nax.set_ylabel('loss')\n\n\nplt.show()\n\n\n\n\n손실도 안정적으로 줄어들고 있습니다. 그렇다면 이제 남은 것은 무엇일까요? 무엇이 문제인지 알아보기 위해 여기서 사이킷런의 \\(h_1(x)\\)를 그려보도록 합시다.\n\n사이킷런 결과 분석(계속)\n\n# h_1을 시각화\nh_1 = gb_clf2_sk.estimators_[0,0]\n\nfig = plt.figure(figsize=(10,5), dpi=100)\nax = plt.axes()\n# 트리 그림 그리기\ndot_data = export_graphviz(h_1, out_file=None, \n                           max_depth=3, precision=3)\ngraph = graphviz.Source(dot_data, format=\"png\")\ntree_img = imread(graph.render())\nax.imshow(tree_img)\nax.axis('off')\nplt.show()\n\n# fig.savefig(\"gb_clf_sk_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n위처럼 그려지는 \\(h_1(x)\\) 대해서 이전에 했던것처럼 각 노드별로 결정되는 숫자값을 조사해보도록 합시다. 루트 노드와 깊이 1에서 첫번째 노드에 대한 squared_error, value를 계산해보겠습니다.\n\n# 첫번째 회귀트리 h_1(x)이 타겟으로 삼을 확률의 잔차\n# 잔차는 타겟 y와 h_0(x)가 출력한 값을 확률료 바꾼 값과의 차이\n# 첫단계에서 그값은 y의 평균이 된다.\nresidual_1 = y - y_mean\n\n# 루트 노드 squared_error, value\n# value는 노드에 모인 샘플들의 평균\nroot_mean = residual_1.mean()\n\nroot_squared_error = np.mean((residual_1 - root_mean)**2)\nprint(f\"root squared_error={root_squared_error:.3f}\")\nprint(f\"root value={root_mean:.3f}\")\n\n# 깊이1 첫번째 노드\ndepth1_first_node_idx = X_clf2[:,1]&lt;=0.06\ndepth1_first_node_target = residual_1[depth1_first_node_idx]\n\n# squared_error은 평균과 각 타겟 사이의 mse\nprint(\"첫번째 트리의 depth1 first node 결과\")\nprint(f\"squared_error: {np.mean((depth1_first_node_target - depth1_first_node_target.mean())**2):.3f}\") \nprint(f\"value: {depth1_first_node_target.mean():.3f}\") \n\nroot squared_error=0.250\nroot value=0.000\n첫번째 트리의 depth1 first node 결과\nsquared_error: 0.055\nvalue: 0.441\n\n\n결과를 보면 value는 해당 노드에 모인 샘플의 타겟 평균, squared_error은 타겟 평균과의 제곱오차 평균임을 알 수 있습니다.\n이제 두번째 단계로 리프노드를 조사해봅시다. 이전과 조사방식은 똑같습니다.\n\n# 깊이2 첫번째 노드\ndepth2_first_node_idx = X_clf2[:,0]&lt;=-0.418\n\ndepth2_first_node_target = residual_1[depth1_first_node_idx & depth2_first_node_idx]\ndepth2_first_node_sample = X_clf2[depth1_first_node_idx & depth2_first_node_idx]\n\n# squared_error은 평균과 각 타겟 사이의 mse\nprint(\"첫번째 트리의 depth2 first node 결과\")\nprint(f\"squared_error: {np.mean((depth2_first_node_target - depth2_first_node_target.mean())**2):.3f}\") \nprint(f\"value: {depth2_first_node_target.mean():.3f}\") \n\n첫번째 트리의 depth2 first node 결과\nsquared_error: 0.000\nvalue: -0.500\n\n\n앗! 이번에는 squared_error은 사이킷런과 동일하게 계산되지만 value는 동일하지 않습니다.\n이것으로 분류 문제에서 사이킷런이 사용하는 개별 학습기 회귀트리에서 출력하는 값은 샘플 타겟의 평균이 아니란 것을 알 수 있습니다.\n\nmy_h_1 = gb_clf2_tree_dev['learners'][1]\n\n# h_1을 시각화\nfig = plt.figure(figsize=(10,5), dpi=100)\nax = plt.axes()\n# 트리 그림 그리기\ndot_data = export_graphviz(my_h_1, out_file=None, \n                           max_depth=3, precision=3)\ngraph = graphviz.Source(dot_data, format=\"png\")\ntree_img = imread(graph.render())\nax.imshow(tree_img)\nax.axis('off')\nplt.show()\n\n# fig.savefig(\"gb_clf_h1.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n\n\n\nTreeBoost\n앞서 knn과 결정트리를 사용해서 약한 학습기가 특정 모델로 제한될 필요가 없다는 특징을 알아봤습니다. 그런데 결정트리를 사용한다면 매 반복에서 구해지는 약한 학습기들은 출력값으로 트리의 리프노드 숫자만큼의 출력값만 가지게 됩니다. 더 쉽게 이해하기 위해 다음 그림을 봅시다.\n\n어떤 반복단계 \\(m\\)에서 결정트리로 만들어진 약한 학습기 \\(h_m(\\mathbf{x})\\)가 위 그림과 같다고 할때 이 함수는 출력값을 영역 네 개로 나누게 됩니다. 이 영역을 \\(R_{jm}\\)이라고 표시합시다. \\(m\\)번째 트리의 \\(j\\)번째 영역이란 의미입니다. 전체 영역 개수는 \\(J_m\\)으로 표시합니다. 그럼 이 함수는 어떤 입력이 들어와도 출력값은 각 영역에서 계산되는 출력값 \\(b_{jm}\\)만 출력하게 됩니다. 이를 식으로 표시하면\n\\[\nh\\left(\\mathbf{x}; \\{b_m, R_j\\}_{j=1}^{J_m} \\right) = \\sum_{j=1}^{J_m} b_{jm} 1(\\mathbf{x} \\in R_{jm}) \\tag{15}\n\\]\n처럼 쓸 수 있습니다. 위 식에서 \\(1()\\)은 입력되는 \\(\\mathbf{x}\\)가 \\(\\mathbf{x} \\in R_{jm}\\)을 만족하면 1 아니면 0인 identity 함수입니다.\n이 표현법으로 아래 모델의 업데이트 식\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m h(\\mathbf{x};\\mathbf{a}_m)\n\\]\n을 다음처럼 바꿔 쓸 수 있습니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m \\sum_{j=1}^{J_m} b_{jm} 1(\\mathbf{x} \\in R_{jm}) \\tag{16}\n\\]\n식(16)에서 \\(b_{jm}\\)은 다음처럼 리프노드에 모인 샘플들의 타겟값 평균이 됩니다.\n\\[\nb_{jm} = ave_{\\mathbf{x} \\in R_{jm}} \\tilde{y}_i\n\\]\n\\(m\\)번째 약한 학습기가 피팅하는 타겟은 pseudo-residual \\(\\tilde{y}_i\\)이라는 것을 유념해야 합니다.\n약한 학습기의 리프노트 출력이 해당 노드에 모인 샘플들의 타겟 평균값이 되는, 즉 위 식에서 \\(b_{jm}\\)을 출력하는 트리는 현재 우리가 직접 코딩한 그래디언트 부스팅이 사용하고 있는 트리입니다. 그런데 사이킷런에서 사용하는 개별 트리는 이 \\(b_{jm}\\) 값을 출력하지 않았음을 상기합시다. 그래서 지금 이 이야기를 하고 있는 것입니다. 우리가 계산한 \\(h_1(\\mathbf{x})\\)의 첫번째 리프노드 출력값은 -0.5였는데 실제 사이킷런에서 출력한 값은 -2.0이였습니다.\n식(16)을 더 이해하기 편하게 그림으로 그려봅시다. 첫번째 약학 학습기가 만들어낸 잔차를 학습하는 두번째 학습기를 실제로 그려보겠습니다.\n먼저 첫번째 약한 학습기로 부터 잔차를 구합니다.\n\n# r_im = y - proba(F)\nresidual_for_h1 = y_clf2 - proba(gb_clf2_tree_dev['learners'][0])\nresidual_for_h1\n\narray([ 0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5, -0.5, -0.5,  0.5,\n       -0.5,  0.5,  0.5, -0.5,  0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5,\n       -0.5, -0.5,  0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5, -0.5, -0.5,\n       -0.5,  0.5, -0.5,  0.5,  0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n        0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5,\n        0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5, -0.5,  0.5,\n       -0.5, -0.5, -0.5, -0.5, -0.5,  0.5,  0.5, -0.5,  0.5,  0.5, -0.5,\n        0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n       -0.5,  0.5, -0.5, -0.5,  0.5, -0.5, -0.5, -0.5,  0.5,  0.5, -0.5,\n       -0.5])\n\n\n이렇게 구한 잔차를 데이터로 하고 그 위에 두번째 약한 학습기를 실제로 그려보면\n\n# colab 노트북을 로컬런타임에 연결했다면 이 셀을 실행 \n# 호스팅 런타임이면 실행안해도 됨\n# https://plotly.com/python/renderers/#setting-the-default-renderer\nimport plotly.io\n\n# local에서 그냥 실행하는 상황이면 notebook, jupyterlab 으로\nplotly.io.renderers.default = \"notebook\" \n\n\n# https://plotly.com/python/sliders/\n# https://stackoverflow.com/questions/62397485/plotly-relabelling-animation-tick-marks-on-the-slider-bar\n\nX = X_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 20)\nyy = np.linspace(y_min, y_max, 20)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred = my_h_1.predict(X_grid)\npred = pred.reshape(X1.shape)\n\nlayout = go.Layout(\n    title='Gradient H_1(x)',\n    width=600, height=600,\n    margin=dict(l=0, r=0, b=0, t=25),\n    scene = dict(\n        xaxis = dict(title='x1', range=[x_min, x_max],),\n        yaxis = dict(title='x2', range=[y_min, y_max],),\n        zaxis = dict(title='residual', range=[-2,2],),\n        aspectratio=dict(x=1, y=1, z=1)\n    )\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\nfig.add_trace(\n    go.Scatter3d(\n        x=X[:,0], y=X[:,1], z=residual_for_h1, \n        mode='markers',\n        marker=dict(\n            symbol='circle', size=3, color='#E64A45',\n            line=dict(color='#000000', width=0.5),\n            opacity=1.0\n        ),\n        name='Data', visible=True\n    )\n)\n\n# Add traces, one for each slider step\nrho_m = np.linspace(0, 3, 52)\nfor rho in rho_m:\n    fig.add_trace(\n        go.Surface(\n            x=X_grid[:,0].reshape(X1.shape), y=X_grid[:,1].reshape(X1.shape), \n            z=pred * rho, \n            showscale=False,  opacity=1.,\n            contours=dict(\n                x=dict(show=True, highlight=True),\n                y=dict(show=True,  highlight=True),\n                z=dict(show=True,  highlight=True),\n            ),visible=False\n        )\n    )\n\nfig.data[25].visible = True\n\n# Create and add slider\nsteps = []\n\nfor i in range(1, len(fig.data)):\n    step = dict(\n        method=\"update\",\n        args=[\n            {\"visible\": [True] + [False] * (len(fig.data)-1)},\n            # {\"title\": \"rho: \" + f\"{rho_m[i-1]:.2f}\"}\n        ],  # layout attribute\n        label=f\"{rho_m[i-1]:.2f}\"\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [\n    dict(\n        active=17,\n        currentvalue={\"prefix\": \"rho: \"},\n        pad={\"l\":10, \"t\": 50, \"r\":10, \"b\":10},\n        steps=steps\n    )\n]\n\nfig.update_layout(sliders=sliders)\n\nfig.show()\n\n\n                                                \n\n\n위 처럼 그러집니다. 위 그림은 식(16)에서 \\(\\sum_{j=1}^{J_m} b_{jm} 1(\\mathbf{x} \\in R_{jm})\\)에 해당하는 부분입니다. 이 그래디언트로 선탐색을 해서 \\(\\rho_m\\)을 구할텐데 위 그래프에서 rho_m 슬라이드 바를 움직면 스탭사이즈 rho_m에 따른 변화가 그려집니다.\n위 그래프에서 볼 수 있는것 처럼 두번째 약한 학습기는 전체 영역을 네개로 나누고 각 영역에서 동일한 값을 출력합니다. 이 출력값을 \\(\\gamma_{jm}\\)이라 두면 식(16)은 다음처럼 쓸 수 있습니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) +  \\sum_{j=1}^{J_m} \\gamma_{jm} 1(\\mathbf{x} \\in R_{jm}) \\tag{17}\n\\]\n여기서 \\(\\gamma_{jm} = \\rho_{m} b_{jm}\\)입니다.\n두번째 약한 학습기가 출력값을 식(17)처럼 출력한다는 말은 각 리프노트에 모인 샘플의 타겟값들을 평균한 값에 선탐색으로 찾아낸 스탭사이즈 \\(\\rho_m\\)을 일괄적으로 곱해서 출력한다는 것입니다. 그런데 각 출력값 \\(\\gamma_{jm}\\)은 완전히 분리된 값이므로 이를 각각 분리된 네개의 개별적인 기저함수separate basis function에 의한 출력값을 더한다고 생각할 수 있습니다. 예를 들어 \\(b_{1m}\\)이 변하면 \\(b_{2m}\\)이 그 영향을 받아서 같이 변하지 않는다는 의미입니다. 그 개별 기저함수의 출력값은 결국 다음처럼 손실함수 \\(L\\)을 최소화 시키는 값이 \\(J\\)개가 될 것입니다.\n\\[\n\\{\\gamma_{jm}\\}_{j=1}^J = \\underset{\\{\\gamma_j\\}_{i=1}^J}{\\text{argmin}} \\sum_{i=1}^N L \\left( y_i, F_{m-1}(\\mathbf{x}_i) + \\sum_{j=1}^J \\gamma_j 1(\\mathbf{x} \\in R_{jm})\\right)\n\\]\n그런데 각 영역에서 함수값은 완전히 분리disjoint되어 있기 때문에 각 영역별로 최적화를 따로 수행해도 됩니다.\n\\[\n\\gamma_{jm} = \\underset{\\gamma}{\\text{argmin}} \\sum_{\\mathbf{x}_i \\in R_{jm}} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma) \\tag{18}\n\\]\n\nNewton’s Method\n위 식(18)을 풀기 위해 최적화 수법 중 2계법인 뉴턴메소드를 사용합니다. 테일러 시리즈 2차 근사를 하고 근사된 식을 \\(\\gamma\\)로 미분하여 0으로 두고 방정식을 풉니다.\n\\[\n\\sum_{\\mathbf{x}_i \\in R_{jm}} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma) = \\sum_{\\mathbf{x}_i \\in R_{jm}} \\left(  L(y_i, F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial L}{\\partial F} +  \\frac{1}{2} \\gamma^2 \\frac{\\partial^2 L}{\\partial F^2} + O(\\gamma^3)\\right)\n\\]\n이제 \\(\\gamma\\)에 대해 미분하고 \\(\\gamma\\)의 2차항까지 남기면\n\\[\n\\begin{aligned}\n\\sum_{\\mathbf{x}_i \\in R_{jm}} \\frac{d}{d \\gamma} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma) &=  \\sum_{\\mathbf{x}_i \\in R_{jm}} \\frac{d}{d \\gamma} \\left(  L(y_i, F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial L}{\\partial F} +  \\frac{1}{2} \\gamma^2 \\frac{\\partial^2 L}{\\partial F^2} + O(\\gamma^3)\\right) \\\\\n&\\approx \\sum_{\\mathbf{x}_i \\in R_{jm}} \\left( \\frac{\\partial }{\\partial F} L(y_i + F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial^2}{\\partial^2 F} L(y_i + F_{m-1}(\\mathbf{x}_i)) \\right)\n\\end{aligned}\n\\]\n결과를 0으로 놓고 정리합니다.\n\\[\n\\sum_{\\mathbf{x}_i \\in R_{jm}} \\left( \\frac{\\partial }{\\partial F} L(y_i + F_{m-1}(\\mathbf{x}_i)) + \\gamma \\frac{\\partial^2}{\\partial^2 F} L(y_i + F_{m-1}(\\mathbf{x}_i)) \\right) = 0\n\\]\n시그마 기호를 풀고 이항하여 정리하면\n\\[\n\\gamma = \\frac{\\sum_{\\mathbf{x}_i \\in R_{jm}} -\\frac{\\partial}{\\partial F} L(\\cdot) }{\\sum_{\\mathbf{x}_i \\in R_{jm}} \\frac{\\partial^2}{\\partial F^2} L(\\cdot) }\n\\]\n위 식에서 식([4])에 의해 분자는 다음과 같습니다.\n\\[\n-\\frac{\\partial }{\\partial F} L(y_i, F_{m-1}(\\mathbf{x}_i)) = y_i - p_{m-1}(\\mathbf{x}_i)\n\\]\n위 식에서 \\(p_{m-1}(\\mathbf{x}_i)\\)는 \\(F_{m-1}(\\mathbf{x}_i)\\)가 출력한 로짓을 확률로 바꾼 값입니다.\n이 결과를 이용해서 분모를 다음처럼 정리할 수 있습니다.\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial F^2} L(y_i, F_{m-1}(\\mathbf{x}_i)) &= \\frac{\\partial}{\\partial F} -y_i + p_{m-1}(\\mathbf{x}_i) \\\\\n&= \\frac{\\partial}{\\partial F} \\left( -y_i + \\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\right) \\\\\n&=  \\frac{\\partial}{\\partial F} \\left( -y_i + \\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^{-1} e^{F_{m-1}(\\mathbf{x}_i)} \\right) \\\\\n&= -\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^{-2} e^{2F_{m-1}(\\mathbf{x}_i)} + \\left( 1+e^{F_{m-1}(\\mathbf{x}_i)} \\right)^{-1} e^{F_{m-1}(\\mathbf{x}_i)} \\\\\n&= \\frac{-e^{2F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2} + \\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\\\\n&= \\frac{-e^{2F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2} + \\frac{e^{F_{m-1}(\\mathbf{x}_i)}+e^{2F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2} \\\\\n&= \\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{\\left(1+e^{F_{m-1}(\\mathbf{x}_i)}\\right)^2}=\\frac{e^{F_{m-1}(\\mathbf{x}_i)}}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\times \\frac{1}{1+e^{F_{m-1}(\\mathbf{x}_i)}} \\\\[5pt]\n&= p_{m-1}(\\mathbf{x}_i) \\left( 1- p_{m-1}(\\mathbf{x}_i) \\right)\n\\end{aligned}\n\\]\n정리한 분자 분모를 원래 식에 대입하면 다음과 같은 결과를 얻을 수 있습니다.\n\\[\n\\gamma_{jm} = \\frac{\\sum_{\\mathbf{x}_i\\in R_{jm}} y_i - p_{m-1}(\\mathbf{x}_i) }{\\sum_{\\mathbf{x}_i\\in R_{jm}} p_{m-1}(\\mathbf{x}_i)(1-p_{m-1}(\\mathbf{x}_i))} \\tag{[5]}\n\\]\n위 식의 의미는 \\(m\\)번째 결정 트리 \\(h_m(\\mathbf{x})\\)는 \\(J_m\\)개의 출력을 출력하도록 학습되는데 각 출력값은 최종 리프 노드에 속하는 샘플 \\(\\mathbf{x}_i \\in R_{jm}\\)들을 사용해서 계산될 수 있다는 것입니다.\n아래 그림은 각 영역별로 최적화를 거쳐 바로 출력값을 구한 두번째 약한학습기의 모습입니다.\n\nX = X_clf2\n\npred = h_1.predict(X_grid)\npred = pred.reshape(X1.shape)\n\ndata = [\n    go.Scatter3d(x=X[:,0], y=X[:,1], z=residual_for_h1, mode='markers',\n                marker=dict(\n                    symbol='circle', size=3, color='#E64A45',\n                    line=dict(color='#000000', width=0.5),\n                    opacity=1.0\n                ),\n                name='Data'\n    ),\n    go.Surface(x=X_grid[:,0].reshape(X1.shape), y=X_grid[:,1].reshape(X1.shape), \n                z=pred, \n                showscale=False,  opacity=1.,\n                contours=dict(\n                x=dict(show=True, highlight=True),\n                y=dict(show=True,  highlight=True),\n                z=dict(show=True,  highlight=True),\n            ),\n    ),\n]\n\nlayout = go.Layout(\n    title='Gamma_r1',\n    width=500, height=500,\n    margin=dict(l=0, r=0, b=0, t=25),\n    scene = dict(\n        xaxis = dict(title='x1', range=[x_min, x_max],),\n        yaxis = dict(title='x2', range=[y_min, y_max],),\n        zaxis = dict(title='residual', range=[-2,2],),\n        aspectratio=dict(x=1, y=1, z=1)\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n이렇게 약한 학습기로 결정트리를 사용할 때 트리의 리프노드 출력값을 조절하는 방식을 TreeBoost라고 합니다.\nTreeBoost 방식을 사용하면 약한 학습기를 학습시키고 난후 모델을 업데이트 할 때 선탐색을 하지 않기 때문에 훨씬 빠르게 모델을 적합시킬 수 있습니다.\n지금까지 이야기한 TreeBoost를 구현하기 위해서는 보통의 DecisionTreeRegressor로 부터 최종 출력값을 TreeBoost에 맞도록 계산하는 추가 과정이 필요합니다. \\(h_m(\\mathbf{x})\\)의 출력을 적절히 만들어내는 과정은 다음 절차로 코딩할 수 있습니다.\n\n학습된 \\(h_m(\\mathbf{x})\\)을 이용하여 입력된 샘플들이 몇번 리프로 가는지 알아낸다. 즉 다음을 판단한다 \\(\\mathbf{x}_i \\in R_{jm}\\)\n1에서 얻어진 샘플들이 도착하는 리프노드 인덱스를 유일하게 만들어 리프노드 인덱스 집합을 얻는다.\n2에서 만든 리프노드 인덱스를 순회하면서 각 리프노드에 모이는 샘플들의 타겟(확률의 잔차)를 구하고 위 식([5])로 리프노드의 출력값을 계산한다.\n\n위 과정을 \\(h_1(\\mathbf{x})\\)에 대해서 시험적으로 코딩해보면 아래와 같습니다.\n\n# 첫번째 회귀트리 h_1(x)이 타겟으로 삼을 확률의 잔차\n# 잔차는 h_0(x)가 출력한 타겟의 평균과의 차이가 된다.\nresidual_1 = y - y_mean\nprint(residual_1.shape)\n# 첫번째 회귀트리 h_1(x)이 타겟으로 삼을 확률의 잔차를 \n# 만들어낸 이전 트리 h_0(x)의 예측 확률\nprev_proba = y_mean\n\n####################################################\n# Friedman의 \"TreeBoost\"에서 hm 출력 만들어 내기\n####################################################\n# 1. 각 샘플이 몇번 리프로 가는지 알아낸다.\nleaf_idx_X = h_1.apply(X_clf2)\nprint(\"샘플들이 몇번 노드로 가는지 알아냄\")\nprint(leaf_idx_X)\n\n# 2. 얻어진 샘플에 대한 리프노드 인덱스를 유일하게 만들어 \n#    리프노드 인덱스 집합을 얻는다.\nRm = sorted(set(leaf_idx_X))\nprint(\"\\n샘플들이 도착하는 리프노드 인덱스\")\nprint(Rm)\n\n# 3. 2에서 만든 리프노드 인덱스를 순회하면서 \n#    각 리프노드의 출력값을 계산한다.\n# 트리의 전체 노드 수만큼 자리를 만든다.\nhm_output = np.zeros(max(Rm)+1)\nfor j in Rm:\n    residual = residual_1[leaf_idx_X==j]\n    # 첫번째 트리의 예측확률은 값이 한개밖에 없으니니까 분모 sum()은 그냥\n    # 해당 샘플 개수를 곱한다. eq([5])\n    hm_output[j] =  residual.sum() / (residual.shape[0]*(prev_proba*(1-prev_proba)))\n\nwith np.printoptions(precision=3):\n    print(\"\\n계산된 리프노드의 출력\")\n    print(hm_output)\n\n    # 4. 1에서 얻은 결과 인덱스를 3에서 얻은 리프노드\n    #    결과에서 조회하여 최종 출력을 만든다.\n    print(\"\\n샘플들에 대한 출력\")\n    print(hm_output[leaf_idx_X])\n\n(100,)\n샘플들이 몇번 노드로 가는지 알아냄\n[6 3 5 5 6 6 6 5 5 5 3 5 3 6 5 3 3 5 5 2 3 5 5 5 3 3 5 3 5 5 5 5 5 5 6 5 3\n 3 5 5 5 5 5 5 6 3 3 3 3 5 3 3 3 3 5 3 3 6 5 5 5 5 3 3 2 3 5 5 5 5 5 5 6 5\n 3 5 5 3 3 5 3 5 3 5 5 5 5 5 5 6 5 5 3 5 5 5 3 3 6 5]\n\n샘플들이 도착하는 리프노드 인덱스\n[2, 3, 5, 6]\n\n계산된 리프노드의 출력\n[ 0.     0.    -2.     2.     0.    -1.418  1.636]\n\n샘플들에 대한 출력\n[ 1.636  2.    -1.418 -1.418  1.636  1.636  1.636 -1.418 -1.418 -1.418\n  2.    -1.418  2.     1.636 -1.418  2.     2.    -1.418 -1.418 -2.\n  2.    -1.418 -1.418 -1.418  2.     2.    -1.418  2.    -1.418 -1.418\n -1.418 -1.418 -1.418 -1.418  1.636 -1.418  2.     2.    -1.418 -1.418\n -1.418 -1.418 -1.418 -1.418  1.636  2.     2.     2.     2.    -1.418\n  2.     2.     2.     2.    -1.418  2.     2.     1.636 -1.418 -1.418\n -1.418 -1.418  2.     2.    -2.     2.    -1.418 -1.418 -1.418 -1.418\n -1.418 -1.418  1.636 -1.418  2.    -1.418 -1.418  2.     2.    -1.418\n  2.    -1.418  2.    -1.418 -1.418 -1.418 -1.418 -1.418 -1.418  1.636\n -1.418 -1.418  2.    -1.418 -1.418 -1.418  2.     2.     1.636 -1.418]\n\n\n첫번째 학습기 \\(h_1(x)\\)에 대해서 샘플 100개에 대한 출력값을 성공적으로 구할 수 있었습니다. 마지막에 출력된 값 100개와 위에서 그린 \\(h_1(x)\\)의 리프 노드 출력값을 비교해보세요. 값 100개가 그림에 나타난 value값들로 구성되었음을 알 수 있을 것입니다.\n이제 모든 조각이 완성되었습니다! 이제 우리는 분류 문제에서 각 개별 트리 학습기가 어떤 값을 출력하면 선탐색하지 않고 효율적으로 경사하강을 할 수 있는지 알았습니다.\n이 결과를 앞서 “직접구현 2”에서 작성한 코드에 추가해봅시다.\n\nlogit = lambda p: np.log(p / (1-p))\nproba = lambda z: 1 / (1+np.exp(-z))\n\n# 그래디언트부스팅 직접 만들기\ndef train_tree_boost(X, y, h, M=10, lr=0.1, loss=None):\n    \"\"\"\n    X: train samples (N,D), N: # of samples, D: the dimension of a sample\n    y: target\n    h: weak learner instance, must be a DeicisionTreeRegressor\n    M: # of week learner (except for the first dummy learner)\n    lr: learning rate\n    loss: loss function whose args are predictions and ground truth\n    \"\"\"\n    loss_values = [] # 학습 중 로스를 저장\n    probas = [] # 개별 트리 학습기라 로짓을 출력하면 확률로 바꿔서 여기에 저장\n    r_im_s = [] # 개별 트리 학습기가 만들어낸 확률의 잔차(그래디언트)를 저장\n    Ho = [None, ] # 개별 트리 학습기가 출력할 출력을 저장\n\n    # 위 알고리즘 (1)에서처럼 상수로 첫번째 예측기를 정한다,  m=0\n    # 단 이번에는 평균에 대한 로짓을 함수값으로 가진다.\n    H = [ logit(y.mean()) ] \n    Fm_1 = np.ones(X.shape[0])*H[0]\n\n    # 학습\n    for m in range(1, M+1):# 최초 더미 예측기 제외 M개 트리 \n        # 각 단계의 학습기에서 만들어낸 로스 저장\n        if loss is not None:\n            loss_values.append( loss(y, Fm_1) )\n\n        # 잔차는 다시 확률로 바꿔서 계산\n        probas.append(proba(Fm_1))\n        r_im = y - probas[-1]\n        r_im_s.append(r_im)\n\n        # 구해진 잔차 r_im에 대해서 학습기 hm을 학습\n        h_ = clone(h)\n        \n        H.append( h_.fit(X, r_im) )\n        \n        ################################################################\n        # Freidman TreeBoost\n        # H[m] 학습이 끝났으면 리프노드에서 출력할 출력 구하기\n        # 1. 각 샘플이 몇번 리프로 가는지 알아낸다.\n        leaf_idx_X = H[m].apply(X)\n\n        # 2. 얻어진 샘플에 대한 리프노드 인덱스를 유일하게 만들어 \n        #    리프노드 인덱스 집합을 얻는다.\n        Rm = sorted(set(leaf_idx_X))\n\n        # 3. 2에서 만든 리프노드 인덱스를 순회하면서 \n        #    각 리프노드의 출력값을 계산한다.\n        ho = np.zeros(max(Rm)+1)\n\n        for j in Rm:\n            residual = r_im_s[-1][leaf_idx_X == j]\n            prev_proba = probas[-1][leaf_idx_X == j]\n            ho[j] = residual.sum() / np.sum(prev_proba*(1-prev_proba)) \n\n        # 4. H[m]의 출력을 저장한다.\n        # 이제부터 H[m]의 출력은 H[m].predict(X)로 얻는 것이 아니라\n        # Ho[m][ H[m].apply(X) ] 로 얻으면 된다.\n        Ho.append(ho)\n        ################################################################\n\n        # update\n        Fm_1 += lr*Ho[-1][H[-1].apply(X)]\n\n    # print('return')    \n    return {'learners':H, 'learners_out':Ho, \n            'learning_rate':lr, 'loss_values':loss_values}\n\n\n# 예측하기\ndef predict_tree_boost(X, tree_boost):\n    H = tree_boost['learners']\n    Ho = tree_boost['learners_out']\n    lr = tree_boost['learning_rate']\n\n    F = np.ones(X.shape[0])*H[0]\n\n    # 여기서 계산되는 함수값을 로짓이다.\n    for m in range(1, len(H)):\n        F += lr*Ho[m][H[m].apply(X)]\n\n    # 리턴하기 전에 확률값으로 바꾼다.\n    pred = proba(F)\n\n    return pred\n\n이제 만든 함수를 테스트 해봅시다. 사이킷런과 결과를 비교하기 위해 GradientBoostingClassifier를 실행했을 때와 동일한 조건으로 실행합시다.\nlearning_rate = 1.0\ngbrt = GradientBoostingClassifier(criterion='squared_error', \n                                 learning_rate=learning_rate, \n                                  n_estimators=50, max_depth=2)\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\ntb_clf2_tree_dev = train_tree_boost(X_clf2, y_clf2, h, M=50, \n                              lr=learning_rate, loss=deviance_loss)\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred_tb_x_clf2_tree_dev = predict_tree_boost(X_grid, tb_clf2_tree_dev)\n\nZ = np.zeros(X_grid.shape[0])\nZ[pred_tb_x_clf2_tree_dev &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, dpi=100)\n\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf2_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('sklearn')\n\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('tree boost')\n\nplt.show()\n\npred_tb_x_clf2_tree_dev = Z.copy()\n#\n# fig.savefig(\"clf2_sk_and_tb.png\", dpi=image_dpi, bbox_inches='tight')\n\n\n\n\n\nnp.sum( pred_tb_x_clf2_tree_dev != pred_x_clf2_sk )\n\n0\n\n\n사이킷런 결과와 완벽하게 일치합니다!\n이렇게 사이킷런에서 그래디언트 부스팅을 구현한 방식을 완전히 재현했습니다. 몇가지 다른 예를 테스트해보겠습니다.\n\n\n추가 예제\n\ngaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(\n    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=10\n)\nX2, y2 = make_gaussian_quantiles(\n    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=10\n)\n\nX_clf3 = np.concatenate((X1, X2))\ny_clf3 = np.concatenate((y1, -y2 + 1))\n\nfig = plt.figure(dpi=100)\nax = plt.axes()\n\nax.plot(X_clf3[y_clf3==0][:, 0], X_clf3[y_clf3==0][:, 1], 'o', color='C1')\nax.plot(X_clf3[y_clf3==1][:, 0], X_clf3[y_clf3==1][:, 1], '^', color='C2')\n\nplt.show()\n\n\n\n\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\ntb_clf3_tree_dev = train_tree_boost(X_clf3, y_clf3, h, M=10, \n                                   lr=learning_rate, loss=deviance_loss)\n\n\nlearning_rate = 1.0\ngb_clf3_sk = GradientBoostingClassifier(criterion='squared_error', \n                                        learning_rate=learning_rate, \n                                        n_estimators=10, max_depth=2)\ngb_clf3_sk.fit(X_clf3, y_clf3)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)\n\n\n\n# 그림으로 확인\nX = X_clf3\ny = y_clf3\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred = predict_tree_boost(X_grid, tb_clf3_tree_dev)\npred_tb_x_clf3_tree_dev = np.zeros(X_grid.shape[0])\npred_tb_x_clf3_tree_dev[pred &gt;= 0.5] = 1.\npred_tb_x_clf3_tree_dev = pred_tb_x_clf3_tree_dev.reshape(X1.shape)\n\npred_x_clf3_sk = gb_clf3_sk.predict(X_grid)\npred_x_clf3_sk = pred_x_clf3_sk.reshape(X1.shape)\n\nfig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=2, dpi=100)\n\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_x_clf3_sk, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\nax[0].set_title('sklearn')\n\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, pred_tb_x_clf3_tree_dev, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\nax[1].set_title('tree boost')\n\nplt.show()\n\n\n\n\n\n\nnp.sum( pred_tb_x_clf3_tree_dev != pred_x_clf3_sk )\n\n0\n\n\n\n\nIris dataset\n\n# 아이리스에 대해서\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\nX_iris = iris['data']\n# y_iris = iris['target']\ny_iris = (iris[\"target\"] == 2).astype(int)\n\n\n# y_iris[130] = 0\n# np.log( y_iris.mean() / (1-y_iris.mean()) )\n\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 1.0\ntb_iris = train_tree_boost(X_iris, y_iris, h, M=10, \n                              lr=learning_rate, loss=deviance_loss)\n\ngb_iris_sk = GradientBoostingClassifier(criterion='squared_error', \n                                 learning_rate=learning_rate, \n                                  n_estimators=10, max_depth=2)\ngb_iris_sk.fit(X_iris, y_iris)\n\nGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(criterion='squared_error', learning_rate=1.0,\n                           max_depth=2, n_estimators=10)\n\n\n\npred_tb_iris_proba = predict_tree_boost(X_iris, tb_iris)\npred_iris_sk_proba = gb_iris_sk.predict_proba(X_iris)\n\n\n# 소수 6자리에서 반올림해서 비교\nnp.sum( \n    (\n        np.round(pred_tb_iris_proba, 6) != np.round(pred_iris_sk_proba[:,1],6)\n    ).astype(int)  \n)\n\n0\n\n\n\n\n\n규제, shrinkage\n마지막으로 shrinkage라고 하는 규제 방법에 대해서 이야기하고 글을 마무리 하겠습니다. 앞서 개별 트리 학습기의 리프 노드 출력값을 테일러 시리즈 2차 근사를 통해 다음처럼 직접 구하는 방법을 알아봤습니다.\n\\[\nF_m(x) = F_{m-1}(\\mathbf{x}) + \\sum_{j=1}^{J_m} \\gamma_{jm} \\mathbb{1}(\\mathbf{x} \\in R_{jm}), \\qquad \\gamma_{jm} = \\underset{\\gamma}{\\text{argmin}} \\sum_{\\mathbf{x}_i \\in R_{jm}} L \\left(y_i, F_{m-1}(\\mathbf{x}_i)+\\gamma \\right)\n\\]\n이렇게 구해진 값은 2차 근사 형태로 선탐색까지 마친 결과이므로 더 이상 스탭사이즈를 곱하지 않고 그 값을 바로 \\(F_{m-1}(\\mathbf{x})\\)에 더하게 됩니다. 이런 이유로 앞선 실험에서 학습률을 1로 두었던 것입니다. (그래디언트에 곱하는 숫자를 1로 두어 아무것도 곱하지 않는것과 같다는 의미)\n하지만 최종 구현에서는 \\(F_{m-1}(\\mathbf{x})\\)에 더해지는 \\(\\gamma_{jm}\\)의 기여 정도를 조정하기 위해 마치 학습률처럼 1보다 작은 상수를 곱하게 됩니다. 따라서 위 업데이트 룰은 다음처럼 됩니다.\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\nu \\sum_{j=1}^{J_m} \\gamma_{jm} \\mathbb{1}(\\mathbf{x} \\in R_{jm}), \\qquad 0 &lt; \\nu \\le 1\n\\]\n이렇게 개별 학습기의 출력값을 적당히 축소시키면 \\(\\nu=1\\)인 경우 보다 모델의 일반화 성능이 크게 증가함을 확인할 수 있습니다.\n\nh = DecisionTreeRegressor(random_state=42, max_depth=2)\n\nlearning_rate = 0.1\ntb_clf2_tree_dev_nu = train_tree_boost(X_clf2, y_clf2, h, M=50, \n                              lr=learning_rate, loss=deviance_loss)\n\n\n# 그림으로 확인\nX = X_clf2\ny = y_clf2\n\neps = X.std() / 2.\nx_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\ny_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\nxx = np.linspace(x_min, x_max, 500)\nyy = np.linspace(y_min, y_max, 500)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\npred = predict_tree_boost(X_grid, tb_clf2_tree_dev_nu)\nZ = np.zeros(X_grid.shape[0])\nZ[pred &gt;= 0.5] = 1.\nZ = Z.reshape(X1.shape)\n\nfig, ax = plt.subplots(figsize=(10,4), nrows=1, ncols=2, dpi=100)\n\nax[0].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[0].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[0].contourf(X1, X2, pred_tb_x_clf2_tree_dev, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[0].set_title(r\"$\\nu=1.0$\")\nax[0].set_xlim(x_min, x_max)\nax[0].set_ylim(y_min, y_max)\n\nax[1].plot(X[y==0][:, 0], X[y==0][:, 1], 'o', color='C1')\nax[1].plot(X[y==1][:, 0], X[y==1][:, 1], '^', color='C2')\nax[1].contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\nax[1].set_title(r\"$\\nu=$\"+f\"{learning_rate}\")\nax[1].set_xlim(x_min, x_max)\nax[1].set_ylim(y_min, y_max)\n\nplt.show()\n\n# fig.savefig(\"shrinkage.png\", dpi=image_dpi, bbox_inches='tight')"
  },
  {
    "objectID": "posts/gradientboost/gradient_boosting.html#마무리",
    "href": "posts/gradientboost/gradient_boosting.html#마무리",
    "title": "A Gentle Introduction to Gradient Boosting",
    "section": "마무리",
    "text": "마무리\n이렇게 좀 길었지만 그래디언트 부스팅에 대해서 자세히 알아봤습니다. 이상의 내용을 잘 이해하고 있으면 최근 가장 각광받고 있는 XGBoost라는 알고리즘을 이해하는 큰 도움이 됩니다. 왜냐하면 XGBoost도 기본적인 논리의 전개는 지금까지 알아본 내용과 모두 같기 때문입니다. 세부적으로 여러 다른 점이 있기는 하지만 핵심적인 차이점은 약한 학습기를 학습시키는 방식으로 목적함수에 규제항을 적용하고 이를 테일러 시리즈 2차 근사하여 뉴턴 메소드를 사용하는 부분입니다.*) 그런데 이 방식도 사실 우리 글에서 알아본 TreeBoost에서 리프노드 값을 결정하는 것과 동일한 방식입니다.\n제 블로그의 글이 늘 그렇지만 라이브러리를 가져다 사용하는 입장에서는 큰 도움이 되지 않을지 모르겠습니다. 하지만 원리를 파악하고 싶어하는 분들께는 꼭 도움이 되디라 믿으며 글을 마치도록 하겠습니다.\n\n*) 물론 가중 분위수 스케치weighted quantile sketch나 하드웨어 특성을 활용해서 효율을 높이는 부분은 너무 세부적이라 이해하기 힘든 내용들이긴 합니다. 하지만 개별 트리를 학습하는데 쓰이는 손실이나 분기점을 찾기 위한 랜덤서치 방식은 이 글을 읽고 나면 충분히 이해할 수 있습니다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "트랜스포머가 딥러닝 세상을 지배한 지금 과거 CNN, RNN을 모르고 딥러닝을 안다고 할 수 없듯이 이제는 트랜스포머를 이해하지 못하고 딥러닝을 공부한다고 말할 수 없는 시대가 되었다.\n트랜스포머가 초기 타겟한 작업이 번역이기 때문에 트랜스포머를 설명하는 글에서 단골로 등장하는 예제가 번역기 예제다. 주로 영어-독일어, 영어-스페인어 예제가 많다. 하지만 쉽게 접할 수 있는 알파벳권 언어 사이의 번역기 예제에 비해 영어-한국어 데이터를 사용해서 번역기를 학습시키는 예제는 이상하리만큼 찾아보기 힘들었다. 왜 그런지 이유는 잘 모르겠지만 어쨌든 거의 없다. 그래서 영어-한국어 문장쌍이 들어있는 원시데이터를 사용해 T5 모델로 영어-한국어 번역기를 데모 수준 정도로 학습하는 예제를 만들어 블로그에 포스팅하면 많은 사람들에게 도움이 되지 않을까 해서 이 글을 적게 되었다.\n이 글에서는 트랜스포머에 대한 기초적인 내용은 다루지 않고 오직 데이터를 어떻게 준비하고, 어떻게 데이터를 모델에 입력하여 학습을 시키고, 마지막으로 어떻게 영어로 부터 한국어 번역 문장을 출력시키는가 하는 것에만 초점을 맞추었다. 그리고 코드를 복잡하게 만드는 그 어떤 테크닉도 사용하지 않는다. 오로지 가장 간단하게 한국어 번역기를 구축하는데만 집중할 것이다. 사실 이 글의 대부분 내용은 허깅페이스 도움말에 있는 것을 정리한것에 지나지 않는다. 하지만 입문자나 이제 막 트랜스포머를 이용해서 한국어 번역기를 만들고자 하는 사람들은 허깅페이스 도움말을 보고 이 내용을 모두 정리하기 쉽지 않은 것이 사실이어서 이글이 꽤 도움이 되리라 생각한다.\n\n\n\n\n시작하기전에 필요한 라이브러리를 설치한다. 본인 컴퓨터에 이미 관련 라이브러리가 설치되어 있다면 설치하지 않아도 된다.\n먼저 허깅페이스의 트랜스포머스 라이브러리를 설치한다.\n\n!pip install transformers\n\n그 다음은 데이터 셋을 다운받는기 위해 다음 명령을 실행해서 허깅페이스 Datasets 라이브러리를 설치한다.\n\n!pip install datasets\n\n그리고 T5 모델의 토크나이저가 sentencepiece를 사용하므로 다음을 실행해서 설치한다.\n\n!pip install sentencepiece\n\n또 모델 평가를 위해 허깅페이스 evaluate 라이브러리와 BLEU 점수를 계산하기위해 sacrebleu를 설치한다.\n\n!pip install evaluate\n\n\n!pip install sacrebleu\n\n\n\n\n모두 설치가 완료되었다면 데이터 셋을 다운받아야 한다. 먼저 허깅페이스 사이트에 접속해서 상단 메뉴에 Datasets를 클릭하고 아래처럼 검색 조건을 맞추면\n\n좌측 작은 메뉴에서 Languages를 선택한다.\nLanguages 하단에 보이는 여러 언어중에 Korean을 선택한다.\n다시 우측 검색 필터 창에 en을 적는다.\n\n데이터 셋 네 개가 보이는데 이 중에서 bongsoo/news_talk_en_ko를 사용하도록 하겠다.\nbongsoo/news_talk_en_ko를 클릭해서 나오는 화면에서 Files and Versions를 클릭하면 tsv 파일이 보이는데 이 파일에는 영어 문장과 한국어 문장이 한줄에 탭 문자로 구분되어 적혀있다. 로컬 디스크이 이 파일을 다운받고 파일을 읽어보면 다음처럼 확인된다.\n\n[노트] 로컬 또는 코랩 런타임에 파일을 다운 받지 않았다면 굳이 다운받을 필요없고 이 셀은 스킵하자. 그냥 데이터 파일 한줄에 영어 문장과 짝이 되는 한국어 문장이 탭문자로 구분되어 있다는 것만 알면 된다. 실제 데이터를 가져올 때는 허깅페이스를 통해 다운 받게 된다.)\n\n\n!head -5 news_talk_en_ko_train_130000.tsv\n\nSkinner's reward is mostly eye-watering.    스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\nEven some problems can be predicted.    심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\nOnly God will exactly know why. 오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\nBusinesses should not overlook China's dispute. 중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\nSlow-beating songs often float over time.   박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n데이터 파일은 아주 단순한 형태인 것을 알 수 있다. 직접 tsv파일을 다운받아서 사용해도 되나 허깅페이스 허브로 부터 바로 다운받아 사용하는 편이 더 편하다. 다음 명령으로 다운받을 수 있다.\n\n# 데이터 셋을 다운받을 함수를 임포트 한다.\nfrom datasets import load_dataset\n\n\n# 좀 전에 알아본 체크포인트를 사용해서 데이터를 받아온다.\nen_ko = load_dataset(\"bongsoo/news_talk_en_ko\")\n\nUsing custom data configuration bongsoo--news_talk_en_ko-e7f00bc8f76f18d5\nFound cached dataset csv (/home/metamath/.cache/huggingface/datasets/bongsoo___csv/bongsoo--news_talk_en_ko-e7f00bc8f76f18d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n\n\n\n\n\n이제 데이터 객체를 확인해보면 DatasetDict라는 것을 알 수 있고 안에 train 키만 있는 것이 확인된다.\n\nen_ko\n\nDatasetDict({\n    train: Dataset({\n        features: [\"Skinner's reward is mostly eye-watering.\", '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.'],\n        num_rows: 1299999\n    })\n})\n\n\ntrain키에 Dataset 객체가 하나 있는데 features가 첫번째 데이터로 되어있고 행수는 1299999개인 것을 보아 데이터 파일에 컬럼명이 적혀있는 헤더라인이 없어서 첫줄을 헤더로 읽은것 같다. 첫줄을 데이터로 다시 집어 넣고 컬럼명은 en, ko로 설정하기 위해 데이터 셋을 pandas로 읽어드린다.\n\nimport pandas as pd\n\n\n# 허깅페이스 데이터셋을 판다스 포맷으로 세팅\nen_ko.set_format(type=\"pandas\")\n\n\n# 'train'키의 모든 행을 DataFrame df에 할당\ndf = en_ko[\"train\"][:]\n\n# 잘 담겼는지 확인한다.\ndf.head()\n\n\n\n\n\n\n\n\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n\n\n0\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n1\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n2\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n3\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n4\nI can't even consider uninsured treatments.\n보험 처리가 안 되는 비급여 시술은 엄두도 못 낸다.\n\n\n\n\n\n\n\n예상처럼 첫 줄이 헤더가 되었으니 이를 수정한 DataFrame을 만든다.\n\nexample_0 = list(df.columns)\nexample_0\n\n[\"Skinner's reward is mostly eye-watering.\",\n '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.']\n\n\n적당히 조작해서 컬럼명이 en, ko가 되게 하고 example_0가 첫 행이 되도록 만든다.\n\nexample_0_df = pd.DataFrame({col: [value] for col, value in zip(('en', 'ko'), example_0)})\n\n\ndf.columns = ('en', 'ko')\n\n\nen_ko_df = pd.concat([example_0_df, df],).reset_index(drop=True)\nen_ko_df.head()\n\n\n\n\n\n\n\n\nen\nko\n\n\n\n\n0\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n1\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n2\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n3\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n4\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n\n\n\n\n\n이렇게 데이터 셋을 DataFrame으로 만들었다. 이제 이 en_ko_df로 부터 다시 허깅페이스 데이터 셋을 생성하자.\n\nfrom datasets import Dataset\n\n\ndataset = Dataset.from_pandas(en_ko_df)\n\n\ndataset\n\nDataset({\n    features: ['en', 'ko'],\n    num_rows: 1300000\n})\n\n\n다시 데이터 셋을 확인해보면 features가 제대로 표시되고 샘플 수도 1300000개 인것을 확인할 수 있다.\n이렇게 만들어진 DataFrame으로 부터 데이터 셋이 잘 초기화되는 것을 확인했으니 en_ko_df를 세조각으로 쪼개서 tsv파일로 저장하자.\n\n# 각 데이터 셋의 샘플수를 정한다.\nnum_train = 1200000\nnum_valid = 90000\nnum_test = 10000\n\n설정된 크기만큼 DataFrame을 자른다.\n\nen_ko_df_train = en_ko_df.iloc[:num_train]\n\n\nen_ko_df_valid = en_ko_df.iloc[num_train:num_train+num_valid]\n\n\nen_ko_df_test = en_ko_df.iloc[-num_test:]\n\n다시 tsv파일로 저장한다.\n\nen_ko_df_train.to_csv(\"train.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_valid.to_csv(\"valid.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_test.to_csv(\"test.tsv\", sep='\\t', index=False)\n\n이렇게 tsv파일 세개로 데이터를 정리했다. 이제 필요할때 이 파일을 읽어 허깅페이스 데이터셋을 만들 수 있다.\n아래처럼 스플릿을 정의한 사전을 load_dataset에 넘기면 된다. 이때 delimiter를 탭 문자로 지정해야 한다.\n\ndata_files = {\"train\": \"train.tsv\", \"valid\": \"valid.tsv\", \"test\": \"test.tsv\"}\n\n\ndataset =  load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n\nUsing custom data configuration default-02a3611b1810efcd\n\n\nDownloading and preparing dataset csv/default to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\nDataset csv downloaded and prepared to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n제대로 로딩되었는지 dataset을 확인해보자.\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 10000\n    })\n})\n\n\nDatasetDict에 train, valid, test 키로 120만 문장, 9만 문장, 1만 문장이 저장된 것을 확인할 수 있다.\n이 데이터 셋에서 개별 샘플에 대한 접근은 [split][feature][row num] 형태로 가능하다.\n\n# train 스플릿에서 영어 3개와 한국어 3개 샘플을 가져온다.\nprint(dataset['train']['en'][:3], dataset['train']['ko'][:3])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n그런데 feature와 row num은 순서를 바꿔서 사용할 수 도 있다.\n\nprint(dataset['train'][:3]['en'], dataset['train'][:3]['ko'])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n데이터를 어떻게 조회하는지는 데이터 구성 방식에 따라 조금씩 다르므로 데이터 셋을 보고 몇번 해보면 금방 접근법을 알 수 있다.\n\n\n\n데이터 셋 준비를 마쳤으니 학습할 차례이다. 허깅페이스에서 제공하는 필요 클래스를 임포트 한다.\n먼저 선학습 모델을 사용하기 위한 클래스를 임포트 한다. AutoTokenizer는 선학습된 모델이 사용한 토크나이저를 읽기 위해 필요하며 AutoModelForSeq2SeqLM은 시퀀스 투 스퀀스 방식으로 작동하는 선학습된 모델을 불러 올 때 마지막에 분류기 헤드를 붙여서 모델을 로딩하기 위해 사용한다.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n2023-03-01 16:05:02.191320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-01 16:05:02.266647: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-03-01 16:05:02.281905: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-01 16:05:02.592853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n\n다음은 데이터 콜레이터를 임포트한다. 시쿼스 투 시퀀스 학습 과정은 인코더 입력 시퀀스, 디코더 입력 시퀀스, 디코더 출력 시퀀스를 필요로 하는데 미니배치로 부터 이를 적절히 정리해서 모델에 입력하는 작업이 필요하다. 예를 들면 미니 배치 내에 있는 인코더 입력 시퀀스의 길이를 맞춘다든지 디코더 입력시퀀스를 오른쪽으로 한칸 쉬프트시켜 디코더 출력 시퀀스를 만드는 작업등이 콜레이터에서 일어나는 작업인데 이런 작업을 DataCollatorForSeq2Seq가 자동으로 처리하게 된다.\n\nfrom transformers import DataCollatorForSeq2Seq\n\n그리고 학습에 필요한 클래스를 임포트 한다. 학습에 필요한 설정을 Seq2SeqTrainingArguments에 정의하고 실제 학습은 Seq2SeqTrainer로 하게 된다. Seq2SeqTrainer는 generate()함수를 제공한다.\n\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n허깅페이스 라이브러리로는 마지막으로 데이터 셋을 로딩하는 함수와 번역 결과를 측정할 함수를 로딩한다.\n\nfrom datasets import load_dataset, load_metric\n\n그외 필요한 각종 라이브러리를 임포트 한다.\n\nimport numpy as np\nimport torch\nimport multiprocessing\n\n허깅페이스에서 파이토치 기반 구현을 사용하므로 gpu가 있다면 device를 세팅한다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\n미리 학습된 모델의 체크포인트를 세팅한다. 여기서 사용할 모델은 한국어와 영어에 미리 학습된 KE-T5모델을 사용한다. T5모델은 트랜스포머의 인코더, 디코더 구조를 모두 사용하는 모델로 번역기를 만들 때 사용할 수 있는 모델이다. 아래처럼 모델 체크 포인트와 T5 모델에 입력될 최대 토큰 길이를 설정한다.\n\nmodel_ckpt = \"KETI-AIR/ke-t5-base\"\nmax_token_length = 64\n\n\n\n\n먼저 모델 체크 포인트를 사용하여 KE-T5 모델이 학습할때 함께 사용한 토크나이저를 불러온다. 허깅페이스 트랜스포머스 라이브러리를 사용할 때 가장 핵심이 되며 익숙해지기 쉽지 않은 부분이 이 토크나이저라고 개인적으로 생각한다.\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n\n토크나이저를 로딩할때 sentencepiece가 없다고 에러가 나면 위 제시한 라이브러리가 설치 안된 것이므로 설치하고 다시 시도한다.\n토크나이저를 불러왔으니 현재 사용하는 데이터셋에서 샘플을 가져와 토크나이징해보는 것이 좋을 것이다. 학습 세트에서 10번 샘플을 가지고 실험해보자. 먼저 10번 샘플을 뿌려보고\n\ndataset['train'][10]['en'], dataset['train'][10]['ko']\n\n('Any academic achievement requires constant repetition.',\n '어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.')\n\n\n토크나이저에 각 문장을 입력하고 토큰화된 상태로 돌려 받는다.\n\ntokenized_sample_en = tokenizer(dataset['train'][10]['en'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_en\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenized_sample_ko = tokenizer(dataset['train'][10]['ko'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_ko\n\n{'input_ids': [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n문장에 토큰으로 쪼개지고 각 토큰이 숫자로 변환된 것을 볼 수 있다. 이렇게 숫자화된 토큰을 input_ids로 반환하고 추가로 트랜스포머 인코더, 디코더에 쓰일 패딩 마스크도 함께 attention_mask로 돌려준다. 마스크가 모두 1인 이유는 샘플이 하나밖에 없어서 이다. 샘플 몇개를 더 실험해보면\n\ntokenizer(dataset['train'][:3]['en'], \n          max_length=max_token_length, \n          padding=True, truncation=True)\n\n{'input_ids': [[388, 6809, 2952, 17, 8, 32204, 43, 8023, 6687, 28, 9495, 91, 3, 1], [4014, 322, 3170, 147, 67, 23274, 3, 1, 0, 0, 0, 0, 0, 0], [11783, 4412, 96, 6556, 709, 1632, 3, 1, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}\n\n\n미니배치에 있는 샘플의 최대길이메 맞춰서 패딩되는 모습을 확인할 수 있다. 실제로 어떻게 토큰화 되었는지 확인해보자.\n\npd.DataFrame(\n    [\n        tokenized_sample_en['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_en['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nids\n13941\n10114\n25542\n9361\n20526\n742\n32268\n12520\n3\n1\n\n\ntokens\n▁Any\n▁academic\n▁achievement\n▁requires\n▁constant\n▁re\npet\nition\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\n\npd.DataFrame(\n    [\n        tokenized_sample_ko['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_ko['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nids\n404\n12663\n15\n10775\n2334\n6\n15757\n21\n29819\n1736\n26778\n4342\n15\n1701\n3\n1\n\n\ntokens\n▁어떤\n▁학문\n이\n든지\n▁일정\n의\n▁성취\n를\n▁이루기\n▁위해서는\n▁끊임없는\n▁반복\n이\n▁필요하다\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\nKE-T5를 학습할때 학습된 규칙대로 토큰화가 진행된다. 영어에서 repetition은 re, pet, ition으로 쪼개진 것을 볼 수 있고, 한국어에서 성취를은 성취, 를로 쪼개지고 학문이든지는 학문, 이, 든지로 쪼개진것을 볼 수 있다. 토큰 앞에 _표시는 이 토큰 앞에는 공백이 있어야 한다는 의미다. 그리고 마지막에 엔드 토큰인 &lt;/s&gt;가 항상 붙게 되는 것도 확인할 수 있다.\n이제 앞서 tsv파일로 부터 로딩한 dataset내의 문장을 모두 토크나이저를 사용해서 숫자로 바꾸는 작업을 해야 한다. 즉 문자로된 문장을 숫자로 바꿔 특성화 해야 한다. dataset.map()함수에 각 샘플을 토큰화 하는 함수를 만들어 전달하면 map()이 모든 샘플에 대해 전달받은 함수를 적용하게 되는데 함수는 이렇게 작성하면 된다.\n\ndef convert_examples_to_features(examples):\n    ###########################################################################\n    # with 쓰는 옛날 방식\n    # input_encodings = tokenizer(examples['en'], \n    #                             max_length=max_token_length, truncation=True)\n    \n    # Setup the tokenizer for targets\n    # with tokenizer.as_target_tokenizer():\n    # target_encodings = tokenizer(text_target=examples['ko'], \n    #                             max_length=max_token_length, truncation=True)\n    #\n    #\n    # return {\n    #     \"input_ids\": input_encodings[\"input_ids\"],\n    #     \"attention_mask\": input_encodings[\"attention_mask\"],\n    #     \"labels\": target_encodings[\"input_ids\"]\n    # }\n    \n    # 그런데 이렇게 하면 인풋하고 한번에 처리 가능함.\n    model_inputs = tokenizer(examples['en'],\n                             text_target=examples['ko'], \n                             max_length=max_token_length, truncation=True)\n    \n    return model_inputs\n\nconvert_examples_to_features()가 하고 싶은 일은 dataset에 있는 “어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.”라는 샘플 문장을 [404,12663,15,10775,2334,6,15757,21,29819,1736,26778,4342,15,1701,3,1]라는 정수로 바꾸는 것이다. convert_examples_to_features()가 dataset에 적용될 때 넘겨 받는 examples는 다음과 같이 넘어 온다.\nexamples= {'en':['sent1', 'sent2', ... , 'sent1000'], # 이건 문장 1000개짜리 리스트\n           'ko':['sent1', 'sent2', ... , 'sent1000']}\n기본으로 미니 배치 사이즈는 1000으로 세팅되어 있다.(함수 기본인자는 여기서 확인 가능)\n미니 배치로 넘어온 문장 샘플을 영어 문장과 한국어 문장을 각각 인풋과 타겟으로 토큰화하고 이로 부터 input_ids, attention_mask, labels로 묶어 리턴하는 방식이 예전에 쓰던 방식으로 함수 위쪽에 주석처리 되어 있다. 타겟 문장을 토큰화 할 때 타겟에서 필요로 하는 특수 토큰을 추가하는 경우 이를 처리하기위해 타겟 토큰 토큰화 때는 with tokenizer.as_target_tokenizer():라는 컨텍스트 매니저를 사용했는데 최근 업데이트에서는 그냥 tokenizer에 text_target인자에 타겟 문장을 넣어서 한번에 다 처리할 수 있다. 이렇게 model_inputs을 반환하면 dataset에 있던 각 레코드 마다 en, ko 특성에 추가로 input_ids, attention_mask, labels 특성이 더 추가 되게 된다. 사실 en, ko 특성은 더이상 필요없기 때문에 convert_examples_to_features()를 적용할 때 없애라는 인자를 세팅한다.\n바로 dataset에 함수를 적용해보자. 그냥 해도되나 좀 더 빠르게 하기 위해 num_proc 인자에 스레드 개수를 지정한다.\n\nNUM_CPU = multiprocessing.cpu_count() \nNUM_CPU\n\n20\n\n\n그리고 remove_columns 인자에 기존 특성 이름인 en, ko를 전달해서 기존 특성은 제거하게 한다. 이 특성이 있으면 이후 콜레이터가 샘플들을 미니 배치로 묶을 때 패딩처리를 못하게 된다.\n\ntokenized_datasets = dataset.map(convert_examples_to_features, \n                                 batched=True, \n                                 # 이걸 쓰지 않으면 원 데이터 'en', 'ko'가 남아서\n                                 # 아래서 콜레이터가 패딩을 못해서 에러남\n                                 remove_columns=dataset[\"train\"].column_names,\n                                 num_proc=NUM_CPU) \n\n\n[노트] dataset.map()이 실행되면서 출력되는 출력은 생략됨\n\nconvert_examples_to_features()이 dataset의 모든 샘플에 다 적용되고 나면 tokenized_datasets는 다음처럼 된다.\n\ntokenized_datasets\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 10000\n    })\n})\n\n\n기존에 있던 특성 en, ko는 사라졌고 en은 input_ids와 attention_mask로 ko는 labels로 바뀐것을 확인할 수 있다. 예를 들어 학습 세트에 10번 데이터를 보면 다음처럼 다 숫자라 바뀌게 된것이다.\n\ntokenized_datasets['train'][10]\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'labels': [404,\n  12663,\n  15,\n  10775,\n  2334,\n  6,\n  15757,\n  21,\n  29819,\n  1736,\n  26778,\n  4342,\n  15,\n  1701,\n  3,\n  1]}\n\n\n토크나이저를 써서 숫자로 부터 토큰화 해보면 다음과 같다.\n\nprint( '원 데이터    :', dataset['train'][10]['en'] )\nprint( '처리 후 데이터:', tokenized_datasets['train'][10]['input_ids'] )\nprint( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['input_ids']) )\n\nprint('\\n')\nprint( '원 데이터    :', dataset['train'][10]['ko'] )\nprint( '처리 후 데이터:', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['labels']) )\nprint( '토큰화       :', tokenized_datasets['train'][10]['labels'] )\n\n원 데이터    : Any academic achievement requires constant repetition.\n처리 후 데이터: [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1]\n토큰화       : ['▁Any', '▁academic', '▁achievement', '▁requires', '▁constant', '▁re', 'pet', 'ition', '.', '&lt;/s&gt;']\n\n\n원 데이터    : 어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.\n처리 후 데이터: ['▁어떤', '▁학문', '이', '든지', '▁일정', '의', '▁성취', '를', '▁이루기', '▁위해서는', '▁끊임없는', '▁반복', '이', '▁필요하다', '.', '&lt;/s&gt;']\n토큰화       : [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1]\n\n\n데이터 특성화를 모두 마쳤으므로 이제 모델을 로딩하자. AutoModelForSeq2SeqLM를 사용해서 선학습 모델을 불러오면 선학습된 T5모델 마지막에 파인튜닝할 수 있는 분류 헤드를 붙인 모델을 반환한다.\n\n\n\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n위처럼 모델을 로딩하고 모델 출력 시켜보면 T5 모델 레이어가 매우 길게 출력되는데 제일 마지막 부분에 다음과 같이 분류 헤드가 붙어 있는 것을 확인할 수 있다. 헤드를 보면 모델에서 출력하는 벡터는 768차원이고 이를 단어장 사이즈인 64128로 변환시키고 있는 것을 알 수 있다.\n(lm_head): Linear(in_features=768, out_features=64128, bias=False)\n이렇게 생성된 model은 인코더-디코더 구조를 가지는 트랜스포머이므로 이 모델을 포워딩 하려면 인코더 인풋과 디코더 인풋을 넣어줘야 한다. 모델을 만들고 가장 먼저해야되는 작업은 포워딩 테스트라고 개인적으로 생각한다. 임의의 입력을 넣고 출력이 의도대로 나오는지 확인하는 것이다. 이런 작업은 직접 만든 모델이 아닐 수록 중요한데 이렇게 해야지 모델이 제대로 작동하는지 또 어떤 구조로 되어 있는지 쉽게 이해할 수 있기 때문이다. 포워드 테스트를 하기위해 간단한 영어문장으로 예제를 준비한다.\n\nencoder_inputs = tokenizer(\n    [\"Studies have been shown that owning a dog is good for you\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\ndecoder_targets = tokenizer(\n    [\"개를 키우는 것이 건강에 좋다는 연구 결과가 있습니다.\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\n영어 문장은 인코더의 입력이 되고 한국어 문장은 디코더의 타겟이 된다. 아래처럼 모두 숫자로 변환되어 있다.\n\nprint( encoder_inputs )\nprint( decoder_targets )\n\ntensor([[24611,    84,   166,  8135,    38,   847,    91,    16,  8146,    43,\n           667,    40,   106,     1]], device='cuda:0')\ntensor([[15833, 12236,   179, 16120, 28117,  1007,  3883,   327,     3,     1]],\n       device='cuda:0')\n\n\n이제 디코더 입력을 만들기위해 model._shift_right를 사용해 디코더 출력을 오른쪽으로 쉬프트 시킨다.\n\ndecoder_inputs = model._shift_right(decoder_targets)\n\ndecoder_inputs와 decoder_targets이 어떻게 다른지 비교해보면\n\npd.DataFrame(\n    [\n        tokenizer.convert_ids_to_tokens(decoder_targets[0]),\n        tokenizer.convert_ids_to_tokens(decoder_inputs[0])\n    ],\n    index=('decoder target', 'decoder input')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\ndecoder target\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n&lt;/s&gt;\n\n\ndecoder input\n&lt;pad&gt;\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n\n\n\n\n\n\n\n위처럼 오른쪽으로 쉬프트된 디코더 입력은 &lt;pad&gt; 토큰이 추가되었다. 이렇게 출력으로 쓰이는 문장을 오른쪽으로 쉬프트시켜 티처포싱Teacher forcing을 진행하게 된다. 다음처럼 model에 인코더 입력, 디코더 입력, 디코더 타겟을 입력하고 포워드 시킨다.\n\n# forward pass\noutputs = model(input_ids=encoder_inputs, \n                decoder_input_ids=decoder_inputs, \n                labels=decoder_targets)\n\nmodel의 outputs에는 다음과 같은 키가 있다.\n\noutputs.keys()\n\nodict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])\n\n\n손실함수 값을 다음처럼 확인할 수 있고 grad_fn이 있기 때문에 output.loss를 백워드 시킬 수 있다.\n\noutputs.loss\n\ntensor(87.8185, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n\n\n인코더의 마지막 상태는 (1, 14, 768)이다. 각 숫자는 순서대로 샘플 수, 스탭 수, 모델 사이즈를 나타낸다. 즉 인코더로 들어가는 14개 토큰이 각각 768차원 벡터로 인코딩되었다.\n\noutputs['encoder_last_hidden_state'].shape\n\ntorch.Size([1, 14, 768])\n\n\nlogit은 디코더 입력 토큰 10개에 대한 그 다음 토큰 예측 10개를 담고있다. 샘플 한개에 대해서 10개 토큰에 대해서 64128개 단어에 대한 확률값이 들어 있다.\n\noutputs['logits'].shape\n\ntorch.Size([1, 10, 64128])\n\n\nlogit에 argmax를 씌워서 토큰화시켜보면 다음과 같다.\n\ntokenizer.convert_ids_to_tokens( torch.argmax(outputs['logits'][0], axis=1).cpu().numpy() )\n\n['큐브', '큐브', '▁비일비재', '▁비일비재', '▁베네', '▁비일비재', '▁베네', '▁베네', '큐브', '큐브']\n\n\n마지막 헤더가 학습이 되지 않았기 때문에 적절한 아웃풋이 나오지 않지만 입력과 출력의 텐서 모양을 보면 포워드 패스가 제대로 작동한다는 것을 알 수 있다.\n지금까지 데이터 셋, 토크나이저, 모델에 대해서 알아봤다. 이제 학습을 위해 두 단계가 남았는데 하나는 데이터를 미니배치 형태로 모아 주는 콜레이터collator와 나머지 하나는 모델을 평가할 매트릭이다\n\n\n\n파이토치에서 모델을 학습시키기 위해서 DataLoader를 사용하게 되는데 이 데이터 로더의 역할은 for 루프를 돌면서 데이터 셋으로 부터 샘플을 미니 배치 수만큼 가져오는 것이다. 이때 샘플을 미니 배치 수만큼 무작위로 가져와 어떤 식으로든 각 샘플을 짝맞춤해서 반환해야하는데 크기가 통일된 간단한 이미지 데이터인 경우 특별히 할것이 없지만 서로 크기가 다른 샘플들을 다루는 경우는 반환전 크기 또는 길이를 맞춘다든지 패딩을 한다든지 하는 추가 작업이 필요하게 된다. 이런 작업이 일어나는 곳이 collate_fn으로 지정되는 함수이다.\n시퀀스 투 시퀀스 모델을 학습시킬때 이런 콜레이터 함수가 하는 전형적인 역할은 입력 또는 출력 문자열을 패딩하고 조금 전 모델에서 알아봤듯이 디코더 타겟을 오른쪽으로 한칸 쉬프트 시켜서 디코더 입력으로 만드는 일이다. 앞서 이런 과정을 간단히기 직접 코딩해서 확인했지만 이런 작업을 자동으로 처리해주는 클래스가 DataCollatorForSeq2Seq이다.\n우선 콜레이터를 만들기 위해서는 토크나이저와 모델을 넘겨야 한다.\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n앞서 만들어논 tokenized_datasets에서 샘플 두개를 조회하면 다음처럼 전체 결과는 사전으로 리턴되며 사전의 각 키 아래에 여러 샘플들의 값이 리스트로 들어있게 된다.\n\n# 각 항목아래 샘플들이 리스트 형태로 묶여 반환된다.\ntokenized_datasets[\"train\"][1:3]\n\n{'input_ids': [[4014, 322, 3170, 147, 67, 23274, 3, 1],\n  [11783, 4412, 96, 6556, 709, 1632, 3, 1]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n 'labels': [[6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n  [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]]}\n\n\n콜레이터에 샘플을 넘길 때는 개별 샘플이 사전으로 묶이는 형태가 되어야 되므로 아래처럼 한번 가공하게 된다.\n\n# 콜레이터에는 샘플을 개별 {}로 넘겨야 됨\n[tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n\n[{'input_ids': [4014, 322, 3170, 147, 67, 23274, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1]},\n {'input_ids': [11783, 4412, 96, 6556, 709, 1632, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]}]\n\n\n위에 반환된 결과를 보면 각 샘플이 사전 {}으로 묶이고 샘플 하나에는 input_ids, attention_mask, labels이 존재한다. 각 샘플을 리스트로 묶어서 콜레이터에게 전달하고 반환되는 값을 확인해보자.\n\n# 콜레이터를 돌리면 알아서 패딩하고 쉬프트 시킨다.\nbatch = data_collator(\n    [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n)\n\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n반환된 batch의 키를 확인해보면 decoder_input_ids가 생긴것을 확인할 수 있다.\n\nbatch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\nbatch의 각 키에 어떤 값들이 들어있는지 확인해보자.\n\nbatch\n\n{'input_ids': tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n        [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,    15,\n          1587,     3,     1],\n        [ 9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,  2255,\n             3,     1,  -100]]), 'decoder_input_ids': tensor([[    0,  6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,\n            15,  1587,     3],\n        [    0,  9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,\n          2255,     3,     1]])}\n\n\n출력된 batch를 정리하면 아래처럼 된다.\n{\n    'input_ids': \n        tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n                [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), \n    'attention_mask': \n        tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1, 1, 1, 1]]), \n    'labels': \n        tensor(\n            [[ 6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n             [ 9881,18590,3837,70,4341,1086,677,35,426,2255,3,1,-100]]), \n    'decoder_input_ids': \n        tensor(\n            [[ 0,6842,404,951,5767,15387,    27,   831, 800,  4378, 15,  1587,     3],\n             [ 0,9881,18590,3837,70,4341,1086,677, 35,   426,2255,     3,     1]])\n}\n새로 생긴 decoder_input_ids는 앞에 0()이 붙어 있는 것이 보이고 label에서 끝에 1이 사라져 label이 오른쪽으로 쉬프트된 것임을 알 수 있다. 그리고 또 두번째 샘플 labels에서 마지막에 -100 이 보인다. 이 값은 label이 패딩된 것을 나타내며 손실 함수값을 계산할 때 -100이 있는 위치는 손실을 계산하지 않게 된다. 이렇게 시퀀스 투 시퀀스 모델을 학습하기 위해 필요한 자잘한 작업을 콜레이터가 알아서 자동으로 처리한다.\n\n\n\n마지막으로 학습한 모델을 측정할 매트릭을 준비해야 한다. 번역 모델에서는 주로 BLEU 점수를 사용한다. BLEU 점수는 번역기가 생성한 문장이 레퍼런스(정답이라는 표현을 사용하지 않는 이유는 제대로 된 번역 문장이 오직 하나가 아니기 때문)문장과 얼마나 비슷한지 측정하는 점수라고 생각하면 된다. 단 같은 단어가 반복된다든지 레퍼런스 문장보다 너무 짧은 문장을 생성한다든지 하면 패널티를 부여 한다. 그렇기 때문에 레퍼런스 문장과 길이가 최대한 비슷하고 다양한 단어를 사용하면서 생성된 문장의 단어가 레퍼런스 단어에 많이 보여야 높은 점수를 얻게 된다.\nBLEU를 계산하기 위해 허깅페이스 evaluate 라이브러리와 sacrebleu라이브러리를 제일 처음에 설치했었다.\nsacrebleu 라이브러리는 BLEU 구현체에서 사실상 표준 라이브러리이며 각 모델이 다른 토크나이저를 쓰는 경우 이를 BPE로 통일 시켜 BLEU 점수를 계산한다고 한다. 참고링크\nevaluate라이브러리로 이 sacrebleu를 불러온다.\n\nimport evaluate\n\nmetric = evaluate.load(\"sacrebleu\")\n\n아래와 같은 예제가 있을 때 두 영어 문장을 번역기가 predictions처럼 번역했고 데이터 셋에 두 문장의 레퍼런스 번역이 references처럼 두개씩 있을 때 bleu점수를 계산해보면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 100.00000000000004,\n 'counts': [21, 19, 17, 15],\n 'totals': [21, 19, 17, 15],\n 'precisions': [100.0, 100.0, 100.0, 100.0],\n 'bp': 1.0,\n 'sys_len': 21,\n 'ref_len': 21}\n\n\n첫 예에서는 predictions가 references의 두 문장 중 하나와 완전히 일치하므로 score가 100점이 나왔다. 하지만 약간 다른 식으로 번역을 한다면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"딥러닝 프레임워크가 잘 개발되었기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 25.28116160010779,\n 'counts': [14, 7, 4, 1],\n 'totals': [19, 17, 15, 13],\n 'precisions': [73.6842105263158,\n  41.1764705882353,\n  26.666666666666668,\n  7.6923076923076925],\n 'bp': 0.9000876262522591,\n 'sys_len': 19,\n 'ref_len': 21}\n\n\n점수가 떨어지는 것을 확인할 수 있다. 아래 함수는 모델의 예측과 레이블을 가지고 bleu를 계산하는 헬퍼 함수로 트랜스포머 학습 코스 번역기 매트릭에서 제공하는 코드를 그대로 복사 한 것이다.\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Some simple post-processing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n    \n    return result\n\n\n\n\n학습을 간단히 하기위해 허깅페이스에서 제공하는 Seq2SeqTrainer클래스를 사용한다. 학습 세부 조건은 Seq2SeqTrainingArguments를 사용하여 설정한다. 다음 코드로 학습에 필요한 세부 사항을 설정할 수 있다.\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"chkpt\",\n    learning_rate=0.0005,\n    weight_decay=0.01,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=128,\n    num_train_epochs=1,\n    save_steps=500,\n    save_total_limit=2,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"no\",\n    predict_with_generate=True,\n    fp16=False,\n    gradient_accumulation_steps=2,\n    report_to=\"none\" # Wandb 로그 끄기\n)\n\n이런 저런 자잘한 세팅을 해서 training_args를 만들고 trainer를 생성한다. 지금까지 준비한 model, training_args, tokenized_datasets, data_collator, tokenizer, compute_metrics를 넘기면 된다.\n\ntrainer = Seq2SeqTrainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"valid\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n이제 아래 코드로 드디어 학습을 할 수 있다!\n\n[주의] 코랩에서 실행한다면 per_device_train_batch_size를 12정도로 줄여서 학습해야 하는데 학습 시간만 10시간이 넘게 걸린다.\n\n\ntrainer.train()\n\n\n[노트] 학습 과정에서 출력되는 로그 문장들이 너무 길어서 여기선 생략 되었음\n\n학습이 끝났으면 다음 셀을 실행해서 결과를 저장한다.\n\ntrainer.save_model(\"./results\")\n\n\n\n\n학습과 저장을 성공적으로 마쳤으면 다음 명령으로 모델을 불러올 수 있다.\n\nmodel_dir = \"./results\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\nmodel.cpu();\n\nloading file spiece.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading configuration file ./results/config.json\nModel config T5Config {\n  \"_name_or_path\": \"./results\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"gelu_new\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"gated-gelu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": true,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.24.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64128\n}\n\nloading weights file ./results/pytorch_model.bin\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./results.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n\n\n로딩된 모델을 테스트하기 위해 다음 두 문장을 준비한다.\n\ninput_text = [\n    \"Because deep learning frameworks are well developed, in these days, machine translation system can be built without anyone's help.\",\n    \"This system was made by using HuggingFace's T5 model for a one day\"\n]\n\n모델이 입력하기위해 토크나이저로 토큰화 시킨다.\n\ninputs = tokenizer(input_text, return_tensors=\"pt\", \n                   padding=True, max_length=max_token_length)\n\n/home/metamath/miniconda3/envs/torchflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n\n\ninputs를 확인해보면 input_ids와 attention_mask로 토큰화 된것을 알 수 있다. 첫번째 문장이 더 길기 때문에 두번째 문장의 마스크는 마지막에 0으로 패딩된 것도 확인할 수 있다.\n\ninputs\n\n{'input_ids': tensor([[ 8127,  5859,  5789, 22309,     8,    69,   484,  6560,     4,    20,\n           572,  1258,     4,  9872, 46301,  1076,   147,    67,  3807,  1215,\n          3993,    17,     8,   787,     3,     1],\n        [  465,  1076,    62,   565,    81,  1676,   992, 60049,  1044, 17400,\n            17,     8,   745,   466,  3900,    40,    16,   165,   688,     1,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0]])}\n\n\nmodel.generate()에 입력을 넣고 출력을 생성한다. 이때 빔서치를 하기 위해 num_beams=5로 설정한다.\n\nkoreans = model.generate(\n    **inputs,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\nkoreans.shape\n\ntorch.Size([2, 20])\n\n\n생성된 결과를 디코딩해보면 다음처럼 나쁘지 않게 번역되는 것을 확인할 수 있다.\n\n[ \n    tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(korean)) for korean in koreans\n]\n\n['&lt;pad&gt; 딥러닝 틀이 잘 개발되기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.&lt;/s&gt;',\n '&lt;pad&gt; 이 시스템은 HuggingFace의 T5 모델을 하루 동안 사용해 만든 시스템입니다.&lt;/s&gt;&lt;pad&gt;']\n\n\n마지막으로 테스트 셋에 대해서 몇개 문장을 가져와 번역해보자. 만들어 놓은 tokenized_datasets과 data_collator를 pytorch DataLoader에 그대로 전달해서 데이터 로더를 만들 수 있다.\n\nfrom torch.utils.data import DataLoader\n\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n)\n\n이터레이터로 만들어 한 미니 배치만 가져온다.\n\ntest_dataloader_iter = iter(test_dataloader)\n\n\ntest_batch = next(test_dataloader_iter)\n\n콜레이터에 의해 반환된 미니 배치에는 다음처럼 labels, decoder_input_ids 따위도 가지고 있으므로 모델에 입력하기 위해 input_ids, attention_mask만 남긴다.\n\ntest_batch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\n\ntest_input = { key: test_batch[key] for key in ('input_ids', 'attention_mask') }\n\n\nkoreans = model.generate(\n    **test_input,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\n이제 입력문장, 정답 그리고 생성된 문장을 비교하기 위해 우선 test_batch.labels에 -100으로 인코딩된 부분을 패딩 코튼으로 교체 한다.\n\nlabels =  np.where(test_batch.labels != -100, test_batch.labels, tokenizer.pad_token_id)\n\n\neng_sents = tokenizer.batch_decode(test_batch.input_ids, skip_special_tokens=True)[10:20]\n\n\nreferences = tokenizer.batch_decode(labels, skip_special_tokens=True)[10:20]\n\n\npreds = tokenizer.batch_decode( koreans, skip_special_tokens=True )[10:20]\n\n\nfor s in zip(eng_sents, references, preds):\n    print('English   :', s[0])\n    print('Reference :', s[1])\n    print('Translated:', s[2])\n    print('\\n')\n\nEnglish   : Yes, I'll see you at the parking lot at 3 p.m.\nReference : 네, 오후 3시에 주차장에서 뵙죠.\nTranslated: 네, 오후 3시에 주차장에서 뵙겠습니다.\n\n\nEnglish   : I'm happy to see Jessica Huh take over my role.\nReference : Jessica Huh가 제 역할을 인계받게 되어서 저는 기니다.\nTranslated: 제시카 허가 제 역할을 맡아서 기뻐요.\n\n\nEnglish   : I agree with you that she is qualified for the position.\nReference : 저도 부장님 의견대로 그녀가 이 자리의 적임자라고 생각합니다.\nTranslated: 나는 그녀가 이 직책에 자질이 있다고 당신과 동의합니다.\n\n\nEnglish   : Nick, I was told that your department will be divided into two.\nReference : Nick, 당신 부서가 둘로 나뉜다면서요?\nTranslated: 닉, 당신의 부서가 두 가지로 나뉘게 될 거라고 들었습니다.\n\n\nEnglish   : Yes, all staff involved in online advertising will have their own office space located on the 2nd floor.\nReference : 네, 다음 달 초부터 온라인 광고와 관련된 직원들은 2층에 위치한 개별 사무실 공간을 사용한다고 합니다.\nTranslated: 네, 온라인 광고에 관련된 모든 직원들은 2층에 각자의 사무실 공간이 위치해 있습니다.\n\n\nEnglish   : What happens to the remaining staff?\nReference : 남은 직원들은 어떻게 되나요?\nTranslated: 남은 스태프에게 무슨 일이 일어났나요?\n\n\nEnglish   : The remaining staff will stay in the current space on the 3rd floor, and the division will continue to be called the advertising department.\nReference : 남은 직원들은 3층 현재 자리에 남을 것이고, 부서는 계속 광고 부서로 불린다고 하네요.\nTranslated: 나머지 직원들은 3층 현 공간에 머물게 되며, 이 부서는 계속 광고부서로 불리게 된다.\n\n\nEnglish   : I have a question about the year-end tax adjustment.\nReference : 이번 연말 정산 관련해서 질문이 있어요.\nTranslated: 저는 연말정산에 대해 궁금한 점이 있습니다.\n\n\nEnglish   : Is there any problem?\nReference : 무슨 문제라도 있으신가요?\nTranslated: 혹시 문제가 있나요?\n\n\nEnglish   : I am registering my dependent this time, so do I need to submit any particular documents?\nReference : 제가 이번에 부양가족을 등록하려고 하는데, 따로 제출해야 하는 서류가 있나요?\nTranslated: 이번에 내 국적 등록을 하고 있으니 특별히 서류 제출을 해야 하나요?\n\n\n\n\n두 시간동안 대충 1 에폭만 학습한 것치고는 꽤 그럴듯 하게 번역을 하는 것을 알 수 있다.\n\n\n\n이상으로 영어-한국어 번역기를 처음부터 학습시키는 방법을 정리했다. 이 글을 글쓴이가 의도한대로 빠르게 읽고 이해하기 위해서는 트랜스포머에 대한 이해가 선행되야 한다. 트랜스포머에 대한 자세한 설명은 진짜로 주석달린 트랜스포머를 참고하자. 하지만 트랜스포머나 스퀀스 투 시퀀스 모델에 대해 잘 모른다 하더라도 한국어 번역기를 만들고자 할때 느끼는 막막함은 어느정도 해소할 수 있으리라 생각한다.\n이 글을 읽고 코드를 실행해보고 나서 DataCollatorForSeq2Seq나 Seq2SeqTrainer 를 쓰지 않고 직접 이 부분을 만들어서 모델을 학습 시켜본다면 트랜스포머를 이용한 번역 작업기 만들기를 훨씬 더 상세히 이해할 수 있을 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#본-글의-목적",
    "href": "posts/em/em_algorithm.html#본-글의-목적",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "본 글의 목적",
    "text": "본 글의 목적\n머신러닝을 공부하다 보면 한번은 보게되는 알고리즘이 바로 EM 알고리즘이다. 많은 문헌에서 이 알고리즘을 설명할 때 K-평균 군집화로 시작해서 가우시안 혼합으로 끝을 맺는다. 하지만 두 알고리즘에 대해서 설명하는 것은 EM 알고리즘의 적용 예를 설명하는 것이지 EM 알고리즘을 근본적으로 이해하기 위한 논리를 설명하는 것이 아니어서 해당 내용을 모두 읽어봐도 EM 알고리즘이 도대체 무엇인지 감을 잡기 힘든 경우가 대부분이다.\nEM 알고리즘에 대해서 자세한 설명이 부족하게 된 원인은 개인적인 견해지만 EM 알고리즘이 확률과 통계를 기반으로 하는 알고리즘이기 때문이라 생각한다. 확률과 통계는 알아야 할 내용도 많고 매우 추상적이기 때문에 (적어도 나에게는) 기본적으로 쉽게 접근할 수 없는 문제가 있다. 그런데 그런 내용들이 복잡하게 얽혀 있다면 지면의 한계 또는 난이도의 제약으로 충분한 설명을 하지 못하는 것이 어쩌면 당연할 수도 있다는 생각이 든다.\n이는 관련 분야 전공자들도 어느정도 인정하는 부분인데(https://bayestour.github.io/blog/2019/06/23/EM_algorithm.html) 이런 어려움은 정식으로 출판된 문헌에서도 확인할 수 있다. “The Elements of Statistical Learning”을 예로 들면 EM 알고리즘을 설명하는 8.5절에 뭉크의 절규 아이콘이 붙어 있다.\n확률, 통계에 대한 초보적 지식을 가진 공대생이 참고할 만한 좋은 책은 “패턴인식과 머신러닝”(이하 PRML로 표기)인데 9장 전체를 할애하여 EM 알고리즘을 설명하고 있다. 그런데 이 교재 역시 내용을 전개하는 순서가 좋지 못해서 전체적인 맥락을 이해하기 매우 힘들다.\n이런 이유로 이 글은 PRML의 설명을 재구성하여 가능한 쉬운 예와 코드를 곁들여 EM 알고리즘을 이해하는 것을 목적으로 한다. 그렇기 때문에 수식 번호와 기호법은 PRML과 동일하게 구성하였다. 수식 번호에 (x.x)형식은 PRML 수식을 그대로 사용한 것이다. 혹시나 이 글을 읽고 PRML을 다시 읽을 때 혼란을 최소화 하기 위해서이다.\n가급적 쉽게 설명하려고 많은 고민을 하였으나 기본적으로 이 글을 읽기 위한 선수 지식이 있음을 피할 수는 없었다. 이 글을 읽기 위한 선수 지식은 다음과 같다.\n\n이항분포\n가능도 함수\n경사 하강법\n최대 가능도 추정MLE:Maximum Likelihood Estimation\n라그랑지 승수와 간단한 제약 최적화\npython 문법과 scipy.optimize.minimize() 함수 사용법\n\n(선수 지식이 이 정도인데 이 글 정말 쉽게 이해할 수 있는 글 맞는건지?? ;;;)"
  },
  {
    "objectID": "posts/em/em_algorithm.html#기호",
    "href": "posts/em/em_algorithm.html#기호",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "기호",
    "text": "기호\n다음에 이 글 전반에 걸쳐 사용하는 기호를 정리하였다.\n\n\\(x\\), \\(\\mathbf{x}\\) : \\(D\\)차원 벡터인 데이터, 스칼라인 경우 \\(D=1\\)\n\\(N\\) : 데이터 \\(\\mathbf{x}\\)의 개수\n\\(N_k\\) : \\(k\\)번째 분포에서 샘플링된 데이터 개수\n\\(\\mathbf{X}\\) : 데이터 \\(\\mathbf{x}_n\\)이 행인 행렬. 차원은 (N,D)\n\\(K\\) : 데이터를 샘플링한 분포의 개수\n\\(\\mathbf{z}\\) : \\(K\\)차원 벡터인 잠재변수, 이 잠재변수는 원핫인코딩된 다항변수이다.\n\\(\\mathbf{Z}\\) : 잠재변수 \\(\\mathbf{z}_n\\)이 행인 행렬. 차원은 (N,K)\n\\(n_t\\) : 이항분포에서 시도 횟수"
  },
  {
    "objectID": "posts/em/em_algorithm.html#문제-설정",
    "href": "posts/em/em_algorithm.html#문제-설정",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "문제 설정",
    "text": "문제 설정\n본격적인 설명을 하기 앞서 EM알고리즘을 개략적으로 설명한 다음 글을 읽어보길 추천한다.\n\n“박준석, 2019, EM 알고리즘 이해 및 구현하기”(https://bayestour.github.io/blog/2019/06/23/EM_algorithm.html)\n\n간단하게 문제를 설정하고 매우 직관적으로 EM알고리즘을 설명하는 좋은 글이다. EM 알고리즘에 대해 어느 정도 이해를 하고 있는 것 같은데 깔끔하게 정리가 안되는 느낌을 가지고 있다면 꼭 한번 읽어보길 추천한다.\n본 글에서는 윗 글에서 다루고 있는 똑같은 사례에 좀 더 자세한 설명을 추가하는 것으로 논의를 시작하고자 한다. 박준석(2019)의 원문격에 해당하는 논문(https://www.nature.com/articles/nbt1406?proof=true) 에서는 베르누이 확률분포를 따르는 동전을 한 세트에 열번씩 다섯 세트 던지는 상황을 이야기 하고 있다. 이것을 박준석(2019)에서는 \\(n_t=10\\)인 이항분포에서 다섯 번 샘플링하는 방식으로 이야기하고 있다. 본 글에서도 후자를 기준으로 하며 그 문제는 다음과 같다.\n\n\\(\\text{Bin}(x \\mid n_t =10, \\mu_1)\\)과 \\(\\text{Bin}(x \\mid n_t=10, \\mu_2)\\)인 이항분포 두 개가 있다. 두 분포로 부터 독립적으로 다섯 번 샘플링을 하는데 (10, 4, 3, 7, 8)처럼 샘플링이 되었다. 이 정보를 가지고 \\(\\mu_1\\), \\(\\mu_2\\)를 추정하시오.\n\n이 상황을 코드로 구현하면 다음과 같다.\n이 글은 다음 링크를 통해 colab에서 직접 실행하면서 읽을 수 있다.\n\n\n# 글 전체에서 필요한 모듈을 임포트한다.\nimport numpy as np\nimport itertools\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# 구글 코랩에서 그래프에 LaTeX를 원활히 쓰기 위한 설정으로 코랩이 아니면 실행 안함\nmatplotlib.rc('text', usetex=True)\nmatplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng &gt; /dev/null\n\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nExtracting templates from packages: 100%\n\n\n\n# 두 이항분포의 알지 못하는 파라미터\nMU_1, MU_2 = 0.8, 0.45\n\n# 이항분포에서 시행횟수 n_t\nn_t = 10\n\n\n# 샘플링\nnp.random.seed(1)\n\n# 적당히 샘플링하고\nx_from_p1 = np.random.binomial(n_t, MU_1, 3)\nx_from_p2 = np.random.binomial(n_t, MU_2, 2)\n\n# 섞어서 X를 만든다.\nX = np.concatenate((x_from_p1, x_from_p2))\nnp.random.shuffle(X)\n\nprint(X)\n\n[10  4  3  7  8]\n\n\n여기서 우리가 하고자 하는 것은 데이터 (10, 4, 3, 7, 8)을 이용하여 두 분포의 파라미터 \\(\\mu_1\\)과 \\(\\mu_2\\)를 추정하는 것이다. 위 코드에서 \\(\\mu_1=0.8\\), \\(\\mu_2=0.45\\)로 둔 것이 확인되지만 원래 이 숫자는 우리가 추정해야 하는 것이다.\n만약 샘플 다섯 개가 분포 하나로 부터 나온 간단한 경우라면 분포의 파라미터를 추정하기 위해 데이터에 대한 최대 가능도 추정을 하면 된다. 하지만 샘플링하는 분포는 두 개이며 어느 분포에서 어떤 데이터가 샘플링되었는지 모르는 상황이다. 따라서 데이터에 대한 가능도 함숫값을 계산할 수 가 없다. 각 데이터가 어느 분포에서 샘플링되었는지 알고 있다면 각 분표별로 데이터를 나누고 각각 최대 가능도 추정을 하면 될것이다. 결국 위 상황에 대해서 모든 정보를 다 알고 있다고 말할 수 있으려면 샘플링된 숫자 다섯 개가 어느 분포에서 샘플링되었는지도 알아야 한다.\n샘플링된 데이터를 \\(x_n\\)으로 쓰기로 하자. 그리고 \\(x_n\\)이 어느 분포에서 생성되었는지를 나타내는 카테고리 변수를 \\(\\mathbf{z}_n\\)으로 쓰기로 하자. 앞서 말한 것처럼 모든 정보를 다 알고 있다고 하려면 \\(x_1\\), \\(x_2\\), \\(x_3\\), \\(x_4\\), \\(x_5\\)와 이에 해당하는 \\(\\mathbf{z}_1\\), \\(\\mathbf{z}_2\\), \\(\\mathbf{z}_3\\), \\(\\mathbf{z}_4\\), \\(\\mathbf{z}_5\\)도 모두 알아야 한다. 우리가 가진 데이터가 1번 분포, 2번 분포, 2번 분포, 1번 분포, 1번 분포에서 생성되었다면 \\(\\mathbf{z}_n\\)은 각각 다음과 같을 것이다.\n\\[\n\\mathbf{z}_1 = (1, 0)^\\text{T} \\\\\n\\mathbf{z}_2 = (0, 1)^\\text{T} \\\\\n\\mathbf{z}_3 = (0, 1)^\\text{T} \\\\\n\\mathbf{z}_4 = (1, 0)^\\text{T} \\\\\n\\mathbf{z}_5 = (1, 0)^\\text{T}\n\\]\n이제 \\(x_n\\), \\(\\mathbf{z}_n\\)을 모두 모아 행렬 \\(\\mathbf{X}\\), \\(\\mathbf{Z}\\)로 표기하자. 행렬 \\(\\mathbf{X}\\)에서 한 행은 \\(x_n\\)인데 만약 샘플링되는 데이터가 벡터라면 \\(\\mathbf{x}_n^{\\text{T}}\\)가 될 것이다. 행렬 \\(\\mathbf{Z}\\)에서 한 행은 \\(\\mathbf{z}_n^{\\text{T}}\\)이다. 행렬 \\(\\mathbf{X}\\)와 \\(\\mathbf{Z}\\)는 다음과 같게 된다.\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5\n\\end{bmatrix}= \\begin{bmatrix}\n10\\\\ 4\\\\ 3\\\\ 7\\\\ 8\n\\end{bmatrix} \\qquad\n\\mathbf{Z} = \\begin{bmatrix}\n\\mathbf{z}_1^{\\text{T}} \\\\\n\\mathbf{z}_2^{\\text{T}} \\\\\n\\mathbf{z}_3^{\\text{T}} \\\\\n\\mathbf{z}_4^{\\text{T}} \\\\\n\\mathbf{z}_5^{\\text{T}}\n\\end{bmatrix}=\\begin{bmatrix}\n1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 0\n\\end{bmatrix}\n\\]\n간단하게 아래 코드로 행렬 \\(\\mathbf{X}\\), \\(\\mathbf{Z}\\)를 만들어 두자.\n\n# X: (N,D), (5,1)\nX = X.reshape(-1,1)\nprint(X)\n\n# Z: (N,K), (5,2)\nZ = np.array([[1,0],[0,1],[0,1],[1,0],[1,0]])\nprint(Z)\n\n# 노트북 전체에 사용될 전역 변수 설정\nN, D = X.shape\nK = 2\n\n[[10]\n [ 4]\n [ 3]\n [ 7]\n [ 8]]\n[[1 0]\n [0 1]\n [0 1]\n [1 0]\n [1 0]]\n\n\n이렇게 두 행렬 \\(\\{\\mathbf{X}, \\mathbf{Z}\\}\\)가 모두 주어지는 데이터를 완전 데이터 세트complete data set라 한다. 완전 데이터 세트일 때 최대 가능도 추정을 실제로 해보자."
  },
  {
    "objectID": "posts/em/em_algorithm.html#완전-데이터-세트에-대한-최대-가능도-추정",
    "href": "posts/em/em_algorithm.html#완전-데이터-세트에-대한-최대-가능도-추정",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "완전 데이터 세트에 대한 최대 가능도 추정",
    "text": "완전 데이터 세트에 대한 최대 가능도 추정\n\n가능도 함수\n문제 설정에서 주어진 문제는 확률분포가 2개인 경우지만 \\(K\\)개로 가정하고 이야기하자. 주어진 \\(K\\)개 분포중 특정 \\(k\\)번째 분포가 선택될 확률을 \\(\\pi_k\\)로 표시하자. 그러면 어떤 샘플 \\(x\\)에 대한 확률질량함수는 다음처럼 주어진 확률분포함수를 선형조합하여 얻을 수 있을 것이다. 이때 선형조합 계수는 \\(\\pi_k\\)가 될 것이다.\n\\[\np(x)  = \\sum_{k=1}^K \\pi_k \\text{Bin}(x \\mid n_t, \\mu_k) \\tag{9.7}\n\\]\n직관적으로 식(9.7)이 맞을 것 같지만 정말 그렇게 되는지는 불완전 데이터 세트에 대한 가능도 함수를 구할 때 다시 정식으로 유도해보자.\n앞서 살펴봤듯이 \\(x\\)가 어떤 분포에서 샘플링 되었는지를 나타내는 잠재변수latent variable \\(\\mathbf{z}\\)는 \\(K\\)차원 멀티누이multinoulli 변수이다. 따라서 변수 \\(\\mathbf{z}\\)의 확률질량함수는 식(9.10)처럼 쓸 수 있다.\n\\[\np(\\mathbf{z}) = \\prod_{k=1}^{K} \\pi_k^{z_k} \\tag{9.10}\n\\]\n\\(\\mathbf{z}\\)가 주어졌다면 샘플 \\(x\\)가 어느 분포를 따르는지 알 수 있으므로 해당 샘플의 확률분포 함수는 그 \\(\\mathbf{z}\\)가 가리키는 분포의 \\(\\text{Bin}(x \\mid n_t, \\mu_k)\\)가 된다. 다시말해 \\(\\mathbf{z}\\)가 주어진 조건하에서 \\(x\\)의 확률분포 함수는 식(9.11)처럼 결정되게 된다.\n\\[\np(x \\mid \\mathbf{z}) = \\prod_{k=1}^K \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} \\tag{9.11}\n\\]\n\\(\\mathbf{z}\\)에서 \\(z_k\\)는 오직 하나만 1이고 나머지는 0이므로 잘 생각해보면 식(9.11)이 타당함을 알 수 있다. 이제 \\(x\\)와 \\(\\mathbf{z}\\)의 결합확률분포를 생각하자.\n\\[\np(x,\\mathbf{z}) = p(x \\mid \\mathbf{z})p(\\mathbf{z}) = \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n파라미터를 \\(\\boldsymbol{\\theta}=(\\pi_1, \\pi_2, ..., \\pi_k, \\mu_1, \\mu_2, ..., \\mu_k)^{\\text{T}}\\)로 쓰면 완전 데이터 세트의 \\(\\boldsymbol{\\theta}\\)에 대한 가능도 함수는 다음과 같다.\n\\[\np(x,\\mathbf{z} \\mid \\boldsymbol{\\theta}) = \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n모든 데이터를 고려하기 위해 데이터에 대한 인덱스 \\(n\\)을 도입하고 독립성 가정하에서 가능도를 구하기 위해 모두 곱해주자.\n\\[\np(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \\text{Bin}(x_n \\mid n_t, \\mu_k)^{z_{nk}}\n\\]\n이제 가능도 함수에 로그를 적용하면 최종적으로 완전 데이터 세트에서 로그 가능도 함수가 구해진다.\n\\[\n\\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) = \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\tag{9.36}\n\\]\n식(9.36)에서 \\(z_{nk}\\)를 모두 알고 있으므로 로그 가능도함수의 함숫값을 실제로 구할 수 있고 이를 이용하여 수지척으로 식(9.36)을 최적화 할 수 있다.\n\n\n수치적 방법\n가능도 함수를 구했으므로 이 가능도 함수를 기울기 하강법을 이용하여 직접 최적화 시켜볼 수 있다.\n\nfrom scipy.stats import binom\nfrom scipy.optimize import minimize, Bounds\n\n식(9.36)을 목적함수로 설정하고 나머지 최적화 과정은 사이파이 최적화 함수를 쓰기로 하자. 위처럼 필요한 모듈을 임포트 한다. 결정해야하는 변수는 \\(\\mu_1\\), \\(\\mu_2\\), \\(\\pi_1\\), \\(\\pi_2\\)이고 초기값은 적당히 초기화 한다.\n\n# 초기 파라미터를 주어진 데이터 중에서 아무거나 골라 만든다.\nnp.random.seed(34)\n\nmu_0 = X[np.random.choice(N, K)] / n_t\n\n# pi는 동일하게 설정한다.\npi_0 = np.array([0.5, 0.5])\n\nprint(mu_0)\nprint(pi_0)\n\n[[0.4]\n [0.3]]\n[0.5 0.5]\n\n\n이제 식(9.36)을 그대로 코딩한다.\n\ndef loglikelihood_XZ_(mu, pi, X, Z):\n    \"\"\"\n    eq(9.36)\n    mu     : (K,D)\n    pi     : (K,)\n    X      : (N,D) global variable\n    Z      : (N,K)\n    -----------------------------------------\n    N,D,K : gloval variables\n    \"\"\"\n    # N, D, K = X.shape[0], X.shape[1], Z.shape[1]\n\n    sigma_n = 0.0\n\n    for n in range(N):\n        sigma_k = 0.0\n\n        for k in range(K):\n            loglikelihood_x = np.log(binom.pmf(X[n,0], n_t, mu[k,0])+1.0e-8)\n            sigma_k += Z[n,k] * (np.log(pi[k]+1.0e-8) + loglikelihood_x)\n\n        sigma_n += sigma_k\n\n    return sigma_n\n\nscipy.optimize.minimize()함수를 사용하기 위해서는 최적화 변수가 1차원 벡터 형식으로 전달되어야 한다. 위 loglikelihood_XZ_()함수는 \\(\\mu_k\\)와 \\(\\pi_k\\)를 (K,D), (K,) 형태로 전달받으므로 minimize()함수에 바로 사용할 수 없다. 그래서 \\(\\mu_k\\)와 \\(\\pi_k\\) 한 줄로 펴서 벡터 형태로 만든 다음 전달할 래퍼함수를 하나 더 만든다.\n\ndef loglikelihood_XZ(theta, X, Z):\n    \"\"\"\n    theta[:K*D] : mu, (K,D)\n    theta[K*D:] : pi, (K,)\n    X           : (N,D)\n    Z           : (N,K)\n    -----------------------------------\n    N,D,K : gloval variables\n    \"\"\"\n\n    # N, D, K = X.shape[0], X.shape[1], Z.shape[1]\n\n    mu = theta[:K*D].reshape(K,D)\n    pi = theta[-K:]\n\n    return loglikelihood_XZ_(mu, pi, X, Z)\n\n위 함수는 모든 \\(\\mu_k\\)와 \\(\\pi_k\\)를 담은 1차원 벡터 theta를 전달 받는다. 그 후 적당히 theta를 분리하고 loglikehood_XZ_()함수에 전달하고 있다.\n앞서 정의한 두 함수가 같은 값을 계산하는지 확인해본다. 같은 값이 나오면 최종적으로 음수를 곱해서 NLL(Negative Log Likelihood)로 만든다. 사이파이에서 제공하는 minimize()함수는 최소화를 수행하기 때문이다.\n\n# 래퍼함수와 원함수 결과가 같게 나오는지 확인\ntheta = np.hstack((mu_0.flatten(),  pi_0))\nprint(loglikelihood_XZ_(mu_0, pi_0, X, Z))\nprint(loglikelihood_XZ(theta, X, Z))\n\n# 로그 가능도 함수의 최대화를 마이너스 로그 가능도함수의 최소화로 바꾸기 위해\n# 보조 함수 정의\ndef negative_loglikelihood_XZ(theta, X, Z):\n    return -loglikelihood_XZ(theta, X, Z)\n\n-23.26286598620995\n-23.26286598620995\n\n\n이제 각 변수에 대해서 적당히 바운드 제약조건을 설정한다. 각 파라미터에 대해 \\(0 \\le \\pi_k, \\mu_k \\le 1\\)가 보장되어야 할것이다.\n\n# 바운드 제약조건\nbounds = Bounds((0., 0.,   # mu_k의 하한\n                 0., 0),   # pi_k의 하한\n                (1., 1.,   # mu_k의 상한\n                 1., 1.))  # pi_k의 상한\n\n\\(\\pi_k\\)는 멀티누이 변수 \\(\\mathbf{z}\\)의 파라미터이기 때문에 모두 더해서 1이 되어야 하므로 다음처럼 제약조건을 추가한다.\n\\[\n\\sum_{k=1}^K \\pi_k = 1\n\\]\n\n# 등호제약조건, p(z)에 대한 제약조건 다 더해서 1\n# sum pi = 1\ndef constraint(theta):\n    pi = theta[-K:]\n    return pi.sum() - 1.\n\ncons   = ( {'type': 'eq',   'fun': constraint   }, )\n\n이제 목적함수와 제약조건을 minimize()함수에 넘기면 최적화 과정이 수행된다.\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\nprint('Init. input:', x)\n\nres = minimize(negative_loglikelihood_XZ, x, args=(X, Z), method='slsqp',\n               bounds=bounds, constraints=cons,\n               options={'iprint': 2, 'disp': True})\n\nprint(res)\n\nprint(\"sum pi_k:\",res.x[2:].sum())\n\nInit. input: [0.4 0.3 0.5 0.5]\n  NIT    FC           OBJFUN            GNORM\n    1     6     2.326287E+01     5.484928E+01\n    2    13     1.895467E+01     4.216554E+01\n    3    20     1.694546E+01     3.614093E+01\n    4    27     1.525500E+01     3.100563E+01\n    5    34     1.433698E+01     2.762727E+01\n    6    41     1.327568E+01     2.376365E+01\n    7    47     1.170940E+01     2.275331E+01\n    8    53     1.132655E+01     1.118621E+01\n    9    59     1.112874E+01     7.772592E+00\n   10    65     1.110341E+01     7.117206E+00\n   11    71     1.110208E+01     7.069603E+00\n   12    77     1.110207E+01     7.071547E+00\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 11.102073936079918\n            Iterations: 12\n            Function evaluations: 77\n            Gradient evaluations: 12\n     fun: 11.102073936079918\n     jac: array([-3.89838219e-03,  4.10699844e-03, -4.99865413e+00, -5.00201964e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 77\n     nit: 12\n    njev: 12\n  status: 0\n success: True\n       x: array([0.83331527, 0.35004671, 0.60016153, 0.39983847])\nsum pi_k: 1.0\n\n\n몇 회 반복 후 다음처럼 해를 찾게 된다.\nx: array([0.83331527, 0.35004671, 0.60016153, 0.39983847])\n수치적으로 찾은 해는\n\\[\n\\mu_1 = 0.83331527, \\quad \\mu_2=0.35004671, \\quad \\pi_1=0.60016153, \\quad \\pi_2=0.39983847\n\\]\n이며 출력 마지막 줄 sum pi_k: 1.0를 보면 \\(\\pi_k\\)에 대한 제약조건도 잘 지켜지고 있음을 알 수 있다. 사실 이 정도 문제는 가능도 함수가 복잡하지 않아서 직접 미분하여 해석적으로 최대 가능도 해를 구할 수 있다.\n\n\n해석적 방법\n가능도 함수를 직접 미분하여 최적해를 바로 찾아보자. 식(9.36)을 \\(\\mu_j\\)와 \\(\\pi_j\\)로 미분한다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\n&= \\frac{\\partial}{\\partial \\mu_j} \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk}  \\left\\{ \\frac{\\partial}{\\partial \\mu_j} \\ln \\pi_k +  \\frac{\\partial}{\\partial \\mu_j} \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k)  \\right\\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\left[ \\frac{\\partial}{\\partial \\mu_j}  \\left\\{ \\ln \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_k^{x_n} (1-\\mu_k)^{(n_t - x_n)} \\right\\} \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\left[ \\frac{\\partial}{\\partial \\mu_j}  \\left\\{ \\ln \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} + {x_n} \\ln \\mu_k + {(n_t - x_n)} \\ln  (1-\\mu_k) \\right\\} \\right]\n\\end{aligned}\n\\]\n위 과정에서 세 번째 등호는 \\(\\ln \\pi_k\\)가 \\(\\mu_j\\)의 함수가 아니므로 성립한다. 마지막 줄에서 \\(\\{ \\}\\)안을 미분하면 \\(k=j\\)인 경우를 제외하고는 모두 0이 된다. \\(\\{ \\}\\)안 첫째 항은 \\(\\mu_j\\)에 대해서 상수라서 사라지고 둘째항과 셋째항을 미분하면 아래와 같다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) &= \\sum_{n=1}^N z_{nj} \\left( \\frac{x_n}{\\mu_j} - \\frac{n_t - x_n}{1-\\mu_j} \\right) \\\\\n&= \\sum_{n=1}^N z_{nj} \\left( \\frac{x_n - n_t \\mu_j}{\\mu_j (1-\\mu_j)} \\right)\n\\end{aligned}\n\\]\n마지막 식을 0으로 두고 정리한다.\n\\[\n\\sum_{n=1}^N z_{nj} \\left( \\frac{x_n - n_t \\mu_j}{\\mu_j (1-\\mu_j)} \\right)=0\n\\]\n인덱스 \\(n\\)에 관계없는 항을 합산 기호 밖으로 뽑아낸다.\n\\[\n\\frac{1}{\\mu_j (1-\\mu_j)} \\sum_{n=1}^N z_{nj} \\left(x_n - n_t \\mu_j \\right) = 0\n\\]\n그러면 합산 기호에 의한 항이 0이 되어야 하므로\n\\[\n\\sum_{n=1}^N z_{nj} \\left(x_n - n_t \\mu_j \\right) = 0\n\\]\n합산 기호를 분배하고\n\\[\n\\sum_{n=1}^N z_{nj} x_n - \\sum_{n=1}^N z_{nj} n_t \\mu_j = 0\n\\]\n이항한다.\n\\[\nn_t \\mu_j \\sum_{n=1}^N z_{nj} = \\sum_{n=1}^N z_{nj} x_n\n\\]\n좌변에 \\(\\mu_j\\)만 남기고 우변으로 넘기면\n\\[\n\\mu_j = \\frac{\\sum_{n=1}^N z_{nj} x_n }{n_t \\sum_{n=1}^N z_{nj} }\n\\]\n위 식에서 $ {n=1}^N z{nj}=N_j$이므로 최종적으로\n\\[\n\\mu_j= \\frac{\\sum_{n=1}^N z_{nj} x_n}{n_t N_j} \\tag{1}\n\\]\n구해진 식(1)은 \\(j\\)번째 분포를 따르는 샘플들의 평균으로 완전 데이터 세트에 대한 최대 가능도 해가 구해짐을 말해준다.\n이제 \\(\\pi_j\\)에 대해서 미분하여 같은 과정을 반복한다. 단 이 때는 수치적 방법에서와 마찬가지로 \\(\\pi_k\\)를 모두 더해서 1이 되어야 한다는 제약조건을 고려해야한다.\n\\[\n\\sum_{k=1}^K \\pi_k = 1\n\\]\n물론 \\(\\mu_k\\)에도 $ 0 _k $라는 제약조건이 있지만 미분할 때 반영하지 않은 이유(http://www.iro.umontreal.ca/~slacoste/teaching/ift6269/A19/ 에 lecture 4 참고) 는 제약 없이 구해진 해가 제약조건을 만족시키기 때문이다. 식(1)을 보면 구해진 최종해가 \\(\\mu_j \\in [0,1]\\)임을 알 수 있다.\n\\(\\pi_j\\)로 미분하는 과정은 이렇게 제약조건을 반영해야해서 조금 번거롭지만 그리 복잡하진 않기 때문에 직접 해보기로 하자. 라그랑지 승수lagrange multiplier \\(\\lambda\\)를 도입하고 라그랑지안lagrangian을 구성하자.\n\\[\n\\mathcal{L}(\\boldsymbol{\\theta}, \\lambda)= \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\tag{2}\n\\]\n식(2)를 \\(\\pi_j\\)에 대해서 미분하는 과정은 다음과 같다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\pi_j} & \\left\\{ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} \\\\\n&= \\frac{\\partial}{\\partial \\pi_j} \\left[  \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\}  + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K    z_{nk} \\left\\{\\frac{\\partial}{\\partial \\pi_j} \\ln \\pi_k + \\frac{\\partial}{\\partial \\pi_j} \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\right\\}  + \\lambda \\left(  \\sum_{k=1}^K  \\frac{\\partial}{\\partial \\pi_j}(\\pi_i -1) \\right) \\\\\n&= \\sum_{n}^N \\left(  \\frac{z_{nj}}{\\pi_j}\\right) + \\lambda\n\\end{aligned} \\tag{3}\n\\]\n식(2)를 \\(\\lambda\\)에 대해 미분하면 다음과 같다.\n\\[\n\\frac{\\partial}{\\partial \\lambda} \\left\\{ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} =\\sum_{i=1}^K \\pi_i -1 \\tag{4}\n\\]\n이제 식(3),(4)를 모두 0으로 두고 연립방정식을 풀어서 해를 구한다.\n\\[\n\\sum_{n}^N \\left(  \\frac{z_{nj}}{\\pi_j}\\right) + \\lambda = 0 \\tag{5}\n\\]\n\\[\n\\sum_{j=1}^K \\pi_j -1 = 0 \\tag{6}\n\\]\n식(5)으로 부터\n\\[\n\\pi_j = -\\frac{N_j}{\\lambda} \\tag{7}\n\\]\n을 얻고 식(7)을 식(6)에 대입하면 \\(\\lambda = -N\\)을 구할 수 있다. 따라서 최종적으로\n\\[\n\\pi_j = \\frac{N_j}{N} \\tag{8}\n\\]\n완전 데이터 세트에 대한 최대 가능도 추정은 식(1)과 식(8)로 주어진다. 확률분포에 대한 인덱스를 일관적으로 사용하기 위해 인덱스를 \\(k\\)로 바꾸고 다시 적으면 다음과 같다.\n\\[\n\\mu_k= \\frac{\\sum_{n=1}^N z_{nk} x_n}{n_t N_k} \\tag{9}\n\\]\n\\[\n\\pi_k = \\frac{N_k}{N} \\tag{10}\n\\]\n식(9), (10)은 최대 가능도 추정을 통해 구해진 분포의 파라미터 \\(\\mu_k\\)는 \\(k\\)번 째 분포를 따르는 샘플들의 평균이며 혼합계수 \\(\\pi_k\\)는 간단히 분포를 따르는 샘플들의 비가 됨을 알려준다.\n이 결과를 주어진 데이터에 적용해보면 10, 7, 8은 \\(\\text{Bin}(x \\mid n_t = 10, \\mu_1)\\)로 부터 나온 데이터이므로 식(9)에 의해\n\\[\n\\mu_1 = \\frac{10+7+8}{10 \\times 3}=\\frac{25}{30}= 0.833333\n\\]\n4, 3은 \\(\\text{Bin}(x \\mid n_t=10, \\mu_2)\\)로 부터 나온 데이터이므로\n\\[\n\\mu_2 = \\frac{4+3}{10 \\times 2}=\\frac{7}{20}= 0.35\n\\]\n이 된다. 혼합계수에 대해서는 식(10)에 의해 해 \\[\n\\pi_1 = \\frac{3}{5}=0.6, \\qquad \\pi_2 = \\frac{2}{5}=0.4\n\\]\n로 완전 데이터 세트의 최대 가능도 해를 구할 수 있다. 수치적으로 구한 해와 거의 같은 해가 구해지는 것을 확인할 수 있다.\n지금까지 과정을 통해 완전 데이터 세트에 대해서는 가능도 함수를 최대화 시키는 방식으로 비교적 간단하게 파라미터를 추정할 수 있다는 사실을 알았다. 하지만 문제는 우리에게 행렬 \\(\\mathbf{Z}\\)에 대한 정보가 전혀 없다는 것이다. 다시말해 우리에게 주어진 데이터는 불완전 데이터 세트incomplete data set이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#모르는-데이터를-어떻게-할-것인가",
    "href": "posts/em/em_algorithm.html#모르는-데이터를-어떻게-할-것인가",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "모르는 데이터를 어떻게 할 것인가?",
    "text": "모르는 데이터를 어떻게 할 것인가?\n\n평균이란?\n이쯤에서 잠시 머리를 식힐겸 평균에 대해서 이야기해보자. 우리는 주어진 데이터를 정리하는 개념으로 평균을 인식하는 경우가 많다. 예를 들어 10명인 반의 시험 점수 10개를 다 들여다보기 보다 평균점만 보고 그 반의 성적을 대강 짐작하는 식으로 평균을 사용하게 된다. 하지만 평균은 우리가 알고 싶은 숫자를 잘 모를 때 사용할 수도 있다. 비슷한 수준의 학생이 입학한다고 했을 때 열한 번째 학생의 성적을 모르지만 앞서 구해놓은 평균으로 예측해볼 수 있다는 것이다.\n예를 들어보자. 당신은 프로야구팀 감독이며 팀 성적이 좋아서 한국시리즈까지 진출했다. 내일 1차전이 열리는데 우리팀의 승률을 알고 싶다고 하자. 우리팀의 승률은 상대팀의 선발투수에 따라서 달라지는 것이 당연할 것이다. 수석코치에게 물어보니 시즌 전적을 바탕으로 볼 때 1선발이 등판하면 우리팀 승률은 0.45, 2선발이 등판하면 0.6이라고 한다. 모종의 이유로 1차전에 1선발이 등판할지 2선발이 등판할지 예측할 수 없다면 수석코치는 감독에게 1차전 승률을 어떻게 보고해야할까? 대부분 사람들이 당연하다는 듯이 다음처럼 평균을 구할 것이다.\n\\[\n\\frac{0.45+0.6}{2}=0.525 \\tag{11}\n\\]\n위 식은 1선발과 2선발이 등판할 확률을 모르기 때문에 같다고 임의로 결정한 결과이다. 그런데 만약 1선발이 등판할 확률이 80%라면 예측이 좀 달라져야하지 않을까?\n\\[\n0.8 \\times 0.45 + 0.2 \\times 0.6 = 0.48 \\tag{12}\n\\]\n즉 시합 열 번중에 여덟 번은 1선발이 등판하고 두 번은 2선발이 등판한다고 했을 때 평균을 구한 것이다. 만약 상대팀이 선발등판 예고를 2선발로 했다면 더 볼 것도 없이 승률은 0.6이 된다.\n\\[\n0 \\times 0.45 + 1 \\times 0.6 = 0.6 \\tag{13}\n\\]\n지금 우리가 무엇을 하고 있는지 생각해보자.\n상대팀의 선발투수 등판 상태 상태에는 1선발이 나오거나 2선발이 나오거나 두가지 상태가 있다. 다만 이 두 가지 중 어떤 상태로 결정될지는 알 수 없다. 대신 각 상태에 대한 확률을 ‘모두’ 알고 있다면 모든 상태에 대해서 평균을 계산하여 내일 승률을 예측할 수 있는 것이다. 식(11)에서는 각 상태에 50%씩 확률을 할당한 경우, 식(12)는 1선발이 등판하는 상태1에 80%, 2선발이 등판하는 상태2에 20%를 할당한 경우, 식(13)은 상태1에 0%, 상태2에 100%를 할당한 경우에 승률을 계산한 것이다.\n이렇게 알 수 없는 정보가 있는데 그 정보가 가질 수 있는 모든 상태에 대해서 확률을 할당할 수 있다면 평균을 구해서 모르는 부분을 채울 수 있는 것이다.\n\n\n전체적인 전략\n우리는 \\(\\mathbf{Z}\\)가 어떤 모양인지 모른다. 여기서 앞선 논의를 적용해보자.\n\n\\(\\mathbf{Z}\\)를 모르기 때문에 \\(\\mathbf{Z}\\)가 가능한 모든 상태를 생각해보자.\n그리고 그 상태들에 확률을 부여할 수 있다고 가정해보자.\n그러면 모든 상태에 대해서 가능도 함숫값을 구하고 그렇게 구해진 함숫값들을 해당 \\(\\mathbf{Z}\\)에 부여된 확률을 이용해서 평균낼 수 있지 않을까?\n그렇게만 할 수 있다면 그 가능도 함숫값의 평균을 최대화하는 파라미터를 찾을 수 있을 것이다.\n\n마지막 4번 문장에서 평균을 기댓값이란 용어로 바꿔보자.\n\n“그렇게만 할 수 있다면 그 가능도 함숫값의 기댓값을 최대화하는 파라미터를 찾을 수 있을 것이다.”\n\n우리가 알고 싶어하는 기댓값 최대화라는 이슈가 등장한 것이다!\n다시 정리하자. 우리에게 완전 데이터 세트가 주어져 있다면 데이터의 파라미터에 대한 가능도 함숫값을 계산할 수 있고 이를 통해 최대화를 수행할 수 있다. 하지만 불완전 데이터 세트가 주어졌기 때문에 가능도 함숫값을 계산할 수 없다. 대신 모르는 데이터에 대해서 가능한 모든 상태를 상정하고 가능도 함숫값을 구해 그것들의 평균을 계산한다. 이 평균을 최대화 해보자는 것이다.\n현재 설정된 문제에서 가능한 \\(\\mathbf{Z}\\)는 모두 \\(2^5=32\\)가지가 있다. 이제 우리에게 주어진 첫 번째 과제는 존재 가능한 모든 \\(\\mathbf{Z}\\)에 대해서 확률을 부여하는 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#불완전-데이터-세트에-대한-최대-가능도-추정",
    "href": "posts/em/em_algorithm.html#불완전-데이터-세트에-대한-최대-가능도-추정",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "불완전 데이터 세트에 대한 최대 가능도 추정",
    "text": "불완전 데이터 세트에 대한 최대 가능도 추정\n\n가능도 함수\n현재 주어진 데이터 세트가 불완전 데이터 세트라면 \\(\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\\)를 구할 수 없기 때문에 \\(\\mathbf{z}\\)를 주변화 시켜 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접구하는 전략을 써볼 수 있다. 주어진 \\(\\mathbf{X}\\)를 가장 잘 발생시킬것 같은 \\(\\boldsymbol{\\theta}\\)를 찾는 것이 최대 가능도 추정이므로 궁극적으로는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화 시키고 싶은 것이다.\n앞서 직관적인 방법으로 식(9.7)로 \\(p(x)\\)를 정의했었는데 여기서 정식으로 유도해보도록 하자. 우선 식(9.10)은 멀티누이 변수의 확률질량함수이므로 그 자체로 타당하다.\n\\[\np(\\mathbf{z}) = \\prod_{k=1}^{K} \\pi_k^{z_k} \\tag{9.10}\n\\]\n다음으로 어떤 확률분포인지 알려주는 \\(\\mathbf{z}\\)가 주어졌다면 \\(x\\)에 대한 확률분포는 주어진 이항분포 \\(K\\)개중 하나가 되므로 식(9.11)도 어렵지 않게 이해할 수 있다.\n\\[\np(x \\mid \\mathbf{z}) = \\prod_{k=1}^K \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} \\tag{9.11}\n\\]\n확률의 곱법칙에 의해 \\(p(x,\\mathbf{z})=p(\\mathbf{z})p(x \\mid \\mathbf{z})\\)이므로 다음이 성립하고\n\\[\np(x,\\mathbf{z}) = \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n이를 확률의 합법칙에 의해 \\(\\mathbf{z}\\)에 대해서 주변화 하면 \\(p(x)\\)를 얻을 수 있다.\n\\[\np(x) = \\sum_{\\mathbf{z}}  \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k}\n\\]\n위 식에서 \\(\\prod\\)에 의해 곱해지는 항 \\(K\\)개는 \\(z_k\\)가 오직 하나만 1이고 나머지는 다 0인 상태이다. 따라서 한 개 항만 살아남는다. 그런 항 \\(K\\)개를 모든 \\(\\mathbf{z}\\)에 대해서 다 더하고 있으므로 결과적으로는 \\(K\\)개 항만 더해지는 것으로 다음처럼 정리된다.\n\\[\np(x) = \\sum_{\\mathbf{z}}  \\prod_{k=1}^K \\pi_k^{z_k} \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} = \\sum_{k=1}^K \\pi_k \\text{Bin}(x \\mid n_t, \\mu_k) \\tag{9.12}\n\\]\n직관적으로 정의 했던 식(9.7)을 다시 얻을 수 있다. 굳이 힘들게 \\(p(x)\\)를 유도한 이유는 유도 과정에서 \\(\\mathbf{z}\\)를 주변화해야만 하고 그로 인해 최종 식에 \\(\\sum_{k}\\)가 등장한다는 것을 보이기 위함이다. 식(9.12)에서 \\(\\boldsymbol{\\theta}=(\\pi_1, \\pi_2, ..., \\pi_k, \\mu_1, \\mu_2, ..., \\mu_k)^{\\text{T}}\\)로 두면\n\\[\np(x \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^K \\pi_k \\text{Bin}(x \\mid n_t, \\mu_k)\n\\]\n로 쓸 수 있고, 모든 데이터를 고려하기 위해 데이터에 인덱스 \\(n\\)을 도입하면\n\\[\np(x_n \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)\n\\]\n이 된다. \\(\\boldsymbol{\\theta}\\)에 대한 가능도를 구하기 위해 모든 데이터에 대해 곱해준다.\n\\[\np(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\prod_{n=1}^N \\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)\n\\]\n이제 위 식에 로그를 적용하면\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta} )\n= \\sum_{n=1}^N \\ln \\left\\{  \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)  \\right\\} \\tag{9.14}\n\\]\n불완전 데이터 세트에 대한 로그 가능도 함수가 구해진다. 이제 \\(\\mathbf{Z}\\)를 몰라도 데이터에 대한 가능도 함숫값을 구할 수 있다.\n\n\n수치적 방법\n식(9.14)를 최대화 시키기 위해 수치적 수법을 적용해보자. 과정은 완전 데이터 세트에 대한 수치 최적화 과정과 완전히 동일하다.\n\ndef loglikelihood_X_(mu, pi, X):\n    \"\"\"\n    EQ(9.14)\n    mu     : (K,D)\n    pi     : (K,)\n    X      : (N,D)\n    --------------------------------------------\n    N,D,K  : gloval variables\n    \"\"\"\n    # N, D = X.shape[0], X.shape[1]\n\n    sigma_n = 0.0\n\n    for n in range(N):\n        sigma_k = 0.0\n\n        for k in range(K):\n            likelihood_x = binom.pmf(X[n,0], n_t, mu[k,0])\n            sigma_k += pi[k]*likelihood_x\n\n        sigma_n += np.log(sigma_k+1.0e-8)\n\n    return sigma_n\n\ndef loglikelihood_X(theta, X):\n    \"\"\"\n    theta[:K*D] : mu,     (K,D)\n    theta[K*D:] : pi,     (K,)\n    X           : (N,D)\n    \"\"\"\n    # N, D = X.shape[0], X.shape[1]\n\n    mu = theta[:K*D].reshape(K,D)\n    pi = theta[-K:]\n\n    return loglikelihood_X_(mu, pi, X)\n\n\n# 래퍼함수와 원함수 결과가 같게 나오는지 확인\nprint(mu_0, pi_0)\ntheta = np.hstack((mu_0.flatten(),  pi_0))\nprint(\"{:.6f}\".format(loglikelihood_X_(mu_0, pi_0, X)))\nprint(\"{:.6f}\".format(loglikelihood_X(theta, X)))\n\n# 로그 가능도 함수의 최대화를 마이너스 로그 가능도함수의 최소화로 바꾸기 위해\n# 보조 함수 정의\ndef negative_loglikelihood_X(theta, X):\n    return -loglikelihood_X(theta, X)\n\n[[0.4]\n [0.3]] [0.5 0.5]\n-21.484619\n-21.484619\n\n\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\nprint('Init. input:', x)\n\nres = minimize(negative_loglikelihood_X, x, args=(X,), method='SLSQP',\n               bounds=bounds, constraints=cons,\n               options={'iprint': 2, 'disp': True})\n\nprint(res)\n\nprint(\"sum pi_k:\",res.x[2:].sum())\n\nInit. input: [0.4 0.3 0.5 0.5]\n  NIT    FC           OBJFUN            GNORM\n    1     6     2.148462E+01     4.848786E+01\n    2    13     1.577919E+01     2.542786E+01\n    3    20     1.505575E+01     2.600846E+01\n    4    27     1.393468E+01     2.321400E+01\n    5    34     1.327015E+01     2.112075E+01\n    6    41     1.246911E+01     1.746856E+01\n    7    48     1.210598E+01     1.640239E+01\n    8    55     1.129506E+01     1.103584E+01\n    9    62     1.100341E+01     7.409348E+00\n   10    68     1.097328E+01     7.157866E+00\n   11    74     1.096708E+01     7.083495E+00\n   12    80     1.096662E+01     7.070962E+00\n   13    86     1.096662E+01     7.071087E+00\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 10.96661829509231\n            Iterations: 13\n            Function evaluations: 86\n            Gradient evaluations: 13\n     fun: 10.96661829509231\n     jac: array([-2.94947624e-03,  1.06954575e-03, -4.99991703e+00, -5.00010955e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 86\n     nit: 13\n    njev: 13\n  status: 0\n success: True\n       x: array([0.83809726, 0.37573062, 0.57156103, 0.42843897])\nsum pi_k: 1.0\n\n\n구해진 결과는\nx: array([0.83809726, 0.37573062, 0.57156103, 0.42843897])\n이다. 원래 파라미터 변수명으로 적어보면\n\\[\n\\mu_1 = 0.83809726, \\quad \\mu_2=0.37573062, \\quad \\pi_1=0.57156103, \\quad \\pi_2=0.42843897\n\\]\n이다. 완전 데이터 세트에서 결과와 비슷한지만 비교해보자. 첫 행은 완전 데이터 세트에 대한 결과이고 다음 행은 불완전 데이터 세트에 대한 결과이다.\n\n\\(\\mu_1 = 0.83331527, \\quad \\mu_2=0.35004671, \\quad \\pi_1=0.60016153, \\quad \\pi_2=0.39983847\\)\n$ _1 = 0.83809726, _2=0.37573062, _1=0.57156103, _2=0.42843897$\n\n어느정도 유사하게 추정된 것을 확인할 수 있다. 데이터가 더 많았다면 두 결과는 더 비슷해질것이다.\n\n\n해석적 방법\n이제 완전 데이터 세트에서 했던 것처럼 직접 미분하여 해를 찾아보자. 이 과정에서 \\(\\mathbf{z}\\)를 주변화한 것이 어떤 결과를 낳게되는지 확인할 수 있다. 다만 미분 과정에 지저분해서 계산이 꽤 성가시다. 그래서 미분 과정에 별 관심이 없다면 식(9.22)까지 바로 건너 뛰기로 하자.\n먼저 \\(\\mu_j\\)로 미분한다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n&= \\frac{\\partial}{\\partial \\mu_j}\\left[\\sum_{n=1}^N \\ln \\left\\{  \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)  \\right\\} \\right] \\\\\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\frac{\\partial}{\\partial \\mu_j} \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) \\\\\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\frac{\\partial}{\\partial \\mu_j} \\sum_{k=1}^K  \\pi_k \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_k^{x_n} (1-\\mu_k)^{(n_t - x_n)}\n\\end{aligned}\\tag{14}\n\\]\n미분하는 부분만 따로 때서 써보면\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} & \\sum_{k=1}^K  \\pi_k \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix}    \\mu_k^{x_n} (1-\\mu_k)^{(n_t - x_n)} \\\\\n&=   \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{ x_n \\mu_j^{(x_n -1)} (1-\\mu_j)^{(n_t-x_n)} - \\mu_j^{x_n}(n_t - x_n)(1-\\mu_j)^{(n_t - x_n-1)} \\right\\} \\\\[10pt]\n&= \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{ x_n \\mu_j^{x_n} \\mu_j^{-1}(1-\\mu_j)^{(n_t-x_n)}- \\mu_j^{x_n}(n_t - x_n) (1-\\mu_j)^{(n_t - x_n)}(1-\\mu_j)^{-1} \\right\\} \\\\[10pt]\n&= \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{\\frac{x_n \\mu_j^{x_n}(1-\\mu_j)^{(n_t-x_n)}}{\\mu_j} - \\frac{(n_t - x_n) \\mu_j^{x_n} (1-\\mu_j)^{(n_t - x_n)}}{(1-\\mu_j)} \\right\\}\n\\end{aligned} \\tag{15}\n\\]\n편미분 과정에서 \\(j=k\\)인 경우만 남게 되고 합산 기호는 사라진다.\n미분 결과 식(15)를 다시 식(14)에 대입하고 정리하자.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu_j} \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n&=\\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\left\\{\\frac{x_n \\mu_j^{x_n}(1-\\mu_j)^{(n_t-x_n)}}{\\mu_j} - \\frac{(n_t - x_n) \\mu_j^{x_n} (1-\\mu_j)^{(n_t - x_n)}}{(1-\\mu_j)} \\right\\} \\\\[5pt]\n&=\\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j  \\left\\{\\frac{x_n \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_j^{x_n}(1-\\mu_j)^{(n_t-x_n)}}{\\mu_j} - \\frac{(n_t - x_n) \\begin{pmatrix} n_t \\\\ x_n \\end{pmatrix} \\mu_j^{x_n} (1-\\mu_j)^{(n_t - x_n)}}{(1-\\mu_j)} \\right\\} \\\\[5pt]\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j  \\left\\{\\frac{x_n \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\mu_j} - \\frac{(n_t - x_n) \\text{Bin}(x_n \\mid n_t, \\mu_j)}{(1-\\mu_j)} \\right\\} \\\\[5pt]\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\pi_j  \\left\\{ \\frac{x_n \\text{Bin}(x_n \\mid n_t, \\mu_j)-\\mu_j n_t \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\mu_j (1-\\mu_j)} \\right\\}\\\\[5pt]\n&= \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) }   \\left\\{ \\frac{\\pi_j x_n \\text{Bin}(x_n \\mid n_t, \\mu_j)- \\pi_j \\mu_j n_t \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\mu_j (1-\\mu_j)} \\right\\} \\\\[5pt]\n&=  \\sum_{n=1}^N \\frac{1}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) }   \\left\\{ \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j) ( x_n -  \\mu_j n_t) }{\\mu_j (1-\\mu_j)} \\right\\} \\\\[5pt]\n&= \\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   \\frac{  x_n -  \\mu_j n_t }{\\mu_j (1-\\mu_j)}\n\\end{aligned}\n\\]\n마지막 식을 0으로 두고 정리한다.\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   \\frac{  x_n -  \\mu_j n_t }{\\mu_j (1-\\mu_j)} =0\n\\]\n인덱스 \\(n\\)에 관계없는 항을 합산 기호 밖으로 빼고\n\\[\n\\frac{  1 }{\\mu_j (1-\\mu_j)} \\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   (x_n -  \\mu_j n_t) =0\n\\]\n양변에 \\(\\mu_j (1-\\mu_j)\\)를 곱하면\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   (x_n -  \\mu_j n_t) =0\n\\]\n합산 기호를 분배하면\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   x_n - \\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot \\mu_j n_t =0\n\\]\n이므로\n\\[\n\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot \\mu_j n_t =\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   x_n\n\\]\n가 되고 적당히 이항하면\n\\[\n\\mu_j =\\frac{1}{n_t  \\sum_{n=1}^N \\dfrac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) }}\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\cdot   x_n\n\\]\n위 식에서 앞 쪽 분수 분모의 합산항을\n\\[\nN_j=\\sum_{n=1}^N \\frac{\\pi_j\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k) } \\tag{9.18}\n\\] 을 로 두면 최종적으로 다음과 같다.\n\\[\n\\mu_j = \\frac{1}{n_t N_j} \\sum_{n=1}^N \\left( \\frac{ \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j) } \\right) x_n \\tag{9.17}\n\\]\n이제 \\(\\pi_j\\)로 미분할 차례이다. 여기서는 완전 데이터 세트에서처럼 제약조건을 고려하여 라그랑지안을 구성하고 \\(\\pi_j\\)와 라그랑지 승수 \\(\\lambda\\)로 미분한다.\n\\[\n\\mathcal{L}(\\boldsymbol{\\theta}, \\lambda)= \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\tag{16}\n\\]\n식(16)을 \\(\\pi_j\\)에 대해서 미분하는 과정은 다음과 같다.\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\pi_j} & \\left\\{ \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} \\\\\n&= \\frac{\\partial}{\\partial \\pi_j} \\left[  \\sum_{n=1}^N \\ln \\left\\{  \\sum_{k=1}^K  \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)  \\right\\} + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right] \\\\\n&= \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) + \\lambda\n\\end{aligned} \\tag{17}\n\\]\n식(16)를 \\(\\lambda\\)에 대해 미분하면 다음과 같다.\n\\[\n\\frac{\\partial}{\\partial \\lambda} \\left\\{ \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) + \\lambda \\left( \\sum_{i=1}^K \\pi_i -1 \\right) \\right\\} =\\sum_{i=1}^K \\pi_i -1 \\tag{18}\n\\]\n이제 식(17),(18)을 모두 0으로 두고 연립방정식을 풀어서 해를 구한다.\n\\[\n\\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) + \\lambda = 0 \\tag{19}\n\\]\n\\[\n\\sum_{j=1}^K \\pi_j -1 = 0 \\tag{20}\n\\]\n식(19) 양변에 \\(\\pi_j\\)를 곱한다.\n\\[\n\\pi_j \\left[ \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) + \\lambda \\right] = 0\n\\]\n대괄호를 풀고 이항하면\n\\[\n\\pi_j  \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right)  = - \\pi_j  \\lambda\n\\]\n양변을 인덱스 \\(j\\)에 대해 합산해도 등호는 성립한다.\n\\[\n\\sum_{j=1}^K  \\pi_j  \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right)  = - \\sum_{j=1}^K \\pi_j  \\lambda \\tag{21}\n\\]\n식(21) 우변에 식(20)을 적용하면\n\\[\n\\underbrace{\\sum_{j=1}^K   \\sum_{n=1}^N \\left(  \\frac{\\pi_j  \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right)  }_N= -  \\lambda \\tag{22}\n\\]\n식(22)에서 좌변은 \\(N\\)이 된다. 직관적으로 잘 이해가 되지 않으면 잠시 후 다시 알아보도록 하자. 어쨌든 최종적으로\n\\[\n\\lambda = -N \\tag{23}\n\\]\n임을 알 수 있다. 이제 식(23)을 식(19)에 다시 대입하고 정리한다.\n\\[\n\\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) - N  = 0\n\\]\n\\(N\\)을 이항하고 양변에 \\(\\pi_j\\)를 곱하면\n\\[\n\\pi_j \\left\\{ \\sum_{n=1}^N \\left(  \\frac{\\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) \\right\\}   = \\pi_j N\n\\]\n\\(N\\)을 다시 이항하면\n\\[\n\\pi_j = \\frac{ \\sum_{n=1}^N \\left(  \\dfrac{ \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}{\\sum_{k=1}^K \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)} \\right) }{N}\n\\]\n분자를 식(9.18)로 바꿔쓰면\n\\[\n\\pi_j = \\frac{N_j}{N} \\tag{9.22}\n\\]\n를 얻을 수 있다.\n지루한 미분 과정이 마무리되었고 식(9.17), (9.18), (9.22)의 인덱스 \\(j\\)를 \\(k\\)로 바꾸고 같이 정리해보면 다음과 같은 결과를 얻게 된다.\n\\[\n\\begin{aligned}\n& \\mu_k = \\frac{1}{n_t N_k} \\sum_{n=1}^N \\left( \\frac{ \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j) } \\right) x_n  \\\\[10pt]\n& \\pi_k = \\frac{N_k}{N} \\\\[10pt]\n& \\text{where}\\,\\,\\, N_k = \\sum_{n=1}^N \\left( \\frac{ \\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j) } \\right)\n\\end{aligned}\n\\]\n그런데 \\(\\mu_k\\)를 계산하는 결과를 보면 \\(\\mu_k\\)가 계산 과정에서 다시 나타나고 있다. \\(\\mu_k\\)를 구하는데 \\(\\mu_k\\)가 사용되고 있어서 닫힌형식의 해가 아님을 알 수 있다.\n\n반복법\n기껏 힘들게 미분을 해서 해석적으로 해를 찾았지만 해의 형태가 닫힌형식이 아니라는 결과를 확인했다. 왜 이런 결과가 나오게 되었을까? 그 이유는 모르는 데이터 \\(\\mathbf{z}\\)를 주변화해서 없애는 과정 때문에 필연적으로 로그 안에 합산 기호가 나타나기 때문이다. 로그가 지수족 분포함수에 직접 작용하게 되면 지수함수를 상쇄시켜 계산이 간단해지는 장점이 있는데 여기서는 합산 기호 때문에 그런 순기능이 발생하지 않았던 것이다. 그래서 미분 과정도 매우 복잡하며 얻게된 해도 쓸모없어 보이는 형태를 띄고 있는 것이다. 결국 이런 식으로 해를 찾을 수 없다는 결론에 이르게 되는데 여기서 좀 과감한 방법을 시도해보자.\n\\[\n\\begin{aligned}\n& \\color{#318CE7}{\\mu_k} \\color{black} = \\frac{1}{n_t N_k} \\sum_{n=1}^N \\left( \\frac{ \\color{#E52B50}{\\pi_k} \\color{black}{\\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_k}\\color{black}{)}}{\\sum_{j=1}^K \\color{#E52B50}{\\pi_j} \\color{black}{ \\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_j} \\color{black}{)} } \\right) x_n \\\\[10pt]\n& \\color{#318CE7}{ \\pi_k } \\color{black}{ = \\frac{N_k}{N}} \\\\[10pt]\n& \\text{where}\\,\\,\\, N_k = \\sum_{n=1}^N \\left( \\frac{ \\color{#E52B50}{\\pi_k} \\color{black}{ \\text{Bin}(x_n \\mid n_t,} \\color{#E52B50}{\\mu_k} \\color{black}{)}}{\\sum_{j=1}^K \\color{#E52B50}{\\pi_j} \\color{black}{ \\text{Bin}(x_n \\mid n_t,} \\color{#E52B50}{\\mu_j} \\color{black}{)} } \\right)\n\\end{aligned}\n\\]\n식(9.17), (9.18), (9.22)에서 적당히 파라미터에 색을 입혔다. 빨간색 파라미터들은 현재 설정된 파라미터라고 가정하고 이것을 통해 파란색 새로운 파리미터를 계산한다고 생각해보자. 그렇게 계산된 새로운 파라미터를 다시 빨간색에 대입하고 파란색 파라미터를 구해내는 식으로 진행하는 것이다.\n하지만 이런 방식은 다분히 인위적인 것이어서 반복법이 해를 점진적으로 개선해줄 것이라는 보장이 없다. 현재까지는 일종의 궁여지책으로 보여지는데 일단 실험부터 해보도록 하자.\n위 식에서 괄호로 묶인 부분을 간단히 \\(\\gamma_{nk}\\)로 다시 적고 실험해보자.\n\\[\n\\gamma_{nk}=\\frac{ \\color{#E52B50}{\\pi_k} \\color{black}{\\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_k}\\color{black}{)}}{\\sum_{j=1}^K \\color{#E52B50}{\\pi_j} \\color{black}{ \\text{Bin}(x_n \\mid n_t, }\\color{#E52B50}{\\mu_j} \\color{black}{)} }   \\tag{24}\n\\]\n\nmu = mu_0\npi = pi_0\n\n# 설명처럼 스무번 정도 반복해본다.\nfor i in range(20) :\n    # E-step\n    Gamma = np.array([ pi[k]*binom.pmf(X, n_t, mu[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\n    Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n    # M-step\n    Nk = Gamma.sum(axis=0)\n    mu = ((Gamma * X).sum(axis=0) / (Nk*n_t)).reshape(-1,1)\n    pi = Nk / N\n\nprint(\"mu:\")\nprint(mu)\nprint(\"pi: \",pi)\n\nmu:\n[[0.83811241]\n [0.3757192 ]]\npi:  [0.57154992 0.42845008]\n\n\n놀랍게도 수치적으로 구한 최적해와 거의 동일한 해로 수렴하는 것을 확인할 수 있다. 왜 이렇게 되는지 아직 명확하게 이해할 수는 없지만 식(9.17)을 보고 최대한 그럴듯한 이유를 생각해보기로 하자.\n새로 정의한 \\(\\gamma_{nk}\\)는 현재 피라미터 \\(\\pi_k\\), \\(\\mu_k\\)에서 데이터 \\(x_n\\)이 \\(k\\)번째 확률분포에서 발생할것 같은 정도를 나타낸다. 식을 보면 \\(x_n\\)의 \\(k\\)번 째 확률질량 함숫값을 모든 확률분포에서 구한 확률질량 함숫값의 합으로 나누고 있다. 따라서 \\(k\\)번 째 확률분포의 \\(x_n\\)에 대한 책임값responsibility이라고 하기도 한다. 무슨 이야긴지 실제 예를 보면서 이야기해보자.\n\nmu = mu_0\npi = pi_0\n\nprint(\"Init. mu: \")\nprint(mu)\n\nGamma = np.array([ pi[k]*binom.pmf(X, n_t, mu[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\nGamma /= Gamma.sum(axis=1, keepdims=True)\n\nprint(\"Gamma: \")\nprint(Gamma)\n\nprint(\"Summation Gamma for K\")\nprint(Gamma.sum(axis=1))\n\nInit. mu: \n[[0.4]\n [0.3]]\nGamma: \n[[0.94668864 0.05331136]\n [0.55621735 0.44378265]\n [0.44620687 0.55379313]\n [0.82510466 0.17489534]\n [0.88007654 0.11992346]]\nSummation Gamma for K\n[1. 1. 1. 1. 1.]\n\n\n\\(\\gamma_{nk}\\)에서 각 행은 \\(x_n\\)에 대한 \\(k\\)번 째 분포의 책임 정도를 나타내고 있다. 예를 들어 \\(x_1\\)에 대해서는 1번 분포가 0.946정도로 책임을 지며 2번 분포는 0.053정도 책임을 진다는 것이다. \\(x_2\\)에 대해서는 1번 분포가 0.556, 2번 분포가 0.443정도 책임을 진다. 다르게 말하면 현재 파라미터 상태에서는 \\(x_1\\)은 95%정도는 1번 분포에서 나왔을것 같고 5%정도만 2번 분포에서 나왔을 것 같다는 말이 된다. 따라서 \\(\\gamma\\)의 모든 행을 \\(k\\)에 대해 다 더하면 1이 되고 그렇기 때문에 식(22)에서 좌변이 \\(N\\)이 되었던 것이다. 이렇게 다섯 개 데이터에 대해서 어느 분포에서 나왔을 것 같은지를 모두 계산했다.\n이제 식(9.17)처럼 \\(\\gamma_{nk}\\)에 \\(x_n\\)을 직접 곱하게 되면 \\(x_n\\)값을 \\(\\gamma_{nk}\\) 비율대로 쪼개서 각 분포에 할당하게 될 것이다.\n\nprint(\"Gamma*X: \")\nprint(Gamma * X)\nprint(\"Summation Gamma*X for K\")\nprint((Gamma * X).sum(axis=1))\n\nGamma*X: \n[[9.46688636 0.53311364]\n [2.22486939 1.77513061]\n [1.3386206  1.6613794 ]\n [5.77573259 1.22426741]\n [7.04061236 0.95938764]]\nSummation Gamma*X for K\n[10.  4.  3.  7.  8.]\n\n\n위 코드로 계산 해보면 \\(x_1=10\\)인데 1번 분포에 9.466…정도를 할당하고 2번 분포에 0.533… 정도를 할당하게 된다. 다른 모든 데이터도 이렇게 각 분포에 실제 값을 적당히 찢어서 할당하게 된다. 당연히 \\(k\\)에 대해서 다 더하면 실제 데이터 값이 나오게 된다. 실제라면 데이터가 이렇게 쪼개져서 할당될 수 없고 두 분포 중 한쪽으로만 (hard 하게) 배분되어야 하지만 우리는 어느 분포로 할당해야할지 모르기 때문에 적당히 그럴듯 하게 쪼개서 (soft 하게) 할당한 것이라 이해하면 된다.\n이렇게 대충 그럴것이라 생각되는 정도로 데이터를 각 분포에 할당했다면 완전 데이터 가능도에서 한 것처럼 각 분포에 할당된 데이터들을 가지고 평균을 계산해서 파라미터를 추정할 수 있는 것이다.\n\\[\n\\mu_k= \\frac{\\sum_{n=1}^N z_{nk} x_n}{n_t N_k} \\tag{9}\n\\]\n식(9)를 다시보면 \\(k\\)번 째 분포에 할당된 데이터의 총합을 데이터의 개수를 나타내는 \\(N_k\\)로 나누어 평균을 구하는 것을 알 수 있다. 추가로 \\(n_t\\)를 더 나누는 것은 이항분포가 이미 \\(n_t\\)번 베르누이 시행의 합산이기 때문이다. 불완전 데이터 세트에서 \\(N_k\\)의 의미를 따져보면 \\(k\\)번 째 분포에 할당될 것 같은 데이터의 개수이므로 3개, 4개 같이 딱 떨어지기 보다는 3.4와 같이 소수가 될 것이다.\n\nN_k = (Gamma).sum(axis=0,keepdims=True)\nprint(\"N_k: \")\nprint( N_k  )\n\nprint(\"Sum of N_k: \")\nprint(N_k.sum())\n\nprint(\"Sum of all data assigned to k-th distribution\")\nprint( (Gamma*X).sum(axis=0,keepdims=True)  )\n\nprint(\"mu after the first iteration\")\nprint( (Gamma*X).sum(axis=0,keepdims=True) / (N_k*n_t)  )\n\nN_k: \n[[3.65429405 1.34570595]]\nSum of N_k: \n5.0\nSum of all data assigned to k-th distribution\n[[25.84672129  6.15327871]]\nmu after the first iteration\n[[0.70729725 0.45725284]]\n\n\n코드를 통해 \\(N_k\\)를 찍어보면 1번 분포에 3.65개, 2번 분포에 1.34개 정도 할당을 한것을 확인할 수 있다. 물론 다 더하면 5가 된다. 이제 적당히 분리된 데이터를 분포별로 다 더하고 \\(n_t N_k\\)로 나누면 한번 반복이 완전히 완료되고 업데이트된 파라미터가 구해지게 된다.\n[[0.70729725 0.45725284]]\n한번 반복하여 업데이트 한 이 값을 눈여겨 봐두자.\n이런 분석 과정을 통해 식(9.17), (9.18), (9.22)는 현재 파라미터 상태에서 그럴듯하게 새로운 파라미터를 추정하고 있음을 어렴풋이 알 수 있다. 그리고 돌이켜보면 \\(\\gamma_{nk}\\)를 구하는 과정과 앞서 알아본 야구 감독이 1선발 등판, 2선발 등판 상황에 확률을 부여하고 승률의 평균을 구하는 과정이 꽤 닮아있다는 느낌을 받을 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#mathbfz의-사후확률",
    "href": "posts/em/em_algorithm.html#mathbfz의-사후확률",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "\\(\\mathbf{Z}\\)의 사후확률",
    "text": "\\(\\mathbf{Z}\\)의 사후확률\n앞선 글에서 느닷없이 반복법을 들먹이면서 결과적으로는 해가 수렴함을 보였다. 이번 편에서는 이 현상을 평균 관점에서 해석해보도록 하자. 1편에서 평균을 이용하는 전체적인 전략을 이야기 했었다. 다시 한번 상기해보면 다음과 같다.\n\n\\(\\mathbf{Z}\\)를 모르기 때문에 \\(\\mathbf{Z}\\)가 가능한 모든 상태를 생각해보자.\n그리고 그 상태들에 확률을 부여할 수 있다고 가정해보자.\n그러면 모든 상태에 대해서 가능도 함숫값을 구하고 그렇게 구해진 함숫값들을 해당 \\(\\mathbf{Z}\\)에 부여된 확률을 이용해서 평균낼 수 있지 않을까?\n그렇게만 할 수 있다면 그 가능도 함숫값의 평균을 최대화하는 파라미터를 찾을 수 있을 것이다.\n\n그리고 존재 가능한 모든 \\(\\mathbf{Z}\\)(우리 문제에서는 32가지)에 대해서 확률을 부여하는 것을 해결해야 한다고 했다. 우선 존재 가능한 모든 행렬 \\(\\mathbf{Z}\\)를 만들어보자.\n\ndef possible_Z(rows, N):\n    R = [rows for i in range(N)]\n    return np.array( list(itertools.product(*R)) )\n\n#R = [[1,0],[0,1]]\nR = np.eye(K)\nZs = possible_Z(R, N)\nZs.shape\n\n(32, 5, 2)\n\n\n위 함수를 이용해서 존재 가능한 모든 \\(\\mathbf{Z}\\)를 만들 수 있다. 분포가 두개인 경우 서른 두가지 (5,2) 행렬이 만들어지며 몇 가지 확인해보면 다음과 같다.\n\nprint(Zs[0])\nprint(Zs[10])\nprint(Zs[31])\n\n[[1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]]\n[[1. 0.]\n [0. 1.]\n [1. 0.]\n [0. 1.]\n [1. 0.]]\n[[0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]]\n\n\n현재 \\(\\mathbf{z}\\)에 대한 정보는 전혀 없는 상태이므로 \\(x\\)가 주어졌을 때 \\(\\mathbf{z}\\)에 대한 사후확률을 계산해야 한다. \\(x\\)가 주어졌을 때\\(z_k=1\\)일 확률을 \\(p(z_k=1 \\mid x)\\)로 쓰면 베이즈 정리에 의해 다음과 같다.\n\\[\n\\begin{aligned}\n\\gamma(z_k) = p(z_k=1 \\mid x) &= \\frac{p(z_k=1)p(x \\mid z_k =1)}{\\sum_{j=1}^K p(z_j=1)p(x \\mid z_j=1) } \\\\[5pt]\n&= \\frac{\\pi_k \\text{Bin}(x \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x \\mid n_t, \\mu_j)}\n\\end{aligned}\n\\]\n두 번째 등호를 위해 식(9.10)과 (9.11)을 사용하였다.\n\\[\np(\\mathbf{z}) = \\prod_{k=1}^{K} \\pi_k^{z_k} \\tag{9.10}\n\\]\n\\[\np(x \\mid \\mathbf{z}) = \\prod_{k=1}^K \\text{Bin}(x \\mid n_t, \\mu_k)^{z_k} \\tag{9.11}\n\\]\n우리 문제에서 \\(x\\)는 \\(N\\)개가 있으므로 \\(x\\)에 인덱스 \\(n\\)을 도입하면\n\\[\n\\begin{aligned}\n\\gamma(z_{nk}) = p(z_{nk}=1 \\mid x_n) &= \\frac{p(z_{nk}=1)p(x_n \\mid  z_{nk} =1)}{\\sum_{j=1}^K p(z_{nj}=1)p(x_n \\mid z_{nj}=1) } \\\\[5pt]\n&= \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}\n\\end{aligned}\n\\]\n가 되므로 식(22)에서 구했던 \\(\\gamma_{nk}\\)가 \\(\\mathbf{z}\\)의 사후확률임을 알 수 있다. 따라서 벡터 변수 \\(\\mathbf{z}\\)에 대해서는 식(9.10)에 의해\n\\[\np(\\mathbf{z}_n \\mid \\mathbf{x}_n)= \\prod_{k=1}^K \\left\\{ \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right\\}^{z_{nk}}\n\\]\n가 된다.\n이제 모든 데이터를 고려한 \\(\\mathbf{Z}\\)에 대한 \\(\\mathbf{X}\\)를 조건으로 하는 사후확률은 \\(x\\)를 독립적으로 샘플링했다는 가정하에서 다음처럼 쓸 수 있다.\n\\[\np(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) = \\prod_{n=1}^N p( \\mathbf{z}_n \\mid x_n, \\boldsymbol{\\theta}) \\tag{25}\n\\]\n앞서 얻은 결과를 이용하면 최종적으로 다음과 같다.\n\\[\n\\begin{aligned}\np(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})\n&= \\prod_{n=1}^N \\prod_{k=1}^K \\left\\{ \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right\\}^{z_{nk}} \\\\\n&=  \\prod_{n=1}^N \\prod_{k=1}^K \\gamma_{nk}^{z_{nk}}\n\\end{aligned} \\tag{26}\n\\]\n식(26)을 이용하면 모든 \\(\\mathbf{Z}\\) 행렬에 대해서 확률을 부여할 수 있다. 조금 후 자주 사용할 수식이기 때문에 우선 아래 코드로 구현해 놓는다.\n\ndef set_posterior_Z(X, theta_old):\n    X = X\n    theta_old = theta_old\n\n    def posterior_Z(Z):\n        \"\"\"\n        EQ(26)\n        Z : (N,K)\n        X : (N,D)\n        theta_old: (4,), (mu_1, mu_2, pi_1, pi_2)\n        --------------------------------------\n        N,D,K : gloval variables\n        \"\"\"\n        mu = theta_old[:K*D].reshape(K,D)\n        pi = theta_old[-K:]\n\n        Gamma = np.array([ pi[k]*binom.pmf(X, n_t, mu[k][0])\n                         for k in range(K) ]).transpose(1,0,2).squeeze()\n\n        Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n        return Gamma[Z==1].prod()\n\n    return posterior_Z\n\n# 현재 X와 theta_old를 조건으로 하는 Z의 확률분포를 세팅한다.\ntheta_old = np.hstack((mu_0.flatten(), pi_0))\nposterior_Z = set_posterior_Z(X, theta_old)\n\n코드가 약간 복잡한데 조건으로 주어진 \\(\\mathbf{X}\\), \\(\\boldsymbol{\\theta}\\)를 함수 인자로 받지 않도록 하기 위해 클로져를 사용해서 함수를 만들어 리턴한다. 이제 set_posterior_Z()를 호출해서 사후확률을 구해주는 함수를 돌려 받으면 \\(\\mathbf{Z}\\)만 넘겨서 확률분포함수의 함숫값을 간편하게 구할 수 있다.\n\n# 이제 특정 Z에 대한 확률분포함숫값을 구할 수 있다.\nposterior_Z(Zs[0])\n\n0.17061508495234065"
  },
  {
    "objectID": "posts/em/em_algorithm.html#완전-데이터-세트-가능도-함수의-평균",
    "href": "posts/em/em_algorithm.html#완전-데이터-세트-가능도-함수의-평균",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "완전 데이터 세트 가능도 함수의 평균",
    "text": "완전 데이터 세트 가능도 함수의 평균\n이제 식(26)을 이용하여 완전 데이터 세트에 대한 가능도의 \\(\\mathbf{Z}\\)에 대한 평균 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)을 구할 수 있다. 구체적으로 계산하기 전에 개념적으로 앞선 반복법에서 고정하는 파라미터(빨간색)와 조정하는 파리미터(파란색)로 구분한 이유를 잠시 알아보기로 하자.\n완전 데이터 세트의 가능도 함수 \\(\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\\)의 평균을 \\(\\mathbf{Z}\\)에 대해서 구하려면 \\(\\mathbf{Z}\\)에 대한 확률분포를 사용하여 다음처럼 하면 된다.\n\\[\n\\sum_{\\mathbf{Z}} p\\left(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}\\right) \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\tag{27}\n\\]\n식(27)을 최대화 하려면 \\(p\\left(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}\\right)\\)의 \\(\\boldsymbol{\\theta}\\)는 고정하고 \\(\\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})\\)의 \\(\\boldsymbol{\\theta}\\)를 조절해야 할 것이다. 식(27)을 \\(\\mathcal{Q}\\)라는 함수로 다시 적으면 다음과 같다.\n\\[\n\\mathcal{Q}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}\\right) = \\sum_{\\mathbf{Z}} p\\left(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}\\right) \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\tag{9.30}\n\\]\n식(9.30)에서 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)는 \\(\\mathbf{Z}\\)의 사후확률분포를 구하기 위한 고정된 현재 파라미터이고, \\(\\boldsymbol{\\theta}\\)는 \\(\\mathcal{Q}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}\\right)\\)를 최대화 하기 위해 우리가 조정하는 파라미터이다. 이렇게 완전 데이터 세트 가능도 함수의 평균을 구하는 과정에서 이미 지정된 파라미터와 최적화 될 파라미터가 자연스럽게 구분되게 된다. 이것으로 반복법에서 변수를 느닷없이 두 부류로 나눈 것이 \\(\\mathbf{Z}\\)의 사후확률을 구하는 것과 완전 데이터 세트의 가능도 함수 평균을 최대화하는 과정과 관계 있다는 것을 어렴풋이 알 수 있다.\n\n수치적 방법: 반복법에 대한 당위성\n앞서 변수를 두 부류로 나눈것에 대한 이유를 대강 알아보았으니 이번에는 왜 반복법을 사용했는가에 대한 이유를 알아보자. 식(9.30)을 최대화 시키는 \\(\\boldsymbol{\\theta}\\)를 찾으면 되므로 여기서도 수치적 방법을 동원해서 해를 찾아보자. 우선 \\(\\mathcal{Q}\\) 함수를 준비한다. 이미 모든 부속 함수를 만들어 놓았으므로 쉽게 코딩할 수 있다.\n\ndef Q(theta, theta_old):\n    \"\"\"\n    theta     : (4,), (mu_1, mu_2, pi_1, pi_2)\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    -------------------------------------------\n    gloval variables\n    X  : (N,D)\n    Zs : (K^N, N, K)\n    N,D,K\n    \"\"\"\n    # K = Zs.shape[2]\n\n    p_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\n    ret = 0.0\n    for Z_i in Zs : # 모든 Z에 대해서 평균을 낸다.\n        # eq (9.30)\n        ret += p_Z_given_X_and_theta_old(Z_i)*loglikelihood_XZ(theta, X, Z_i)\n\n    return ret\n\n# scipy.optimize.minimize 사용을 위해 -Q로 만들어 놓는다.\ndef negative_Q(theta, theta_old):\n    return -Q(theta, theta_old)\n\n이제 초기값을 설정하고 이전에 만들어 둔 제약조건을 사용해서 최적화 함수를 호출한다.\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\ntheta_old = np.copy(theta)\nprint('Init. input:', x)\n\nres = minimize(negative_Q, x, args=(theta_old), method='SLSQP',\n               bounds=bounds, constraints=cons,\n               options={'iprint': 2, 'disp': True})\n\nprint(res)\n\nprint(\"sum pi_k:\",res.x[2:].sum())\n\nInit. input: [0.4 0.3 0.5 0.5]\n  NIT    FC           OBJFUN            GNORM\n    1     6     2.389727E+01     4.848931E+01\n    2    13     1.906314E+01     3.115227E+01\n    3    20     1.754121E+01     2.277992E+01\n    4    27     1.648351E+01     1.696680E+01\n    5    34     1.624252E+01     1.531351E+01\n    6    41     1.595074E+01     1.074425E+01\n    7    47     1.570851E+01     9.368190E+00\n    8    53     1.556181E+01     6.907221E+00\n    9    59     1.555421E+01     7.045966E+00\n   10    65     1.555393E+01     7.073587E+00\n   11    71     1.555392E+01     7.071202E+00\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 15.553919769048395\n            Iterations: 11\n            Function evaluations: 71\n            Gradient evaluations: 11\n     fun: 15.553919769048395\n     jac: array([-1.69980526e-03,  4.49538231e-04, -4.99988914e+00, -5.00030005e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 71\n     nit: 11\n    njev: 11\n  status: 0\n success: True\n       x: array([0.70728766, 0.45726058, 0.73087497, 0.26912503])\nsum pi_k: 1.0\n\n\n성공적으로 최적화가 수행되었고 구해진 해는 아래와 같다.\nx: array([0.70728766, 0.45726058, 0.73087497, 0.26912503])\n1편과 2편에서 구한 수치해와는 조금 차이가 있다. 첫 두 파라미터는 0.8, 0.45 근처값이 최적해인데 구해진 해는 0.707, 0.457로 최적해로 수렴하다 만것같은 느낌이 든다. 이런 결과가 나온 이유는 여기서 우리가 최적화 시킨 함수는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)이 아니고 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)이기 때문이다. 가능도 함수를 직접 최적화 시키지 못하고 그 평균을 최적화 시켰기 때문에 우리가 원하는 최적점까지 가지 못한 것이다.\n2편에서 반복법을 설명할 때 한 번 반복 후 업데이트 된 해를 눈여겨 봐두자고 했었는데 구해진 해가 그 해와 아주 비슷한 것을 알 수 있다. 2편 반복법에서 첫 번째로 업데이트된 해를 다시보자.\n[[0.70729725 0.45725284]]\n거의 같은 해라는 것을 알 수 있을 것이다. 이는 우연이 아니며 나중에 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 직접 미분하여 구한 해가 식(9.17), (9.22)와 일치하는 것을 알아보도록 할것이다.\n그렇기에 구해진 해를 theta_old로 재설정하고 최적화를 다시 수행하기를 반복하면 원하는 최적해로 수렴할 수 있을 것 같다는 예상을 할 수 있다.\n\n# 각 파라미터를 펼쳐서 1차원 배열 하나로 만든다.\ntheta = np.hstack((mu_0.flatten(), pi_0))\nx = np.copy(theta)\ntheta_old = np.copy(theta)\nprint('Init. input:', x)\n\nfor i in range(20) :\n    res = minimize(negative_Q, x, args=(theta_old), method='SLSQP',\n                bounds=bounds, constraints=cons,\n                options={'iprint': 0, 'disp': True})\n\n    if i % 5 == 0:\n        print(res.x)\n\n    x = np.copy(res.x)\n    theta_old = np.copy(res.x)\n\nInit. input: [0.4 0.3 0.5 0.5]\n[0.70728766 0.45726058 0.73087497 0.26912503]\n[0.8375278  0.37398245 0.57384792 0.42615208]\n[0.83806643 0.37565554 0.57195203 0.42804797]\n[0.83811421 0.37567489 0.57181128 0.42818872]\n\n\n결과를 보면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접 미분하여 구한 결과인 식(9.17), (9.18), (9.22)를 반복적으로 적용하여 구한 해로 수렴하는 듯 보인다.\n지금까지 과정을 다시한번 정리해보자.\n\n2편에서는 \\(\\mathbf{z}\\)에 대한 정보가 없기 때문에 \\(\\mathbf{z}\\)를 주변화 시켜서 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 구하고 이를 직접 수치적으로 또 해석적으로 최적화 시켰다.\n해석적인 과정에서 구해진 해는 닫힌 형식이 아니라서 궁여지책으로 반복법을 제안하였고 신기하게도 그 방법이 수치해로 수렴하는 것을 확인하였다.\n이번에는 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 계산하고 이를 최적화하였다. \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 유도하는 과정에서 고정해야하는 파라미터와 조절해야하는 파라미터를 자연스럽게 분리할 수 있었다.\n3번에서 최적화 결과는 만족스럽지 못했는데 2번에서 반복법을 여기서도 그대로 적용해보니 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)에 대한 최적해가 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 최적해로 수렴하는 듯 보였다.\n\n현재까지 실험으로 미뤄보면 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 반복적으로 최대화 시킨 결과가 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 최적해로 수렴한다고 결론 내릴 수 있다. 우리의 궁극적인 목적은 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화 시키는 것이기 때문에 결국 목적이 달성된 것이다.\n\n\n해석적 방법\n1, 2편과 마찬가지로 이번에는 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)에 대한 해석적 방법을 고려해보자. 그러기 위해서 먼저 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 우리 문제에 대해서 목적함수로 정식화 해야 한다. 그 후 정식화된 목적함수를 미분할 것이다.\n\n목적함수 정식화\n이제 식(9.36)에 기대값을 취하면 다음과 같다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{Z}} \\left[  \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\n&= \\mathbb{E}_{\\mathbf{Z}} \\left[ \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}_{\\mathbf{Z}} [ z_{nk} ] \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\}\n\\end{aligned}\n\\]\n위 식에서 \\(\\mathbb{E}_{\\mathbf{Z}} [ z_{nk} ]\\) 부분을 풀어야 하는데 이를 위해 \\(x_n\\)이 독립적으로 샘플링 되었다는 독립성 가정에 의해 앞서 유도한 \\(\\mathbf{Z}\\)의 사후확률분포인 다음식을 보자.\n\\[\np(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) = \\prod_{n=1}^N p( \\mathbf{z}_n \\mid x_n, \\boldsymbol{\\theta}) \\tag{25}\n\\]\n식(25)는 인덱스 \\(n\\)에 대해 단순히 곱해진 식이므로 행렬 \\(\\mathbf{Z}\\)에 대한 확률분포를 구성하는 행벡터 \\(\\mathbf{z}_n\\)들은 모두 독립이다. 이런 이유로 \\(\\mathbf{Z}\\)의 사후확률분포에서 \\(z_{nk}\\)의 기댓값은 \\(z_{nk}\\)가 속하는 벡터 \\(\\mathbf{z}_n\\)에 대해서만 기대값을 고려하면 된다. 이를 확률변수 \\(X\\)와 \\(Y\\)가 결합되어 있을 때 결합확률분포에서 \\(X\\)에 대한 기댓값을 구하는 경우를 예를 들어 생각해보면 이해가 쉬워진다. 결합확률분포 \\(p(x, y)\\)에 대해서 확률변수 \\(X\\)의 기댓값은 다음처럼 쓸 수 있다.\n\\[\n\\mathbb{E}_{p(x,y)}[X] = \\sum_i \\sum_j x_i p(x_i, y_i)\n\\]\n그런데 \\(X\\), \\(Y\\)가 독립이면 다음처럼 된다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{p(x,y)}[X] &= \\sum_i \\sum_j x_i p(x_i, y_i) \\\\\n&= \\sum_i \\sum_j x_i p(x_i)p(y_i) \\\\\n&= \\sum_i x_i p(x_i) \\sum_j p(y_j) \\\\\n&= \\mathbb{E}_{p(x)}[X]\n\\end{aligned}\n\\]\n결국 \\(p(x)\\)에 대한 \\(X\\)의 기댓값이 된다. 마찬가지로 \\(z_{nk}\\)의 기댓값은 다음처럼 벡터 \\(\\mathbf{z}_n\\)의 합산에 의해 주어지게 된다. 다시말해 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})\\)에 대한 기댓값에서 \\(p( \\mathbf{z}_n \\mid x_n, \\boldsymbol{\\theta})\\)에 대한 기댓값으로 바뀌게 되었다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ]\n&= \\sum_{\\mathbf{z}_n}  z_{nk} \\cdot p(\\mathbf{z}_n \\mid x_n) \\\\[5pt]\n&= \\sum_{\\mathbf{z}_n}  z_{nk} \\frac{p(x_n, \\mathbf{z}_n)}{p(x_n)} \\\\[5pt]\n&= \\sum_{\\mathbf{z}_n}   \\frac{z_{nk} \\cdot p(x_n, \\mathbf{z}_n)}{p(x_n)} \\\\[5pt]\n&= \\frac{ \\sum_{\\mathbf{z}_n}  z_{nk} \\cdot p(x_n, \\mathbf{z}_n)}{ \\sum_{\\mathbf{z}_n}  p(x_n, \\mathbf{z}_n)} \\\\\n\\end{aligned} \\tag{9.39-1}\n\\]\n위 식에서 \\(\\sum_{\\mathbf{z}_n}\\)은 \\(\\mathbf{z}_n\\) 벡터가 가질 수 있는 모든 벡터들에 대해서 합산하라는 뜻이다. 데이터 \\(x_n\\)하나에 대응되는 \\(\\mathbf{z}_n\\) 벡터 \\(N\\)개에 대해서 합산하라는 뜻이 아님을 주의해야 한다. 예를들어 \\(K=2\\)이면 \\(\\mathbf{z}_n = (0,1)^{\\text{T}}\\), \\(\\mathbf{z}_n = (1,0)^{\\text{T}}\\)인 두 경우에 대해서 합산하라는 뜻이다. \\(p(x_n, \\mathbf{z}_n)\\)은 아래와 같으므로\n\\[\np(x_n, \\mathbf{z}_n) = \\prod_{k=1}^K \\left[ \\pi_k \\text{Bin}(x_n \\mid \\mu_k) \\right]^{z_{nk}}\n\\]\n위 식을 대입하면\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ]\n&= \\frac{ \\sum_{\\mathbf{z}_n}  z_{nk} \\cdot p(x_n, \\mathbf{z}_n)}{ \\sum_{\\mathbf{z}_n}  p(x_n, \\mathbf{z}_n)} \\\\[5pt]\n&= \\frac{ \\sum_{\\mathbf{z}_n} z_{nk} \\prod_{k'=1}^K [\\pi_{k'} \\text{Bin}(x_n \\mid \\mu_{k'})]^{z_{nk'}}}{ \\sum_{\\mathbf{z}_n} \\prod_{j=1}^K [\\pi_j \\text{Bin}(x_n \\mid \\mu_j)]^{z_{nj}} }\n\\end{aligned} \\tag{9.39-2}\n\\]\n가 된다. 식(9.39-2)에서 분모를 보자. 모든 \\(\\mathbf{z}_n\\)에 대해서 \\(\\prod_{j=1}^K\\)를 계산하고 있는데 \\(\\mathbf{z}_n\\)은 오직 한 자리만 1인 원핫 벡터이므로 결국 분모는 \\(\\sum_{j=1}^K\\)가 된다. 분자도 같은 이유로 \\(K\\)항이 합산되는데 \\(z_{nk}\\)가 곱해지기 때문에 \\(k\\)번 째 항만 남고 나머지는 0이 곱해져 다 사라지게 된다. 따라서 최종적으로 다음과 같다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ]\n&= \\frac{ \\sum_{\\mathbf{z}_n} z_{nk} \\prod_{k'=1}^K [\\pi_{k'} \\text{Bin}(x_n \\mid \\mu_{k'})]^{z_{nk'}}}{ \\sum_{\\mathbf{z}_n} \\prod_{j=1}^K [\\pi_j \\text{Bin}(x_n \\mid \\mu_j)]^{z_{nj}} } \\\\[5pt]\n&= \\frac{\\pi_k \\text{Bin}(x_n \\mid \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid \\mu_j)} = \\gamma(z_{nk})\n\\end{aligned} \\tag{9.39}\n\\]\n주어진 결과는 이전에 계산한 책임값 \\(\\gamma_{nk}\\)가 된다.\n이제 식(9.36)에 기대값을 취해 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 직접 구할 수 있다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{Z}} \\left[  \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\n&= \\mathbb{E}_{\\mathbf{Z}} \\left[ \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\right] \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}_{\\mathbf{Z}} [ z_{nk} ] \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}_{\\mathbf{z}_n \\mid x_n} [ z_{nk} ] \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\} \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^K \\gamma(z_{nk}) \\{ \\ln \\pi_k + \\ln \\text{Bin}(x_n \\mid  n_t, \\mu_k) \\}\n\\end{aligned} \\tag{9.40}\n\\]\n식(9.40)에서 기댓값의 선형성이 사용되었다. 아래 코드로 구현 해두도록 하자.\n\ndef E_Z_loglikelihood_XZ(theta, X) :\n    \"\"\"\n    EQ (9.40)\n    theta     : (4,), (mu_1, mu_2, pi_1, pi_2)\n    -------------------------------------------\n    gloval variables\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    N,D,K\n    \"\"\"\n    # N, D = X.shape[0], X.shape[1]\n\n    mu = theta[:K*D].reshape(K,D)\n    pi = theta[-K:]\n\n    mu_old = theta_old[:K*D].reshape(K,D)\n    pi_old = theta_old[-K:]\n\n    # 전역변수 theta_old로 계산하는 것 주의!!\n    Gamma = np.array([ pi_old[k]*binom.pmf(X, n_t, mu_old[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\n    Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n    binom_x_given_mu_k = np.array([ binom.pmf(X, n_t, mu[k][0])\n                                for k in range(K) ]).transpose(1,0,2).squeeze()\n\n    return (Gamma*(np.log(pi+1.0e-8).reshape(1,-1)\n            + np.log(binom_x_given_mu_k+1.0e-8))).sum()\n\n식(9.30)과 (9.40)은 궁극적으로 같은 식이다. 아래 코드로 두 함수의 값을 찍어보면 같은 값이 찍히는 것을 확인할 수 있다.\n\ntheta = np.hstack((mu_0.flatten(), pi_0))\ntheta_old = theta\n\nprint( \"Q(theta, theta_old) : {:.6f}\".format(Q(theta, theta_old)) )\nprint( \"Ez[ln p(X,Z|theta)] : {:.6f}\".format(E_Z_loglikelihood_XZ(theta,  X)) )\n\nQ(theta, theta_old) : -23.897270\nEz[ln p(X,Z|theta)] : -23.897270\n\n\n이제 정식화가 끝났으니 미분하여 0으로 두고 최적해를 찾는 일만 남았다.\n\n\n목적함수 미분\n식(9.36)과 식(9.40)에서 차이점은 \\(z_{nk}\\)가 \\(\\gamma(z_{nk})\\)로 변한것 밖에 없으므로 식(9.40)을 \\(\\mu_k\\)로 미분하고 0으로 두어 최적해를 구하는 과정은 식(9.36)에서 계산한 것과 동일하며 최종적으로 다음처럼 될것이다.\n\\[\n\\mu_k = \\frac{\\sum_{n=1}^N \\gamma(z_{nk}) x_n }{n_t \\sum_{n=1}^N \\gamma(z_{nk}) } \\tag{28}\n\\]\n\\(\\gamma(z_{nk})\\)는 다음과 같으므로\n\\[\n\\gamma(z_{nk}) = \\frac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)}\n\\]\n식(28)에 대입하면\n\\[\n\\mu_k = \\frac{\\sum_{n=1}^N \\left( \\dfrac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right) x_n }{n_t \\sum_{n=1}^N \\left( \\dfrac{\\pi_k \\text{Bin}(x_n \\mid n_t, \\mu_k)}{\\sum_{j=1}^K \\pi_j \\text{Bin}(x_n \\mid n_t, \\mu_j)} \\right) }  \\tag{29}\n\\]\n가 된다. 식(29)는 식(9.17)과 정확히 동일한 식이다.\n\\(\\pi_k\\)로 미분하는 과정도 역시 동일하게 식(5), (6)과 유사한 다음 두식을 얻을 수 있다.\n\\[\n\\sum_{n}^N \\left(  \\frac{\\gamma(z_{nk})}{\\pi_k}\\right) + \\lambda = 0 \\tag{30}\n\\]\n\\[\n\\sum_{j=1}^K \\pi_j -1 = 0 \\tag{31}\n\\]\n식(30), (31)을 연립해서 풀면\n\\[\n\\pi_k = \\frac{N_k}{N} \\tag{32}\n\\]\n를 얻을 수 있는데 이 역시 식(9.22)와 동일한 식이다.\n이 과정을 통해 알 수 있는 사실은 식(9.17), (9.22)와 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 미분하여 얻은 식 (29), (32)는 완전히 동일하다는 점이고, 더욱 중요한 사실은 두 식들이 모양은 같지만 두 번째 구한 식(29), (32)들은 닫힌 형식의 해라는 점이다. 왜냐하면 식(29), (32)에서 우변에 나타난 \\(\\mu_k\\), \\(\\pi_k\\)들은 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 구성할 때 미리 지정한 파라미터들이기 때문이다.\n그렇기 때문에 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 미분하여 얻은 최적해인 식(9.17), (9.22)를 통해서는 원칙적으로 어떤 해도 구할 수 없었지만 \\(\\mathbb{E}_{\\mathbf{Z}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\right]\\)를 미분하여 찾은 최적해는 그것이 비록 만족스런 최적해가 아닐지라도 적어도 계산은 가능해 진 것이다. 즉 \\(\\mu_k\\), \\(\\pi_k\\)를 임의로 지정하고 식(29), (32)를 통해 개선된 파라미터를 계산할 수 있는 것이다. 하지만 이 한번의 업데이트를 통해서 만족스런 해에 도달할 수 없다는 것을 앞선 수치과정으로 확인했다. 아울러 만족스런 해를 얻기 위해서는 식(29), (32)를 반복적으로 시도해야 한다는 사실도 앞선 수치적 방법을 통한 실험으로 알 수 있었다.\n이제 우리에게 남은 마지막 의문은 왜 반복법을 사용하면 해가 수렴하는가 하는 것이다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#반복법이-수렴하는-이유",
    "href": "posts/em/em_algorithm.html#반복법이-수렴하는-이유",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "반복법이 수렴하는 이유",
    "text": "반복법이 수렴하는 이유\n2편과 3편에 걸쳐 제안하고 실험한 반복법의 해가 $ ( )$의 최대값으로 수렴하는 이유를 알아보기 위해 다음 식(9.70)과 같은 분해를 알아보자.\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})= \\mathcal{L}(q, \\boldsymbol{\\theta}) + KL(q \\,||\\, p) \\tag{9.70}\n\\]\n식 (9.70)에서 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)가 무엇인지 유도하기 위해\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\ln \\sum_{\\mathbf{Z}}  p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\n\\]\n에서 우변에 먼저 임의의 \\(q(\\mathbf{Z})\\)를 도입하자.\n\\[\n\\begin{aligned}\n\\ln \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\n&= \\ln \\left\\{ \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\\\[5pt]\n& \\ge \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\qquad \\because \\text{Jensen's inequality}\\\\[5pt]\n&=  \\mathcal{L}(q, \\boldsymbol{\\theta})\n\\end{aligned}\n\\]\n위 식에서 부등식은 옌센 부등식으로 로그 함수는 오목함수라서 \\(\\mathbb{E}[f(x)] \\le f(\\mathbb{E}[x])\\)가 되어서 성립한다. \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 다음처럼 정의되고 정의 그 자체로 최적화 하고자 하는 파라미터를 조건으로 하는 데이터의 로그 가능도 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 하한이 됨을 알 수 있다.\n\\[\n\\mathcal{L}(q, \\boldsymbol{\\theta}) = \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\tag{9.71}\n\\]\n계속해서 식(9.70)을 완성해보자. 완전 데이터 데이터 세트의 확률분포는\n\\[\np(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})  = p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n\\]\n이며, 로그를 취하면\n\\[\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})  = \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\tag{9.73}\n\\]\n식(9.73)을 식(9.71)에 대입하면\n\\[\n\\begin{aligned}\n\\mathcal{L}(q, \\boldsymbol{\\theta}) &= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\left\\{ \\frac{p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta}) - \\ln q(\\mathbf{Z})\\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})  - \\ln q(\\mathbf{Z}) \\right\\} \\qquad \\because \\text{eq(9.73)} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) +  \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) - \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln q(\\mathbf{Z}) \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) - \\ln q(\\mathbf{Z}) \\right\\} + \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{ p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})}{ q(\\mathbf{Z}) } \\right\\} + \\sum_{\\mathbf{Z}} q(\\mathbf{Z})\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\\\[5pt]\n&= -KL(q \\,||\\, p) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\qquad \\because \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) = 1 \\\\[5pt]\n&= -KL(q \\,||\\, p) + \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n\\end{aligned}\n\\]\n\\(-KL(q \\, || \\, p)\\)를 이항하면 식(9.70)이 완성된다. 따라서 식(9.70)에서 \\(KL(q \\, || \\, p)\\)는 식(9.72)와 같다.\n\\[\nKL(q \\,||\\, p) = - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})}{ q(\\mathbf{Z}) } \\tag{9.72}\n\\]\n아니면 대수적으로 아래처럼 \\(\\ln (\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접 분해할 수 도 있다.\n\\[\n\\begin{aligned}\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})  \\qquad \\because \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) = 1\\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})} \\qquad \\because p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})  = p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\left\\{ \\frac{p(\\mathbf{Z},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})}  \\frac{q(\\mathbf{Z})}{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})} \\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\left\\{ \\ln \\frac{p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} + \\ln \\frac{q(\\mathbf{Z})}{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})} \\right\\} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{X},\\mathbf{z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} + \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{q(\\mathbf{Z})}{p(\\mathbf{z}\\,|\\,\\mathbf{x},\\boldsymbol{\\theta})} \\\\[5pt]\n&= \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{X},\\mathbf{Z}\\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{Z},\\boldsymbol{\\theta})}{ q(\\mathbf{Z}) } \\\\[5pt]\n&= \\mathcal{L}(q, \\boldsymbol{\\theta}) + KL(q \\,||\\, p)\n\\end{aligned} \\tag{9.70}\n\\]\n정리를 위해 식(9.70), (9.71), (9.72)를 함께 다시 써보면\n\\[\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})= \\mathcal{L}(q, \\boldsymbol{\\theta}) + KL(q \\,||\\, p) \\tag{9.70}\n\\]\n\\[\n\\mathcal{L}(q, \\boldsymbol{\\theta}) = \\sum_{\\mathbf{Z}} q(\\mathbf{Z})  \\ln \\left\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\right\\} \\tag{9.71}\n\\]\n\\[\nKL(q \\,||\\, p) = - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta})}{ q(\\mathbf{Z}) } \\tag{9.72}\n\\]\n위 식은 \\(\\mathbf{X}\\)에 대한 로그 가능도 함수가 임의로 선택된 \\(q(\\mathbf{Z})\\)에 대해서 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(KL(q \\,||\\, p)\\)로 분해됨을 알려준다. \\(q(\\mathbf{Z})\\)란 \\(\\mathbf{Z}\\)에 대한 분포로 우리는 \\(\\mathbf{Z}\\)에 대해 아는 것이 없으므로 임의로 선택해야 하는 분포이다. 그런데 \\(q(\\mathbf{Z})\\)를 잘 선택하면 \\(KL(q \\,||\\, p)=0\\)으로 만들어 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})=\\mathcal{L}(q, \\boldsymbol{\\theta})\\) 로 만들 수 있다. 바로 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})\\)로 선택하면 식(9.72)에서 \\(KL(q \\,||\\, p)=0\\)이 됨을 알 수 있다. 식(9.70)을 구체적으로 시각화 해보자. 그림을 그리기위해 현재 우리가 가지고 있는 파라미터를 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)라 표시하고 설정해둔다.\n\ntheta_old = np.hstack((mu_0.flatten(), pi_0))\n\n\\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 하한 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(\\text{KL}(q\\,||\\,p)\\)로 나눠 그리기 위해서 식(9.71)을 로그 성질을 이용하여 풀어 적어보면\n\\[\n\\mathcal{L}(q, \\boldsymbol{\\theta}) = \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln q(\\mathbf{Z}) \\tag{33}\n\\]\n위 식에서 두 번째 항은 \\(q(\\mathbf{Z})\\)의 엔트로피라는 것을 알 수 있다. 먼저 두번째 항을 계산하기 위해 임의의 \\(q(\\mathbf{Z})\\)를 넘겨받아 엔트로피를 계산하는 함수를 만든다.\n\ndef entropy_Z(q) :\n    \"\"\"\n    주어진 확률 분포 q(Z)에 대해서 엔트로피를 계산한다.\n    q : Z를 받아서 확률분포를 계산하는 임의의 함수\n    \"\"\"\n    entropy = 0.0\n\n    for Z_i in Zs :\n        pz = q(Z_i)\n        entropy += pz*np.log(pz+1.0e-8)\n\n    return entropy\n\n\\(\\mathbf{Z}\\)에 대한 분포를 임의로 만들어 엔트로피를 계산해본다.\n\ndef dummy_q(*Z):\n    \"\"\"\n    Z에 대해 확률분포를 계산하는 더미함수\n    여기서는 모든 Z에 균등한 함숫값을 계산\n    gloval variables\n    Zs : (K^N, N, K)\n    \"\"\"\n    return 1. / Zs.shape[0]\n\n# 아무 의미없는 q(Z) 테스트\ndummy_q(Zs[1])\n\n0.03125\n\n\n엔트로피를 계산하면 잘 계산되는 것을 확인할 수 있다.\n\nentropy_Z(dummy_q)\n\n-3.4657355827997796\n\n\n\\(q(\\mathbf{Z})\\)를 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)로 제안받아 엔트로피를 계산해보자.\n\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\nentropy_Z(p_Z_given_X_and_theta_old)\n\n-2.4126501017806823\n\n\n이제 부속이 모두 준비되었으므로 \\(q(\\mathbf{Z})\\)를 제안받아 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 계산하는 함수를 만든다. \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 로그가능도의 하한lower bound라는 의미로 ELBO(Evidence Lower BOund)라고 하기도 한다.\n\n# Evidence Lower BOund, abbreviated as ELBO\ndef ELBO(q, theta) :\n    \"\"\"\n    global variables\n    X         : (N,D)\n    Zs        : (K^N, N, K)\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    K\n    \"\"\"\n    E = 0.0\n\n    for Z_i in Zs :\n        E += q(Z_i)*loglikelihood_XZ(theta, X, Z_i)\n\n    return E - entropy_Z(q)\n\nELBO(q, theta)를 사용하여 식(9.70)을 시각화해보기 위해 첫 번째로 \\(q(\\mathbf{Z})\\)를 dummy_q()로 두고 그림을 그려보자.\n\n# 제안분포를 dummy_q()로 선택하여 ELBO를 계산\nl_q_theta_dummy = ELBO(dummy_q, theta_old)\nln_p_X_theta = loglikelihood_X(theta_old, X)\nprint(\"l(q,theta)    : {:.6f}\".format(l_q_theta_dummy))\nprint(\"ln p(X|theta) : {:.6f}\".format(ln_p_X_theta))\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\nmargin = 0.5\n\n# ln p(X|theta_old)\nax.plot([0, 6], [ln_p_X_theta]*2, 'r', lw='3')\nax.text(3.8, ln_p_X_theta-margin,\n        r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='r', fontsize=20)\n\n# L(q_dummy,theta_old)\nax.plot([0, 3], [l_q_theta_dummy]*2, 'b', lw='3')\nax.text(0.8, l_q_theta_dummy-margin,\n        r\"$\\mathcal{L}(q_{\\text{dummy}}, \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\n\n# KL\nax.text(0, ln_p_X_theta+margin, r\"$\\text{KL}(q || p)$\",  color='g', fontsize=20)\nax.arrow(0.5, l_q_theta_dummy, 0, ln_p_X_theta-l_q_theta_dummy,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\nax.arrow(0.5, ln_p_X_theta, 0, l_q_theta_dummy-ln_p_X_theta,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\n\nax.set_ylim([-27, -20])\nplt.xticks([])\nplt.yticks(fontsize=15)\nplt.show()\n\nl(q,theta)    : -23.002070\nln p(X|theta) : -21.484619\n\n\n\n\n\n현재 파라미터 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)에서 dummy_q()를 \\(q(\\mathbf{Z})\\)로 하여 \\(\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}})\\)값을 계산해보면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})\\) 보다 약간 작고 그 차이가 \\(\\text{KL}(q\\,||\\,p)\\)로 나타나는 것을 확인할 수 있다.\n이제 \\(q(\\mathbf{Z})\\)를 \\(\\mathbf{Z}\\)에 대한 사후확률분포로 바꿔보자. \\(q(\\mathbf{Z}) = p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)로 설정한다.\n\n# ELBO를 계산할 때 제안하는 q(Z)를 Z의 사후확률분포로 제안한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nl_q_theta = ELBO(p_Z_given_X_and_theta_old, theta_old)\nln_p_X_theta = loglikelihood_X(theta_old, X)\nprint(\"l(q,theta)    : {:.6f}\".format(l_q_theta))\nprint(\"ln p(X|theta) : {:.6f}\".format(ln_p_X_theta))\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\n# ln p(X|theta_old)\nax.plot([0, 6], [ln_p_X_theta]*2, 'r', lw='3')\nax.text(3.8, ln_p_X_theta-margin, r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='r', fontsize=20)\n\n# L(q(Z|X, theta_old),theta_old)\nax.plot([0, 3], [l_q_theta]*2, 'b', lw='3')\nax.text(0.8, l_q_theta-margin, r\"$\\mathcal{L}(q(\\mathbf{Z}\\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}), \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\nax.arrow(0.5, l_q_theta_dummy, 0, ln_p_X_theta-l_q_theta_dummy,\n         head_width=0.1, head_length=0.2, fc='b', ec='b', length_includes_head=True)\n\n\n# L(q_dummy,theta_old)\nax.plot([0, 3], [l_q_theta_dummy]*2, 'b--', lw='3')\nax.text(0.8, l_q_theta_dummy-margin, r\"$\\mathcal{L}(q_{\\text{dummy}}, \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\n\n# KL\nax.text(0, ln_p_X_theta+margin, r\"$\\text{KL}(q || p)=0$\",  color='g', fontsize=20)\n\nax.set_ylim([-27, -20])\nplt.xticks([])\nplt.yticks(fontsize=15)\nplt.show()\n\nl(q,theta)    : -21.484619\nln p(X|theta) : -21.484619\n\n\n\n\n\n예상대로 \\(\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}})= \\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})\\)가 되며 \\(\\text{KL}(q\\,||\\,p)\\)가 사라진다.\n\n\\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 완전 데이터 세트 로그가능도 평균 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)의 관계\n이쯤에서 지금까지 이야기한 완전 데이트 세트에 대한 로그 가능도 평균과 그것을 반복적으로 최대화 시키는 과정이 식(9.70)과 어떻게 연결되는지 알아보자.\n\\(q(\\mathbf{Z})\\)를 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)로 설정하면 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 식(9.74)처럼 될 것이다.\n\\[\n\\begin{aligned}\n\\mathcal{L}(q, \\boldsymbol{\\theta})\n&= \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) - \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\\\\n&=  \\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) + \\text{const}\n\\end{aligned} \\tag{9.74}\n\\]\n놀랍게도 식(9.74)에서 첫 번째 항은 \\(\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})\\) 또는 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)가 된다. 그리고 두 번째 항은 현재 파라미터 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)에만 의존하고 \\(\\boldsymbol{\\theta}\\)와는 상관없는 상수항이다. 따라서 \\(q(\\mathbf{Z})\\)를 \\(\\mathbf{Z}\\)의 사후확률분포로 설정하고 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)을 계산하는 것이 결국 \\(\\mathbf{Z}\\)의 사후확률분포하에서 완전 데이터 세트의 로그가능도 평균을 구하는 것과 대등한 것이다.\n결국 \\(\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})\\) 또는 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 \\(\\boldsymbol{\\theta}\\)대해 최대화 하는 것이 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 최대화 하는 것이라는 사실을 알 수 있다.\n방금까지 과정이 EM 알고리즘에서 기대값을 구성하는 단계인 Expectation 단계이다.\n정리하면 우리는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화 하는 \\(\\boldsymbol{\\theta}\\)를 찾고 싶은데 \\(\\ln(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 직접 최대화하는 것은 어려우니 \\(\\mathbf{Z}\\)의 사후확률을 도입하여 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 하한인 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 최대한 키워 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)와 동일하게 만들었다.\n그리고 이때 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)는 완전 데이터 세트에 대한 가능도의 기댓값 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)와 상수로 구성되기 때문에 결과적으로 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)를 최대화 시키는 것이 곧 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 최대화 시키는 것을 알았다. 또 중요한 점은 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)이 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 하한이기 때문에 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)을 증가시키면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)는 무조건 증가할 것이라는 점이다.\n\\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 최대화 시키는 것은 이미 3편에서 알아본 것과 같다. \\(\\gamma\\)를 구해 Expectation 단계를 수행한다. \\(\\gamma\\)를 구하는 과정이 곧 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)를 제안하는 것임을 상기하자. 그렇게 제안된 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)하에서 계산된 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 직접 미분하여 얻은 식(29), (32)를 수행한다.\n이 두 과정을 묶어 EM_step()이라는 함수로 만들자.\n\ndef EM_step() :\n    \"\"\"\n    gloval variables\n    X : (N,D)\n    theta_old : (4,), (mu_1, mu_2, pi_1, pi_2)\n    n_t\n    \"\"\"\n    mu_old = theta_old[:K*D].reshape(K,D)\n    pi_old = theta_old[-K:]\n\n    Gamma = np.array([ pi_old[k]*binom.pmf(X, n_t, mu_old[k][0])\n                        for k in range(K) ]).transpose(1,0,2).squeeze()\n    Gamma /= Gamma.sum(axis=1, keepdims=True)\n\n    Nk = Gamma.sum(axis=0)\n    mu_new = ((Gamma * X).sum(axis=0) / (Nk*n_t)).reshape(-1,1)\n    pi_new = Nk / N\n\n    return mu_new, pi_new\n\n실제 EM_step()을 수행하고 새롭게 얻어진 파라미터를 theta_new로 설정한다.\n\nmu_new, pi_new = EM_step()\ntheta_new = np.hstack((mu_new.flatten(), pi_new))\n\nprint(theta_old)\nprint(theta_new)\n\n[0.4 0.3 0.5 0.5]\n[0.70729725 0.45725284 0.73085881 0.26914119]\n\n\n새롭게 계산된 파라미터 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)에 의해 증가된 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 그려보자.\n\n# ELBO를 계산할 때 제안하는 q(Z)를 Z의 사후확률분포로 제안한다.\n# theta_old가 theta_new로 업데이트 되었으나 사후분포를 구성할 때 사용된\n# theta_old는 고정된 상태이다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nl_q_theta_new = ELBO(p_Z_given_X_and_theta_old, theta_new)\nln_p_X_theta_new = loglikelihood_X(theta_new, X)\nprint(\"l(q,theta)    : {:.6f}\".format(l_q_theta_new))\nprint(\"ln p(X|theta) : {:.6f}\".format(ln_p_X_theta))\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\n# ln p(X|theta_new)\nax.plot([0, 6], [ln_p_X_theta_new]*2, 'r', lw='3')\nax.text(3.8, ln_p_X_theta_new-margin*2, r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{new}})$\",\n        color='r', fontsize=20)\nax.arrow(3.5, ln_p_X_theta, 0, ln_p_X_theta_new-ln_p_X_theta,\n         head_width=0.1, head_length=0.2, fc='r', ec='r', length_includes_head=True)\n\n# ln p(X|theta_old)\nax.plot([3, 6], [ln_p_X_theta]*2, 'r--', lw='3')\nax.text(3.8, ln_p_X_theta-margin*2, r\"$\\ln p( \\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='r', fontsize=20)\n\n# L(q(Z|X, theta_old),theta_new)\nax.plot([0, 3], [l_q_theta_new]*2, 'b', lw='3')\nax.text(0.8, l_q_theta_new-margin*2, r\"$\\mathcal{L}(q(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}), \\boldsymbol{\\theta}^{\\text{new}})$\",\n        color='b', fontsize=20)\nax.arrow(0.5, l_q_theta, 0, l_q_theta_new-l_q_theta,\n         head_width=0.1, head_length=0.2, fc='b', ec='b', length_includes_head=True)\n\n# L(q(Z|X, theta_old),theta_old)\nax.plot([0, 3], [l_q_theta]*2, 'b--', lw='3')\nax.text(0.8, l_q_theta-margin*2, r\"$\\mathcal{L}(q(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}), \\boldsymbol{\\theta}^{\\text{old}})$\",\n        color='b', fontsize=20)\n\n# KL\nax.text(0, ln_p_X_theta_new+margin, r\"$\\text{KL}(q || p)$\",  color='g', fontsize=20)\nax.arrow(0.5, l_q_theta_new, 0, ln_p_X_theta_new-l_q_theta_new,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\nax.arrow(0.5, ln_p_X_theta_new, 0, l_q_theta_new-ln_p_X_theta_new,\n         head_width=0.1, head_length=0.2, fc='g', ec='g', length_includes_head=True)\n\nax.set_ylim([-23, -11])\nplt.xticks([])\nplt.yticks(fontsize=15)\nplt.show()\n\nl(q,theta)    : -13.141270\nln p(X|theta) : -21.484619\n\n\n\n\n\n\\(\\boldsymbol{\\theta}^{\\text{old}}\\)에서 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)로 파라미터가 조정되면서 두 함수 모두 점선 위치에서 증가하였다. 증가된 정도를 보면 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{new}})\\)가 \\(\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{new}})\\)보다 조금 더 증가되었다. 그 이유는 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)에서 \\(\\boldsymbol{\\theta}^{\\text{new}}\\) 변경되면서 현재 파라미터 상태에서 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{new}})\\)는 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})\\)와 달라졌으며 그 결과 \\(KL(q \\,||\\, p)\\)는 다음과 같이 0이 아니기 때문이다.\n\\[\nKL(q \\,||\\, p) = - \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta}^{\\text{old}}) \\ln \\frac{p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta}^{\\text{new}})}{ p(\\mathbf{Z} \\mid \\mathbf{X},\\boldsymbol{\\theta}^{\\text{old}}) } \\neq 0\n\\]\n이 한번의 반복으로 원래 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)보다 더 큰 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\) 함숫값을 주는 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)를 찾아내게 되었다. 이 과정이 Maximization 과정이다.\n이제 \\(q(\\mathbf{Z})\\)를 \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{new}})\\)로 재설정하는 Expectation 과정을 반복하면 두 번째 그림 상태로 돌아가게 된다. 여기서 다시 Maximization 과정을 수행하여 조금 더 큰 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\) 함숫값을 주는 \\(\\boldsymbol{\\theta}^{\\text{new}}\\)를 구할 수 있다. 이 과정을 반복하는 동안 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)는 절대 줄어들지 않으며 결과적으로 \\(\\mathbb{E}_{\\mathbf{Z}}[\\ln (\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})]\\)를 최대화하는 과정을 반복하면서 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화하게 되는 것이다.\n이것으로 평균을 최대화 시키는 방법이 왜 유효하고 그것의 반복이 왜 수렴하는지 알 수 있게 되었다."
  },
  {
    "objectID": "posts/em/em_algorithm.html#em-알고리즘-시각화",
    "href": "posts/em/em_algorithm.html#em-알고리즘-시각화",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "EM 알고리즘 시각화",
    "text": "EM 알고리즘 시각화\n지금까지 아주 길게 완전 데이트 세트 가능도 평균의 최대화를 반복하는 과정이 \\(\\ln(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 최대화시키는 이유에 대해서 알아보았다. 이제 실제 우리가 설정한 문제에서 어떤 식으로 최대화가 일어나고 있는지 시각화하면서 이 길고 지루한 과정을 마무리해보자.\n우리가 다루고 있는 문제에서 구하고자 하는 파라미터는 총 4개이므로 이 모든 파라미터가 최적화되는 과정을 시각화 할 수 없다. 그러므로 \\(\\boldsymbol{\\theta}\\)에서 \\(\\mu_1\\)에 대해서만 \\(\\mathcal{L}(q, \\boldsymbol{\\theta})\\)와 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)의 관계를 시각화 해보자.\n현재 파라미터 상태를 적당히 설정한다.\n\nmu_old = np.array([[0.6],[0.45]])\npi_old = np.array([0.6, 0.4])\ntheta_old = np.hstack((mu_old.flatten(), pi_old))\n\n이전에 만들어 놓은 loglikelihood_X()를 이용해서 \\(\\mu_1\\)만을 변수로 하는 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 만든다.\n\ndef loglikelihood_X_mu1(mu_1, X):\n    return loglikelihood_X(np.array([mu_1, 0.45, 0.6, 0.4]), X)\n\n이제 \\(\\mu_1\\)에 따른 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)를 그려본다. 우리 목적은 국부적으로 \\(\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)가 가장 높은 곳을 찾아가는 것이다.\n\nmus = np.linspace(0, 1, 100)\n\n# ln( p(X|theta) )\nloglikelihood_X_by_mu1 = np.array([ loglikelihood_X_mu1(mu,  X)\n                                    for mu in mus ])\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\n\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\n\nplt.show()\n\n\n\n\n그림을 보면 0.8x 정도에서 최대점이 형성되는 것을 알 수 있다.\n완전 데이터 집합의 로그가능도 평균도 \\(\\mu_1\\)만의 함수로 다시 만든다.\n\ndef E_Z_loglikelihood_XZ_mu1(mu_1, X):\n    # mu_1을 제외한 나머지 파라미터들은 이미 최적화된 것으로 가정하고 고정\n    return E_Z_loglikelihood_XZ(np.array([mu_1, 0.45, 0.6, 0.4]), X)\n\n이제 현재 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)를 조건으로 하는 사후확률분포를 이용하여 전체 \\(\\mu_1\\)에 대해서 \\(\\mathcal{L}(q, \\mu_1)\\)을 그려본다. 이때 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)도 함께 그려 상황을 확인해보자.\n\nmus = np.linspace(0, 1, 100)\n\n# 현재 파라미터로 Z의 사후확률 q(Z|X,mu_1^{old})를 세팅한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\n# L(q,mu_1)를 mu_1에 대해서 다시 그린다.\nl_q_theta = np.array([ E_Z_loglikelihood_XZ_mu1(mu, X)\n                        - entropy_Z(p_Z_given_X_and_theta_old) for mu in mus ])\n\n# 현재 \\mu_1^{old}에서 L(q,mu_1) 값을 계산한다.\nl_q_theta_0 = E_Z_loglikelihood_XZ_mu1(mu_old[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_old[0,0], mu_old[0,0]], [-23,-10], 'k--')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n그림에서 확인할 수 있듯이 현재 파리미터에서 \\(\\mathcal{L}(q, \\mu_1^{\\text{old}}=0.6)\\)과 \\(\\ln p(\\mathbf{X} \\mid \\mu^{\\text{old}}_1=0.6)\\)가 같아지는 모습을 확인할 수 있다. 이제 이 상태에서 EM 스탭을 한번 반복한다.\n\n# EM_step으로 새로운 파라미터를 얻는다.\nmu_new, pi_new = EM_step()\n\n# 업데이트된 mu_1^{new}에서 L(q,mu_1) 값을 계산한다.\nmax_l_q_theta = E_Z_loglikelihood_XZ_mu1(mu_new[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_new[0,0], mu_new[0,0]], [-23,-10], 'k--')\nax.plot(mu_new[0,0], max_l_q_theta, 'ro')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\nEM_step() 결과 \\(\\mu_1^{\\text{old}}\\)가 \\(\\mathcal{L}(q, \\mu_1)\\)의 최대점으로 이동한 것을 확인할 수 있다. 파란 점보다 빨간 점이 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)의 최대점에 조금 더 가까워졌다. 새롭게 구해진 파라미터를 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)로 업데이트하고 \\(\\boldsymbol{\\theta}^{\\text{old}}\\)를 조건으로 다시 \\(\\mathbf{Z}\\)의 사후확률분포를 계산한다.\n\nmu_old = mu_new\npi_old = pi_new\ntheta_old = np.hstack((mu_old.flatten(), pi_old))\n\n# 현재 파라미터로 Z의 사후확률을 세팅한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nprint(mu_old)\nprint(pi_old)\n\n[[0.7428154 ]\n [0.43070022]]\n[0.67058506 0.32941494]\n\n\n새롭게 구성된 사후확률하에서 \\(\\mathcal{L}(q, \\mu_1)\\)을 다시 그려보면 \\(\\mu_1^{\\text{old}}=0.743\\)에서 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)과 접할 것이다.\n\nl_q_theta = np.array([ E_Z_loglikelihood_XZ_mu1(mu, X)\n                       - entropy_Z(p_Z_given_X_and_theta_old) for mu in mus ])\n\nl_q_theta_0 = E_Z_loglikelihood_XZ_mu1(mu_old[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_old[0,0], mu_old[0,0]], [-23,-10], 'k--')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n예상대로 새롭게 구성된 \\(\\mathcal{L}(q, \\mu_1)\\)이 \\(\\mu_1=0.743\\)에서 접하는 것을 확인할 수 있다. 이제 다시 EM_step()를 실행한다.\n\nmu_new, pi_new = EM_step()\nmax_l_q_theta = E_Z_loglikelihood_XZ_mu1(mu_new[0,0], X) \\\n                  - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_new[0,0], mu_new[0,0]], [-23,-10], 'k--')\nax.plot(mu_new[0,0], max_l_q_theta, 'ro')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n다시 \\(\\mathcal{L}(q, \\mu_1)\\)의 최대값으로 파라미터가 업데이트 된다. 이런 과정을 계속 반복하면서 \\(\\ln p(\\mathbf{X} \\mid \\mu_1)\\)의 국부 최대점으로 수렴하게 되는 것이다.\n\nmu_old = mu_new\npi_old = pi_new\ntheta_old = np.hstack((mu_old.flatten(), pi_old))\n\n# 현재 파라미터로 Z의 사후확률을 세팅한다.\np_Z_given_X_and_theta_old = set_posterior_Z(X, theta_old)\n\nprint(mu_old)\nprint(pi_old)\n\n[[0.81242457]\n [0.37383545]]\n[0.60686538 0.39313462]\n\n\n\nl_q_theta = np.array([ E_Z_loglikelihood_XZ_mu1(mu, X)\n                       - entropy_Z(p_Z_given_X_and_theta_old) for mu in mus ])\n\nl_q_theta_0 = E_Z_loglikelihood_XZ_mu1(mu_old[0,0], X) \\\n                - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_old[0,0], mu_old[0,0]], [-23,-10], 'k--')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n\nmu_new, pi_new = EM_step()\nmax_l_q_theta = E_Z_loglikelihood_XZ_mu1(mu_new[0,0], X) \\\n                  - entropy_Z(p_Z_given_X_and_theta_old)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(mus, loglikelihood_X_by_mu1, lw=2,\n        label=r\"$\\ln p(\\mathbf{X} \\mid \\mu_1)$\")\nax.plot(mus, l_q_theta, label=r\"$\\mathcal{L}(q, \\mu_1)$\")\nax.plot([mu_new[0,0], mu_new[0,0]], [-23,-10], 'k--')\nax.plot(mu_new[0,0], max_l_q_theta, 'ro')\nax.plot(mu_old[0,0], l_q_theta_0, 'bo')\nax.set_ylim([-23, -10])\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel(r'$\\mu_1$', fontsize=20)\nplt.show()\n\n\n\n\n이렇게 나름 복잡하다면 복잡한 EM 알고리즘을 알아봤다. 소설처럼 술술 읽히는 글은 아닐지라도 EM 알고리즘을 어렴풋이 이해하고 있는 분들께는 나름 도움이 될 것이라는 믿음으로 글을 마무리한다."
  },
  {
    "objectID": "posts/em/em_algorithm.html",
    "href": "posts/em/em_algorithm.html",
    "title": "A Step by Step Introduction to EM Algorithm",
    "section": "",
    "text": "# 글 전체에서 필요한 모듈을 임포트한다.\nimport numpy as np\nimport itertools\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# 구글 코랩에서 그래프에 LaTeX를 원활히 쓰기 위한 설정으로 코랩이 아니면 실행 안함\nmatplotlib.rc('text', usetex=True)\nmatplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng &gt; /dev/null"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#필요-패키지-설치",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#필요-패키지-설치",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "시작하기전에 필요한 라이브러리를 설치한다. 본인 컴퓨터에 이미 관련 라이브러리가 설치되어 있다면 설치하지 않아도 된다.\n먼저 허깅페이스의 트랜스포머스 라이브러리를 설치한다.\n\n!pip install transformers\n\n그 다음은 데이터 셋을 다운받는기 위해 다음 명령을 실행해서 허깅페이스 Datasets 라이브러리를 설치한다.\n\n!pip install datasets\n\n그리고 T5 모델의 토크나이저가 sentencepiece를 사용하므로 다음을 실행해서 설치한다.\n\n!pip install sentencepiece\n\n또 모델 평가를 위해 허깅페이스 evaluate 라이브러리와 BLEU 점수를 계산하기위해 sacrebleu를 설치한다.\n\n!pip install evaluate\n\n\n!pip install sacrebleu"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#data-set",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#data-set",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "모두 설치가 완료되었다면 데이터 셋을 다운받아야 한다. 먼저 허깅페이스 사이트에 접속해서 상단 메뉴에 Datasets를 클릭하고 아래처럼 검색 조건을 맞추면\n\n좌측 작은 메뉴에서 Languages를 선택한다.\nLanguages 하단에 보이는 여러 언어중에 Korean을 선택한다.\n다시 우측 검색 필터 창에 en을 적는다.\n\n데이터 셋 네 개가 보이는데 이 중에서 bongsoo/news_talk_en_ko를 사용하도록 하겠다.\nbongsoo/news_talk_en_ko를 클릭해서 나오는 화면에서 Files and Versions를 클릭하면 tsv 파일이 보이는데 이 파일에는 영어 문장과 한국어 문장이 한줄에 탭 문자로 구분되어 적혀있다. 로컬 디스크이 이 파일을 다운받고 파일을 읽어보면 다음처럼 확인된다.\n\n[노트] 로컬 또는 코랩 런타임에 파일을 다운 받지 않았다면 굳이 다운받을 필요없고 이 셀은 스킵하자. 그냥 데이터 파일 한줄에 영어 문장과 짝이 되는 한국어 문장이 탭문자로 구분되어 있다는 것만 알면 된다. 실제 데이터를 가져올 때는 허깅페이스를 통해 다운 받게 된다.)\n\n\n!head -5 news_talk_en_ko_train_130000.tsv\n\nSkinner's reward is mostly eye-watering.    스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\nEven some problems can be predicted.    심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\nOnly God will exactly know why. 오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\nBusinesses should not overlook China's dispute. 중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\nSlow-beating songs often float over time.   박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n데이터 파일은 아주 단순한 형태인 것을 알 수 있다. 직접 tsv파일을 다운받아서 사용해도 되나 허깅페이스 허브로 부터 바로 다운받아 사용하는 편이 더 편하다. 다음 명령으로 다운받을 수 있다.\n\n# 데이터 셋을 다운받을 함수를 임포트 한다.\nfrom datasets import load_dataset\n\n\n# 좀 전에 알아본 체크포인트를 사용해서 데이터를 받아온다.\nen_ko = load_dataset(\"bongsoo/news_talk_en_ko\")\n\nUsing custom data configuration bongsoo--news_talk_en_ko-e7f00bc8f76f18d5\nFound cached dataset csv (/home/metamath/.cache/huggingface/datasets/bongsoo___csv/bongsoo--news_talk_en_ko-e7f00bc8f76f18d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n\n\n\n\n\n이제 데이터 객체를 확인해보면 DatasetDict라는 것을 알 수 있고 안에 train 키만 있는 것이 확인된다.\n\nen_ko\n\nDatasetDict({\n    train: Dataset({\n        features: [\"Skinner's reward is mostly eye-watering.\", '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.'],\n        num_rows: 1299999\n    })\n})\n\n\ntrain키에 Dataset 객체가 하나 있는데 features가 첫번째 데이터로 되어있고 행수는 1299999개인 것을 보아 데이터 파일에 컬럼명이 적혀있는 헤더라인이 없어서 첫줄을 헤더로 읽은것 같다. 첫줄을 데이터로 다시 집어 넣고 컬럼명은 en, ko로 설정하기 위해 데이터 셋을 pandas로 읽어드린다.\n\nimport pandas as pd\n\n\n# 허깅페이스 데이터셋을 판다스 포맷으로 세팅\nen_ko.set_format(type=\"pandas\")\n\n\n# 'train'키의 모든 행을 DataFrame df에 할당\ndf = en_ko[\"train\"][:]\n\n# 잘 담겼는지 확인한다.\ndf.head()\n\n\n\n\n\n\n\n\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n\n\n0\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n1\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n2\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n3\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n4\nI can't even consider uninsured treatments.\n보험 처리가 안 되는 비급여 시술은 엄두도 못 낸다.\n\n\n\n\n\n\n\n예상처럼 첫 줄이 헤더가 되었으니 이를 수정한 DataFrame을 만든다.\n\nexample_0 = list(df.columns)\nexample_0\n\n[\"Skinner's reward is mostly eye-watering.\",\n '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.']\n\n\n적당히 조작해서 컬럼명이 en, ko가 되게 하고 example_0가 첫 행이 되도록 만든다.\n\nexample_0_df = pd.DataFrame({col: [value] for col, value in zip(('en', 'ko'), example_0)})\n\n\ndf.columns = ('en', 'ko')\n\n\nen_ko_df = pd.concat([example_0_df, df],).reset_index(drop=True)\nen_ko_df.head()\n\n\n\n\n\n\n\n\nen\nko\n\n\n\n\n0\nSkinner's reward is mostly eye-watering.\n스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n\n\n1\nEven some problems can be predicted.\n심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n\n\n2\nOnly God will exactly know why.\n오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n\n\n3\nBusinesses should not overlook China's dispute.\n중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n\n\n4\nSlow-beating songs often float over time.\n박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n\n\n\n\n\n\n\n이렇게 데이터 셋을 DataFrame으로 만들었다. 이제 이 en_ko_df로 부터 다시 허깅페이스 데이터 셋을 생성하자.\n\nfrom datasets import Dataset\n\n\ndataset = Dataset.from_pandas(en_ko_df)\n\n\ndataset\n\nDataset({\n    features: ['en', 'ko'],\n    num_rows: 1300000\n})\n\n\n다시 데이터 셋을 확인해보면 features가 제대로 표시되고 샘플 수도 1300000개 인것을 확인할 수 있다.\n이렇게 만들어진 DataFrame으로 부터 데이터 셋이 잘 초기화되는 것을 확인했으니 en_ko_df를 세조각으로 쪼개서 tsv파일로 저장하자.\n\n# 각 데이터 셋의 샘플수를 정한다.\nnum_train = 1200000\nnum_valid = 90000\nnum_test = 10000\n\n설정된 크기만큼 DataFrame을 자른다.\n\nen_ko_df_train = en_ko_df.iloc[:num_train]\n\n\nen_ko_df_valid = en_ko_df.iloc[num_train:num_train+num_valid]\n\n\nen_ko_df_test = en_ko_df.iloc[-num_test:]\n\n다시 tsv파일로 저장한다.\n\nen_ko_df_train.to_csv(\"train.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_valid.to_csv(\"valid.tsv\", sep='\\t', index=False)\n\n\nen_ko_df_test.to_csv(\"test.tsv\", sep='\\t', index=False)\n\n이렇게 tsv파일 세개로 데이터를 정리했다. 이제 필요할때 이 파일을 읽어 허깅페이스 데이터셋을 만들 수 있다.\n아래처럼 스플릿을 정의한 사전을 load_dataset에 넘기면 된다. 이때 delimiter를 탭 문자로 지정해야 한다.\n\ndata_files = {\"train\": \"train.tsv\", \"valid\": \"valid.tsv\", \"test\": \"test.tsv\"}\n\n\ndataset =  load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n\nUsing custom data configuration default-02a3611b1810efcd\n\n\nDownloading and preparing dataset csv/default to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\nDataset csv downloaded and prepared to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n제대로 로딩되었는지 dataset을 확인해보자.\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['en', 'ko'],\n        num_rows: 10000\n    })\n})\n\n\nDatasetDict에 train, valid, test 키로 120만 문장, 9만 문장, 1만 문장이 저장된 것을 확인할 수 있다.\n이 데이터 셋에서 개별 샘플에 대한 접근은 [split][feature][row num] 형태로 가능하다.\n\n# train 스플릿에서 영어 3개와 한국어 3개 샘플을 가져온다.\nprint(dataset['train']['en'][:3], dataset['train']['ko'][:3])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n그런데 feature와 row num은 순서를 바꿔서 사용할 수 도 있다.\n\nprint(dataset['train'][:3]['en'], dataset['train'][:3]['ko'])\n\n[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n\n\n데이터를 어떻게 조회하는지는 데이터 구성 방식에 따라 조금씩 다르므로 데이터 셋을 보고 몇번 해보면 금방 접근법을 알 수 있다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#hugging-face",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#hugging-face",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "데이터 셋 준비를 마쳤으니 학습할 차례이다. 허깅페이스에서 제공하는 필요 클래스를 임포트 한다.\n먼저 선학습 모델을 사용하기 위한 클래스를 임포트 한다. AutoTokenizer는 선학습된 모델이 사용한 토크나이저를 읽기 위해 필요하며 AutoModelForSeq2SeqLM은 시퀀스 투 스퀀스 방식으로 작동하는 선학습된 모델을 불러 올 때 마지막에 분류기 헤드를 붙여서 모델을 로딩하기 위해 사용한다.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n2023-03-01 16:05:02.191320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-01 16:05:02.266647: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-03-01 16:05:02.281905: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-01 16:05:02.592853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n2023-03-01 16:05:02.592898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n\n다음은 데이터 콜레이터를 임포트한다. 시쿼스 투 시퀀스 학습 과정은 인코더 입력 시퀀스, 디코더 입력 시퀀스, 디코더 출력 시퀀스를 필요로 하는데 미니배치로 부터 이를 적절히 정리해서 모델에 입력하는 작업이 필요하다. 예를 들면 미니 배치 내에 있는 인코더 입력 시퀀스의 길이를 맞춘다든지 디코더 입력시퀀스를 오른쪽으로 한칸 쉬프트시켜 디코더 출력 시퀀스를 만드는 작업등이 콜레이터에서 일어나는 작업인데 이런 작업을 DataCollatorForSeq2Seq가 자동으로 처리하게 된다.\n\nfrom transformers import DataCollatorForSeq2Seq\n\n그리고 학습에 필요한 클래스를 임포트 한다. 학습에 필요한 설정을 Seq2SeqTrainingArguments에 정의하고 실제 학습은 Seq2SeqTrainer로 하게 된다. Seq2SeqTrainer는 generate()함수를 제공한다.\n\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n허깅페이스 라이브러리로는 마지막으로 데이터 셋을 로딩하는 함수와 번역 결과를 측정할 함수를 로딩한다.\n\nfrom datasets import load_dataset, load_metric\n\n그외 필요한 각종 라이브러리를 임포트 한다.\n\nimport numpy as np\nimport torch\nimport multiprocessing\n\n허깅페이스에서 파이토치 기반 구현을 사용하므로 gpu가 있다면 device를 세팅한다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\n미리 학습된 모델의 체크포인트를 세팅한다. 여기서 사용할 모델은 한국어와 영어에 미리 학습된 KE-T5모델을 사용한다. T5모델은 트랜스포머의 인코더, 디코더 구조를 모두 사용하는 모델로 번역기를 만들 때 사용할 수 있는 모델이다. 아래처럼 모델 체크 포인트와 T5 모델에 입력될 최대 토큰 길이를 설정한다.\n\nmodel_ckpt = \"KETI-AIR/ke-t5-base\"\nmax_token_length = 64"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#tokenizer",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#tokenizer",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "먼저 모델 체크 포인트를 사용하여 KE-T5 모델이 학습할때 함께 사용한 토크나이저를 불러온다. 허깅페이스 트랜스포머스 라이브러리를 사용할 때 가장 핵심이 되며 익숙해지기 쉽지 않은 부분이 이 토크나이저라고 개인적으로 생각한다.\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n\n토크나이저를 로딩할때 sentencepiece가 없다고 에러가 나면 위 제시한 라이브러리가 설치 안된 것이므로 설치하고 다시 시도한다.\n토크나이저를 불러왔으니 현재 사용하는 데이터셋에서 샘플을 가져와 토크나이징해보는 것이 좋을 것이다. 학습 세트에서 10번 샘플을 가지고 실험해보자. 먼저 10번 샘플을 뿌려보고\n\ndataset['train'][10]['en'], dataset['train'][10]['ko']\n\n('Any academic achievement requires constant repetition.',\n '어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.')\n\n\n토크나이저에 각 문장을 입력하고 토큰화된 상태로 돌려 받는다.\n\ntokenized_sample_en = tokenizer(dataset['train'][10]['en'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_en\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenized_sample_ko = tokenizer(dataset['train'][10]['ko'], \n                                max_length=max_token_length, \n                                padding=True, truncation=True)\ntokenized_sample_ko\n\n{'input_ids': [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n문장에 토큰으로 쪼개지고 각 토큰이 숫자로 변환된 것을 볼 수 있다. 이렇게 숫자화된 토큰을 input_ids로 반환하고 추가로 트랜스포머 인코더, 디코더에 쓰일 패딩 마스크도 함께 attention_mask로 돌려준다. 마스크가 모두 1인 이유는 샘플이 하나밖에 없어서 이다. 샘플 몇개를 더 실험해보면\n\ntokenizer(dataset['train'][:3]['en'], \n          max_length=max_token_length, \n          padding=True, truncation=True)\n\n{'input_ids': [[388, 6809, 2952, 17, 8, 32204, 43, 8023, 6687, 28, 9495, 91, 3, 1], [4014, 322, 3170, 147, 67, 23274, 3, 1, 0, 0, 0, 0, 0, 0], [11783, 4412, 96, 6556, 709, 1632, 3, 1, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}\n\n\n미니배치에 있는 샘플의 최대길이메 맞춰서 패딩되는 모습을 확인할 수 있다. 실제로 어떻게 토큰화 되었는지 확인해보자.\n\npd.DataFrame(\n    [\n        tokenized_sample_en['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_en['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nids\n13941\n10114\n25542\n9361\n20526\n742\n32268\n12520\n3\n1\n\n\ntokens\n▁Any\n▁academic\n▁achievement\n▁requires\n▁constant\n▁re\npet\nition\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\n\npd.DataFrame(\n    [\n        tokenized_sample_ko['input_ids'],\n        tokenizer.convert_ids_to_tokens(tokenized_sample_ko['input_ids'])\n    ], index=('ids', 'tokens')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nids\n404\n12663\n15\n10775\n2334\n6\n15757\n21\n29819\n1736\n26778\n4342\n15\n1701\n3\n1\n\n\ntokens\n▁어떤\n▁학문\n이\n든지\n▁일정\n의\n▁성취\n를\n▁이루기\n▁위해서는\n▁끊임없는\n▁반복\n이\n▁필요하다\n.\n&lt;/s&gt;\n\n\n\n\n\n\n\nKE-T5를 학습할때 학습된 규칙대로 토큰화가 진행된다. 영어에서 repetition은 re, pet, ition으로 쪼개진 것을 볼 수 있고, 한국어에서 성취를은 성취, 를로 쪼개지고 학문이든지는 학문, 이, 든지로 쪼개진것을 볼 수 있다. 토큰 앞에 _표시는 이 토큰 앞에는 공백이 있어야 한다는 의미다. 그리고 마지막에 엔드 토큰인 &lt;/s&gt;가 항상 붙게 되는 것도 확인할 수 있다.\n이제 앞서 tsv파일로 부터 로딩한 dataset내의 문장을 모두 토크나이저를 사용해서 숫자로 바꾸는 작업을 해야 한다. 즉 문자로된 문장을 숫자로 바꿔 특성화 해야 한다. dataset.map()함수에 각 샘플을 토큰화 하는 함수를 만들어 전달하면 map()이 모든 샘플에 대해 전달받은 함수를 적용하게 되는데 함수는 이렇게 작성하면 된다.\n\ndef convert_examples_to_features(examples):\n    ###########################################################################\n    # with 쓰는 옛날 방식\n    # input_encodings = tokenizer(examples['en'], \n    #                             max_length=max_token_length, truncation=True)\n    \n    # Setup the tokenizer for targets\n    # with tokenizer.as_target_tokenizer():\n    # target_encodings = tokenizer(text_target=examples['ko'], \n    #                             max_length=max_token_length, truncation=True)\n    #\n    #\n    # return {\n    #     \"input_ids\": input_encodings[\"input_ids\"],\n    #     \"attention_mask\": input_encodings[\"attention_mask\"],\n    #     \"labels\": target_encodings[\"input_ids\"]\n    # }\n    \n    # 그런데 이렇게 하면 인풋하고 한번에 처리 가능함.\n    model_inputs = tokenizer(examples['en'],\n                             text_target=examples['ko'], \n                             max_length=max_token_length, truncation=True)\n    \n    return model_inputs\n\nconvert_examples_to_features()가 하고 싶은 일은 dataset에 있는 “어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.”라는 샘플 문장을 [404,12663,15,10775,2334,6,15757,21,29819,1736,26778,4342,15,1701,3,1]라는 정수로 바꾸는 것이다. convert_examples_to_features()가 dataset에 적용될 때 넘겨 받는 examples는 다음과 같이 넘어 온다.\nexamples= {'en':['sent1', 'sent2', ... , 'sent1000'], # 이건 문장 1000개짜리 리스트\n           'ko':['sent1', 'sent2', ... , 'sent1000']}\n기본으로 미니 배치 사이즈는 1000으로 세팅되어 있다.(함수 기본인자는 여기서 확인 가능)\n미니 배치로 넘어온 문장 샘플을 영어 문장과 한국어 문장을 각각 인풋과 타겟으로 토큰화하고 이로 부터 input_ids, attention_mask, labels로 묶어 리턴하는 방식이 예전에 쓰던 방식으로 함수 위쪽에 주석처리 되어 있다. 타겟 문장을 토큰화 할 때 타겟에서 필요로 하는 특수 토큰을 추가하는 경우 이를 처리하기위해 타겟 토큰 토큰화 때는 with tokenizer.as_target_tokenizer():라는 컨텍스트 매니저를 사용했는데 최근 업데이트에서는 그냥 tokenizer에 text_target인자에 타겟 문장을 넣어서 한번에 다 처리할 수 있다. 이렇게 model_inputs을 반환하면 dataset에 있던 각 레코드 마다 en, ko 특성에 추가로 input_ids, attention_mask, labels 특성이 더 추가 되게 된다. 사실 en, ko 특성은 더이상 필요없기 때문에 convert_examples_to_features()를 적용할 때 없애라는 인자를 세팅한다.\n바로 dataset에 함수를 적용해보자. 그냥 해도되나 좀 더 빠르게 하기 위해 num_proc 인자에 스레드 개수를 지정한다.\n\nNUM_CPU = multiprocessing.cpu_count() \nNUM_CPU\n\n20\n\n\n그리고 remove_columns 인자에 기존 특성 이름인 en, ko를 전달해서 기존 특성은 제거하게 한다. 이 특성이 있으면 이후 콜레이터가 샘플들을 미니 배치로 묶을 때 패딩처리를 못하게 된다.\n\ntokenized_datasets = dataset.map(convert_examples_to_features, \n                                 batched=True, \n                                 # 이걸 쓰지 않으면 원 데이터 'en', 'ko'가 남아서\n                                 # 아래서 콜레이터가 패딩을 못해서 에러남\n                                 remove_columns=dataset[\"train\"].column_names,\n                                 num_proc=NUM_CPU) \n\n\n[노트] dataset.map()이 실행되면서 출력되는 출력은 생략됨\n\nconvert_examples_to_features()이 dataset의 모든 샘플에 다 적용되고 나면 tokenized_datasets는 다음처럼 된다.\n\ntokenized_datasets\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1200000\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 90000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 10000\n    })\n})\n\n\n기존에 있던 특성 en, ko는 사라졌고 en은 input_ids와 attention_mask로 ko는 labels로 바뀐것을 확인할 수 있다. 예를 들어 학습 세트에 10번 데이터를 보면 다음처럼 다 숫자라 바뀌게 된것이다.\n\ntokenized_datasets['train'][10]\n\n{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'labels': [404,\n  12663,\n  15,\n  10775,\n  2334,\n  6,\n  15757,\n  21,\n  29819,\n  1736,\n  26778,\n  4342,\n  15,\n  1701,\n  3,\n  1]}\n\n\n토크나이저를 써서 숫자로 부터 토큰화 해보면 다음과 같다.\n\nprint( '원 데이터    :', dataset['train'][10]['en'] )\nprint( '처리 후 데이터:', tokenized_datasets['train'][10]['input_ids'] )\nprint( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['input_ids']) )\n\nprint('\\n')\nprint( '원 데이터    :', dataset['train'][10]['ko'] )\nprint( '처리 후 데이터:', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['labels']) )\nprint( '토큰화       :', tokenized_datasets['train'][10]['labels'] )\n\n원 데이터    : Any academic achievement requires constant repetition.\n처리 후 데이터: [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1]\n토큰화       : ['▁Any', '▁academic', '▁achievement', '▁requires', '▁constant', '▁re', 'pet', 'ition', '.', '&lt;/s&gt;']\n\n\n원 데이터    : 어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.\n처리 후 데이터: ['▁어떤', '▁학문', '이', '든지', '▁일정', '의', '▁성취', '를', '▁이루기', '▁위해서는', '▁끊임없는', '▁반복', '이', '▁필요하다', '.', '&lt;/s&gt;']\n토큰화       : [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1]\n\n\n데이터 특성화를 모두 마쳤으므로 이제 모델을 로딩하자. AutoModelForSeq2SeqLM를 사용해서 선학습 모델을 불러오면 선학습된 T5모델 마지막에 파인튜닝할 수 있는 분류 헤드를 붙인 모델을 반환한다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#model",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#model",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n위처럼 모델을 로딩하고 모델 출력 시켜보면 T5 모델 레이어가 매우 길게 출력되는데 제일 마지막 부분에 다음과 같이 분류 헤드가 붙어 있는 것을 확인할 수 있다. 헤드를 보면 모델에서 출력하는 벡터는 768차원이고 이를 단어장 사이즈인 64128로 변환시키고 있는 것을 알 수 있다.\n(lm_head): Linear(in_features=768, out_features=64128, bias=False)\n이렇게 생성된 model은 인코더-디코더 구조를 가지는 트랜스포머이므로 이 모델을 포워딩 하려면 인코더 인풋과 디코더 인풋을 넣어줘야 한다. 모델을 만들고 가장 먼저해야되는 작업은 포워딩 테스트라고 개인적으로 생각한다. 임의의 입력을 넣고 출력이 의도대로 나오는지 확인하는 것이다. 이런 작업은 직접 만든 모델이 아닐 수록 중요한데 이렇게 해야지 모델이 제대로 작동하는지 또 어떤 구조로 되어 있는지 쉽게 이해할 수 있기 때문이다. 포워드 테스트를 하기위해 간단한 영어문장으로 예제를 준비한다.\n\nencoder_inputs = tokenizer(\n    [\"Studies have been shown that owning a dog is good for you\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\ndecoder_targets = tokenizer(\n    [\"개를 키우는 것이 건강에 좋다는 연구 결과가 있습니다.\"], \n    return_tensors=\"pt\"\n)['input_ids'].to(device)\n\n영어 문장은 인코더의 입력이 되고 한국어 문장은 디코더의 타겟이 된다. 아래처럼 모두 숫자로 변환되어 있다.\n\nprint( encoder_inputs )\nprint( decoder_targets )\n\ntensor([[24611,    84,   166,  8135,    38,   847,    91,    16,  8146,    43,\n           667,    40,   106,     1]], device='cuda:0')\ntensor([[15833, 12236,   179, 16120, 28117,  1007,  3883,   327,     3,     1]],\n       device='cuda:0')\n\n\n이제 디코더 입력을 만들기위해 model._shift_right를 사용해 디코더 출력을 오른쪽으로 쉬프트 시킨다.\n\ndecoder_inputs = model._shift_right(decoder_targets)\n\ndecoder_inputs와 decoder_targets이 어떻게 다른지 비교해보면\n\npd.DataFrame(\n    [\n        tokenizer.convert_ids_to_tokens(decoder_targets[0]),\n        tokenizer.convert_ids_to_tokens(decoder_inputs[0])\n    ],\n    index=('decoder target', 'decoder input')\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\ndecoder target\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n&lt;/s&gt;\n\n\ndecoder input\n&lt;pad&gt;\n▁개를\n▁키우는\n▁것이\n▁건강에\n▁좋다는\n▁연구\n▁결과가\n▁있습니다\n.\n\n\n\n\n\n\n\n위처럼 오른쪽으로 쉬프트된 디코더 입력은 &lt;pad&gt; 토큰이 추가되었다. 이렇게 출력으로 쓰이는 문장을 오른쪽으로 쉬프트시켜 티처포싱Teacher forcing을 진행하게 된다. 다음처럼 model에 인코더 입력, 디코더 입력, 디코더 타겟을 입력하고 포워드 시킨다.\n\n# forward pass\noutputs = model(input_ids=encoder_inputs, \n                decoder_input_ids=decoder_inputs, \n                labels=decoder_targets)\n\nmodel의 outputs에는 다음과 같은 키가 있다.\n\noutputs.keys()\n\nodict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])\n\n\n손실함수 값을 다음처럼 확인할 수 있고 grad_fn이 있기 때문에 output.loss를 백워드 시킬 수 있다.\n\noutputs.loss\n\ntensor(87.8185, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n\n\n인코더의 마지막 상태는 (1, 14, 768)이다. 각 숫자는 순서대로 샘플 수, 스탭 수, 모델 사이즈를 나타낸다. 즉 인코더로 들어가는 14개 토큰이 각각 768차원 벡터로 인코딩되었다.\n\noutputs['encoder_last_hidden_state'].shape\n\ntorch.Size([1, 14, 768])\n\n\nlogit은 디코더 입력 토큰 10개에 대한 그 다음 토큰 예측 10개를 담고있다. 샘플 한개에 대해서 10개 토큰에 대해서 64128개 단어에 대한 확률값이 들어 있다.\n\noutputs['logits'].shape\n\ntorch.Size([1, 10, 64128])\n\n\nlogit에 argmax를 씌워서 토큰화시켜보면 다음과 같다.\n\ntokenizer.convert_ids_to_tokens( torch.argmax(outputs['logits'][0], axis=1).cpu().numpy() )\n\n['큐브', '큐브', '▁비일비재', '▁비일비재', '▁베네', '▁비일비재', '▁베네', '▁베네', '큐브', '큐브']\n\n\n마지막 헤더가 학습이 되지 않았기 때문에 적절한 아웃풋이 나오지 않지만 입력과 출력의 텐서 모양을 보면 포워드 패스가 제대로 작동한다는 것을 알 수 있다.\n지금까지 데이터 셋, 토크나이저, 모델에 대해서 알아봤다. 이제 학습을 위해 두 단계가 남았는데 하나는 데이터를 미니배치 형태로 모아 주는 콜레이터collator와 나머지 하나는 모델을 평가할 매트릭이다"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#collator",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#collator",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "파이토치에서 모델을 학습시키기 위해서 DataLoader를 사용하게 되는데 이 데이터 로더의 역할은 for 루프를 돌면서 데이터 셋으로 부터 샘플을 미니 배치 수만큼 가져오는 것이다. 이때 샘플을 미니 배치 수만큼 무작위로 가져와 어떤 식으로든 각 샘플을 짝맞춤해서 반환해야하는데 크기가 통일된 간단한 이미지 데이터인 경우 특별히 할것이 없지만 서로 크기가 다른 샘플들을 다루는 경우는 반환전 크기 또는 길이를 맞춘다든지 패딩을 한다든지 하는 추가 작업이 필요하게 된다. 이런 작업이 일어나는 곳이 collate_fn으로 지정되는 함수이다.\n시퀀스 투 시퀀스 모델을 학습시킬때 이런 콜레이터 함수가 하는 전형적인 역할은 입력 또는 출력 문자열을 패딩하고 조금 전 모델에서 알아봤듯이 디코더 타겟을 오른쪽으로 한칸 쉬프트 시켜서 디코더 입력으로 만드는 일이다. 앞서 이런 과정을 간단히기 직접 코딩해서 확인했지만 이런 작업을 자동으로 처리해주는 클래스가 DataCollatorForSeq2Seq이다.\n우선 콜레이터를 만들기 위해서는 토크나이저와 모델을 넘겨야 한다.\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n앞서 만들어논 tokenized_datasets에서 샘플 두개를 조회하면 다음처럼 전체 결과는 사전으로 리턴되며 사전의 각 키 아래에 여러 샘플들의 값이 리스트로 들어있게 된다.\n\n# 각 항목아래 샘플들이 리스트 형태로 묶여 반환된다.\ntokenized_datasets[\"train\"][1:3]\n\n{'input_ids': [[4014, 322, 3170, 147, 67, 23274, 3, 1],\n  [11783, 4412, 96, 6556, 709, 1632, 3, 1]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n 'labels': [[6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n  [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]]}\n\n\n콜레이터에 샘플을 넘길 때는 개별 샘플이 사전으로 묶이는 형태가 되어야 되므로 아래처럼 한번 가공하게 된다.\n\n# 콜레이터에는 샘플을 개별 {}로 넘겨야 됨\n[tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n\n[{'input_ids': [4014, 322, 3170, 147, 67, 23274, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1]},\n {'input_ids': [11783, 4412, 96, 6556, 709, 1632, 3, 1],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n  'labels': [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]}]\n\n\n위에 반환된 결과를 보면 각 샘플이 사전 {}으로 묶이고 샘플 하나에는 input_ids, attention_mask, labels이 존재한다. 각 샘플을 리스트로 묶어서 콜레이터에게 전달하고 반환되는 값을 확인해보자.\n\n# 콜레이터를 돌리면 알아서 패딩하고 쉬프트 시킨다.\nbatch = data_collator(\n    [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n)\n\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n반환된 batch의 키를 확인해보면 decoder_input_ids가 생긴것을 확인할 수 있다.\n\nbatch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\nbatch의 각 키에 어떤 값들이 들어있는지 확인해보자.\n\nbatch\n\n{'input_ids': tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n        [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,    15,\n          1587,     3,     1],\n        [ 9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,  2255,\n             3,     1,  -100]]), 'decoder_input_ids': tensor([[    0,  6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,\n            15,  1587,     3],\n        [    0,  9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,\n          2255,     3,     1]])}\n\n\n출력된 batch를 정리하면 아래처럼 된다.\n{\n    'input_ids': \n        tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n                [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), \n    'attention_mask': \n        tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1, 1, 1, 1]]), \n    'labels': \n        tensor(\n            [[ 6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n             [ 9881,18590,3837,70,4341,1086,677,35,426,2255,3,1,-100]]), \n    'decoder_input_ids': \n        tensor(\n            [[ 0,6842,404,951,5767,15387,    27,   831, 800,  4378, 15,  1587,     3],\n             [ 0,9881,18590,3837,70,4341,1086,677, 35,   426,2255,     3,     1]])\n}\n새로 생긴 decoder_input_ids는 앞에 0()이 붙어 있는 것이 보이고 label에서 끝에 1이 사라져 label이 오른쪽으로 쉬프트된 것임을 알 수 있다. 그리고 또 두번째 샘플 labels에서 마지막에 -100 이 보인다. 이 값은 label이 패딩된 것을 나타내며 손실 함수값을 계산할 때 -100이 있는 위치는 손실을 계산하지 않게 된다. 이렇게 시퀀스 투 시퀀스 모델을 학습하기 위해 필요한 자잘한 작업을 콜레이터가 알아서 자동으로 처리한다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#metric",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#metric",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "마지막으로 학습한 모델을 측정할 매트릭을 준비해야 한다. 번역 모델에서는 주로 BLEU 점수를 사용한다. BLEU 점수는 번역기가 생성한 문장이 레퍼런스(정답이라는 표현을 사용하지 않는 이유는 제대로 된 번역 문장이 오직 하나가 아니기 때문)문장과 얼마나 비슷한지 측정하는 점수라고 생각하면 된다. 단 같은 단어가 반복된다든지 레퍼런스 문장보다 너무 짧은 문장을 생성한다든지 하면 패널티를 부여 한다. 그렇기 때문에 레퍼런스 문장과 길이가 최대한 비슷하고 다양한 단어를 사용하면서 생성된 문장의 단어가 레퍼런스 단어에 많이 보여야 높은 점수를 얻게 된다.\nBLEU를 계산하기 위해 허깅페이스 evaluate 라이브러리와 sacrebleu라이브러리를 제일 처음에 설치했었다.\nsacrebleu 라이브러리는 BLEU 구현체에서 사실상 표준 라이브러리이며 각 모델이 다른 토크나이저를 쓰는 경우 이를 BPE로 통일 시켜 BLEU 점수를 계산한다고 한다. 참고링크\nevaluate라이브러리로 이 sacrebleu를 불러온다.\n\nimport evaluate\n\nmetric = evaluate.load(\"sacrebleu\")\n\n아래와 같은 예제가 있을 때 두 영어 문장을 번역기가 predictions처럼 번역했고 데이터 셋에 두 문장의 레퍼런스 번역이 references처럼 두개씩 있을 때 bleu점수를 계산해보면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 100.00000000000004,\n 'counts': [21, 19, 17, 15],\n 'totals': [21, 19, 17, 15],\n 'precisions': [100.0, 100.0, 100.0, 100.0],\n 'bp': 1.0,\n 'sys_len': 21,\n 'ref_len': 21}\n\n\n첫 예에서는 predictions가 references의 두 문장 중 하나와 완전히 일치하므로 score가 100점이 나왔다. 하지만 약간 다른 식으로 번역을 한다면\n\npredictions = [\n    \"저는 딥러닝을 좋아해요.\",\n    \"딥러닝 프레임워크가 잘 개발되었기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.\"\n]\n\nreferences = [\n    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n]\nmetric.compute(predictions=predictions, references=references)\n\n{'score': 25.28116160010779,\n 'counts': [14, 7, 4, 1],\n 'totals': [19, 17, 15, 13],\n 'precisions': [73.6842105263158,\n  41.1764705882353,\n  26.666666666666668,\n  7.6923076923076925],\n 'bp': 0.9000876262522591,\n 'sys_len': 19,\n 'ref_len': 21}\n\n\n점수가 떨어지는 것을 확인할 수 있다. 아래 함수는 모델의 예측과 레이블을 가지고 bleu를 계산하는 헬퍼 함수로 트랜스포머 학습 코스 번역기 매트릭에서 제공하는 코드를 그대로 복사 한 것이다.\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Some simple post-processing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n    \n    return result"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#trainer",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#trainer",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "학습을 간단히 하기위해 허깅페이스에서 제공하는 Seq2SeqTrainer클래스를 사용한다. 학습 세부 조건은 Seq2SeqTrainingArguments를 사용하여 설정한다. 다음 코드로 학습에 필요한 세부 사항을 설정할 수 있다.\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"chkpt\",\n    learning_rate=0.0005,\n    weight_decay=0.01,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=128,\n    num_train_epochs=1,\n    save_steps=500,\n    save_total_limit=2,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"no\",\n    predict_with_generate=True,\n    fp16=False,\n    gradient_accumulation_steps=2,\n    report_to=\"none\" # Wandb 로그 끄기\n)\n\n이런 저런 자잘한 세팅을 해서 training_args를 만들고 trainer를 생성한다. 지금까지 준비한 model, training_args, tokenized_datasets, data_collator, tokenizer, compute_metrics를 넘기면 된다.\n\ntrainer = Seq2SeqTrainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"valid\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n이제 아래 코드로 드디어 학습을 할 수 있다!\n\n[주의] 코랩에서 실행한다면 per_device_train_batch_size를 12정도로 줄여서 학습해야 하는데 학습 시간만 10시간이 넘게 걸린다.\n\n\ntrainer.train()\n\n\n[노트] 학습 과정에서 출력되는 로그 문장들이 너무 길어서 여기선 생략 되었음\n\n학습이 끝났으면 다음 셀을 실행해서 결과를 저장한다.\n\ntrainer.save_model(\"./results\")"
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#test",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#test",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "학습과 저장을 성공적으로 마쳤으면 다음 명령으로 모델을 불러올 수 있다.\n\nmodel_dir = \"./results\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\nmodel.cpu();\n\nloading file spiece.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading configuration file ./results/config.json\nModel config T5Config {\n  \"_name_or_path\": \"./results\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"gelu_new\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"gated-gelu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": true,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.24.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64128\n}\n\nloading weights file ./results/pytorch_model.bin\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./results.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n\n\n로딩된 모델을 테스트하기 위해 다음 두 문장을 준비한다.\n\ninput_text = [\n    \"Because deep learning frameworks are well developed, in these days, machine translation system can be built without anyone's help.\",\n    \"This system was made by using HuggingFace's T5 model for a one day\"\n]\n\n모델이 입력하기위해 토크나이저로 토큰화 시킨다.\n\ninputs = tokenizer(input_text, return_tensors=\"pt\", \n                   padding=True, max_length=max_token_length)\n\n/home/metamath/miniconda3/envs/torchflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n\n\ninputs를 확인해보면 input_ids와 attention_mask로 토큰화 된것을 알 수 있다. 첫번째 문장이 더 길기 때문에 두번째 문장의 마스크는 마지막에 0으로 패딩된 것도 확인할 수 있다.\n\ninputs\n\n{'input_ids': tensor([[ 8127,  5859,  5789, 22309,     8,    69,   484,  6560,     4,    20,\n           572,  1258,     4,  9872, 46301,  1076,   147,    67,  3807,  1215,\n          3993,    17,     8,   787,     3,     1],\n        [  465,  1076,    62,   565,    81,  1676,   992, 60049,  1044, 17400,\n            17,     8,   745,   466,  3900,    40,    16,   165,   688,     1,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0]])}\n\n\nmodel.generate()에 입력을 넣고 출력을 생성한다. 이때 빔서치를 하기 위해 num_beams=5로 설정한다.\n\nkoreans = model.generate(\n    **inputs,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\nkoreans.shape\n\ntorch.Size([2, 20])\n\n\n생성된 결과를 디코딩해보면 다음처럼 나쁘지 않게 번역되는 것을 확인할 수 있다.\n\n[ \n    tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(korean)) for korean in koreans\n]\n\n['&lt;pad&gt; 딥러닝 틀이 잘 개발되기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.&lt;/s&gt;',\n '&lt;pad&gt; 이 시스템은 HuggingFace의 T5 모델을 하루 동안 사용해 만든 시스템입니다.&lt;/s&gt;&lt;pad&gt;']\n\n\n마지막으로 테스트 셋에 대해서 몇개 문장을 가져와 번역해보자. 만들어 놓은 tokenized_datasets과 data_collator를 pytorch DataLoader에 그대로 전달해서 데이터 로더를 만들 수 있다.\n\nfrom torch.utils.data import DataLoader\n\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n)\n\n이터레이터로 만들어 한 미니 배치만 가져온다.\n\ntest_dataloader_iter = iter(test_dataloader)\n\n\ntest_batch = next(test_dataloader_iter)\n\n콜레이터에 의해 반환된 미니 배치에는 다음처럼 labels, decoder_input_ids 따위도 가지고 있으므로 모델에 입력하기 위해 input_ids, attention_mask만 남긴다.\n\ntest_batch.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n\n\n\ntest_input = { key: test_batch[key] for key in ('input_ids', 'attention_mask') }\n\n\nkoreans = model.generate(\n    **test_input,\n    max_length=max_token_length,\n    num_beams=5,\n)\n\n이제 입력문장, 정답 그리고 생성된 문장을 비교하기 위해 우선 test_batch.labels에 -100으로 인코딩된 부분을 패딩 코튼으로 교체 한다.\n\nlabels =  np.where(test_batch.labels != -100, test_batch.labels, tokenizer.pad_token_id)\n\n\neng_sents = tokenizer.batch_decode(test_batch.input_ids, skip_special_tokens=True)[10:20]\n\n\nreferences = tokenizer.batch_decode(labels, skip_special_tokens=True)[10:20]\n\n\npreds = tokenizer.batch_decode( koreans, skip_special_tokens=True )[10:20]\n\n\nfor s in zip(eng_sents, references, preds):\n    print('English   :', s[0])\n    print('Reference :', s[1])\n    print('Translated:', s[2])\n    print('\\n')\n\nEnglish   : Yes, I'll see you at the parking lot at 3 p.m.\nReference : 네, 오후 3시에 주차장에서 뵙죠.\nTranslated: 네, 오후 3시에 주차장에서 뵙겠습니다.\n\n\nEnglish   : I'm happy to see Jessica Huh take over my role.\nReference : Jessica Huh가 제 역할을 인계받게 되어서 저는 기니다.\nTranslated: 제시카 허가 제 역할을 맡아서 기뻐요.\n\n\nEnglish   : I agree with you that she is qualified for the position.\nReference : 저도 부장님 의견대로 그녀가 이 자리의 적임자라고 생각합니다.\nTranslated: 나는 그녀가 이 직책에 자질이 있다고 당신과 동의합니다.\n\n\nEnglish   : Nick, I was told that your department will be divided into two.\nReference : Nick, 당신 부서가 둘로 나뉜다면서요?\nTranslated: 닉, 당신의 부서가 두 가지로 나뉘게 될 거라고 들었습니다.\n\n\nEnglish   : Yes, all staff involved in online advertising will have their own office space located on the 2nd floor.\nReference : 네, 다음 달 초부터 온라인 광고와 관련된 직원들은 2층에 위치한 개별 사무실 공간을 사용한다고 합니다.\nTranslated: 네, 온라인 광고에 관련된 모든 직원들은 2층에 각자의 사무실 공간이 위치해 있습니다.\n\n\nEnglish   : What happens to the remaining staff?\nReference : 남은 직원들은 어떻게 되나요?\nTranslated: 남은 스태프에게 무슨 일이 일어났나요?\n\n\nEnglish   : The remaining staff will stay in the current space on the 3rd floor, and the division will continue to be called the advertising department.\nReference : 남은 직원들은 3층 현재 자리에 남을 것이고, 부서는 계속 광고 부서로 불린다고 하네요.\nTranslated: 나머지 직원들은 3층 현 공간에 머물게 되며, 이 부서는 계속 광고부서로 불리게 된다.\n\n\nEnglish   : I have a question about the year-end tax adjustment.\nReference : 이번 연말 정산 관련해서 질문이 있어요.\nTranslated: 저는 연말정산에 대해 궁금한 점이 있습니다.\n\n\nEnglish   : Is there any problem?\nReference : 무슨 문제라도 있으신가요?\nTranslated: 혹시 문제가 있나요?\n\n\nEnglish   : I am registering my dependent this time, so do I need to submit any particular documents?\nReference : 제가 이번에 부양가족을 등록하려고 하는데, 따로 제출해야 하는 서류가 있나요?\nTranslated: 이번에 내 국적 등록을 하고 있으니 특별히 서류 제출을 해야 하나요?\n\n\n\n\n두 시간동안 대충 1 에폭만 학습한 것치고는 꽤 그럴듯 하게 번역을 하는 것을 알 수 있다."
  },
  {
    "objectID": "posts/gentle-t5-trans/gentle_t5_trans.html#마무리",
    "href": "posts/gentle-t5-trans/gentle_t5_trans.html#마무리",
    "title": "A Gentle Introduction to Creating an English-to-Korean translator with Transformers",
    "section": "",
    "text": "이상으로 영어-한국어 번역기를 처음부터 학습시키는 방법을 정리했다. 이 글을 글쓴이가 의도한대로 빠르게 읽고 이해하기 위해서는 트랜스포머에 대한 이해가 선행되야 한다. 트랜스포머에 대한 자세한 설명은 진짜로 주석달린 트랜스포머를 참고하자. 하지만 트랜스포머나 스퀀스 투 시퀀스 모델에 대해 잘 모른다 하더라도 한국어 번역기를 만들고자 할때 느끼는 막막함은 어느정도 해소할 수 있으리라 생각한다.\n이 글을 읽고 코드를 실행해보고 나서 DataCollatorForSeq2Seq나 Seq2SeqTrainer 를 쓰지 않고 직접 이 부분을 만들어서 모델을 학습 시켜본다면 트랜스포머를 이용한 번역 작업기 만들기를 훨씬 더 상세히 이해할 수 있을 것이다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part1.html",
    "href": "posts/diffusion/ddpm_part1.html",
    "title": "A Gentle Introduction to Diffusion Model: Part 1 - DDPM",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import ConnectionPatch\nfrom PIL import Image\n\n\nimport plotly.io\nimport plotly.graph_objects as go\n\n# colab: coloab\n# jupyter lab: jupyterlab\n# jupyter notebook, quarto blog: notebook\nplotly.io.renderers.default = \"notebook\"\n\n2022년 8월 스테이블 디퓨전이 발표되고 약 1년이 지난 지금 스테이블 디퓨전 모델을 실행하는 툴 사용법에 대한 동영상이 유튜브에 넘쳐나고, 해당 툴로 만들어진 수많은 이미지들이 civitai 같은 사이트에 모델과 함께 개시되고 있습니다. AI 기술이 실 생활에도 영향을 미칠 정도로 발전했다는 것을 보여주는 사례이며 2022년에 발표된 스테이블 디퓨전이 이런 현상을 주도했습니다.\n2023년 현재 인공지능 학습은 파인튜닝을 중심으로 재편되고 있는것 같습니다. 이전에는 인공지능 알고리즘의 원리를 정확히 파악하고 이를 구현하는 것이 목적이었다면 이제는 모델을 자기 목적에 맞게 파인튜닝할 수 있는것이 더 중요한 목표가 되었습니다. 앞서 이야기한 많은 디퓨전 관련 컨텐츠들도 모두 파인튜닝을 이용한 것들입니다. 파인튜닝을 잘하기 위해서 어느 정도 모델의 원리를 이해할 필요가 있습니다. 하지만 처음부터 모델을 구현할 정도로 원리를 깊게 이해할 필요는 없습니다. 이런 생각에는 여러 이견이 있을 수 있지만 학습의 트랜드가 파인튜닝을 중심으로 변하고 있다는 사실 자체를 부인할 수는 없을 것 같습니다.\n트랜드를 따라가는 것도 중요한 일이지만 조금 뒤처져서 여유있게 이전과 마찬가지로 모델의 원리를 납득할 만한 수준, 다시말해 원리를 이해하고 이해한 원리가 맞는지 코드로 실행해서 확인하는 수준으로 파악하는 것이 여전히 즐거운 분들이 계실것입니다. 이 블로그에 있는 모든 글은 그런 분들을 위해 쓰여진 글들이므로 이번에도 디퓨전 모델에 대해 그 수준으로 알아보는 글을 게시하게 되었습니다.\n이 글은 디퓨전 모델을 잘 이해하기 위해서 스테이블 디퓨전의 근간을 이루는 Denoising Diffusion Probabilistic Models[1] 논문을 리뷰합니다. 쉽지 않은 과정이지만 예전처럼 최대한 건너뜀 없이 친절하고 자세히 알아보도록 하겠습니다.*\n\n\n* 이 글은 고등학교 수준의 확률 지식과 약간의 다변수 함수에 대한 지식을 가진 공과대학 4학년 또는 석사 1, 2학기 수준에 맞춰져 있습니다.\n이 글은 다음 링크를 통해 구글 코랩에서 직접 실행하며 읽을 수 있습니다.\n\n\n\n\n아래 그림은 Denoising Diffusion Probabilistic Models 논문(줄여서 DDPM)을 대표하는 그림입니다. 그림에서 \\(\\mathbf{x}_0\\)는 노이즈 없는 깨끗한 이미지 데이터를 의미합니다. 학습을 위해 모은 데이터 셋에서 샘플 하나입니다. 그림은 이 이미지 \\(\\mathbf{x}_0\\)에 단계적으로 노이즈가 확산되는 과정을 오른쪽에서 왼쪽으로 나타내고 있고 반대로 왼쪽에서 오른쪽으로 노이즈가 제거되는 과정을 나타내고 있습니다.\n\n이미지 \\(\\mathbf{x}_0\\)는 숫자 여러 개로 이뤄진 벡터 데이터입니다. 이미지의 픽셀이 숫자 하나에 해당합니다(컬러 이미지라면 픽셀당 숫자 세개). 개별 숫자가 어떤 값을 가지느냐에 따라 우리 눈에 보이는 모습이 달라지게 됩니다. 각 픽셀의 값이 특정 확률 분포를 따른다고 보면 \\(\\mathbf{x}_0\\)는 개별 숫자가 확률 변수인 다차원 확률 변수라 할 수 있습니다. 깨끗한 이미지 \\(\\mathbf{x}_0\\)가 확률 변수라면 이 변수가 따르는 분포가 있을 것이고 이를 다음처럼 표시할 수 있습니다.\n\\[\n\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)\n\\]\n분포 \\(q\\)는 이미지 공간에 원래 존재하는 분포입니다. 하지만 이 분포가 어떤 것인지 알지 못합니다. 만약 \\(q(\\mathbf{x}_0)\\)를 정확히 알고 있다면 이 분포에서 샘플링을 하기만하면 데이터 셋과 동일한 종류의 깨끗한 이미지를 다양하게 얻을 수 있을 것입니다. DDPM 논문에서 \\(q\\)라고 적는 분포는 원래 존재하는 분포, 다시말해 알아내고 싶은 분포를 의미합니다.\n이미지가 확률변수고 이 확률변수가 따르는 분포로 부터 샘플링해서 또 다른 이미지를 만들어 낸다는 이야기가 너무 이상하게 들릴 수 있습니다. 구체적인 이해를 위해 실험을 해보겠습니다.\n다음 셀을 실행해서 16x16x3 크기를 가지는 스프라이트 이미지 데이터 셋을 다운 받습니다. 해당 데이터 셋은 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work에서 사용하는 데이터 셋입니다.\n\n!gdown 1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\nTo: /home/metamath/etc/repo/blog/posts/diffusion/sprites_1788_16x16.npy\n100%|██████████████████████████████████████| 68.7M/68.7M [00:06&lt;00:00, 10.8MB/s]\n\n\n다운 받은 데이터 파일을 로딩하고 255로 나눠 픽셀값을 0~1사이로 노멀라이즈 합니다.\n\nsprites = np.load('sprites_1788_16x16.npy')\n\n\n# 준비된 (64,64)크기의 나비 이미지를 (28,28)로 리사이즈하고\n# 픽셀값을 0~1로 노멀라이즈\nx0 = (sprites / 255)[10]\n\n# 노멀라이즈 확인\nprint(x0.min(), x0.max())\n\n0.0 1.0\n\n\n다운받은 이미지를 화면에 출력해봅니다.\n\nplt.imshow(x0)\nplt.show()\n\n\n\n\n이제 x0가 확률변수라는 것을 실험해보기 위해 scipy에서 제공하는 다변수 정규분포 multivariate_normal를 임포트 합니다.\n\n# x0가 확률변수라는 것을 실험해보기 위해 다변수 정규분포를 임포트\nfrom scipy.stats import multivariate_normal\n\nx0를 16x16x3=768개 숫자를 가지는 벡터 변수로 보고 방금 준비한 x0을 평균으로 하는 정규분포 q_x0를 생성합니다.\n\n# 이미지를 768차원 확률 벡터 변수로 만들고\nx0_flt = x0.reshape(-1)\n\n# 이 이미지 x0_flt를 평균으로 하고\n# 0.1정도되는 적당한 수를 곱해서 공분산을 만들어 정규분포를 정의\n# 이 수가 커지면 평균으로 부터 멀리 떨어진 이미지까지 샘플링되고\n# 작으면 평균과 거의 비슷한 이미지들만 샘플링됨\nq_x0 = multivariate_normal(mean=x0_flt, cov=0.1 * np.eye(len(x0_flt)))\n\n생성된 분포 q_x0에서 값 3개를 샘플링해서 화면이 그려봅니다.\n\n# 3개만 샘플링 해서\nsamples = q_x0.rvs(size=3)\n# 크기를 보면 (3,768)\nprint(samples.shape)\n\n(3, 768)\n\n\n768차원 벡터 변수 3개가 샘플링 되었고 이를 적당히 모양 조정해서 화면에 그리면 다음처럼 그려집니다.\n\n# x0 주변에서 임의로 선택된 샘플들\nplt.imshow(samples.reshape(3,16,16,3).transpose(1,0,2,3).reshape(16,-1,3).clip(0,1))\nplt.show()\n\n\n\n\n나타나는 그림은 샘플링을 시도할 때마다 조금씩 달라지게 되고 q_x0를 만들 때 설정한 분산 cov에 곱하는 숫자 0.1을 더 크게 할 수록 점점 더 노이즈가 많은 이미지가 샘플링 될 수 있습니다. x0 주변에서 완전히 엉뚱한 노이즈 이미지가 샘플링되는 것이 아니라 평균 이미지를 중심으로 노이즈가 낀 이미지가 샘플링되는 것은 분명히 확인할 수 있습니다.\n이미지를 나타내는 변수 \\(\\mathbf{x}_0\\)가 확률변수라는 점을 실험을 통해 분명히 했으므로 나머지 내용을 계속 이어가도록 합시다.\nDDPM의 Fig.2는 완전한 노이즈 \\(\\mathbf{x}_T\\)로 부터 깨끗한 이미지 \\(\\mathbf{x}_0\\)가 만들어지는 과정을 그리고 있습니다. 즉, \\(\\mathbf{x}_T, \\mathbf{x}_{T-1}, \\mathbf{x}_{T-2}, ... , \\mathbf{x}_1\\)이 어떻게 선택되냐에 따라서 \\(\\mathbf{x}_0\\)가 결정된다는 것이고 DDPM 논문에서 이미지가 생성되는 과정을 이렇게 모델링하는 것입니다. 원인과 결과를 따져보자면 \\(\\mathbf{x}_0\\)는 결과가 되고 \\(\\mathbf{x}_T, \\mathbf{x}_{T-1}, \\mathbf{x}_{T-2}, ... , \\mathbf{x}_1\\)들은 \\(\\mathbf{x}_0\\)라는 결과를 만들어낸 원인이 되는 것입니다.\n하지만 오직 \\(\\mathbf{x}_0\\)만 관찰될 수 있고 어떤 \\(\\mathbf{x}_T\\) ~ \\(\\mathbf{x}_1\\)이 선택되어서 지금 보고 있는 \\(\\mathbf{x}_0\\)가 결정되었는지 알 수 없습니다. 이렇게 관찰되는 변수observable variable와 관계되어 영향을 미치지만 직접 관찰되지 않는 변수를 잠재 변수latent variable라 합니다.\n앞서 알아본것 처럼 \\(\\mathbf{x}_0\\)의 분포 \\(q(\\mathbf{x}_0)\\)를 알면 이 분포로 부터 \\(\\mathbf{x}_0\\)를 샘플링할 수 있습니다. 찾고 싶은 \\(q(\\mathbf{x}_0)\\)를 신경망 같은 모델로 만들어볼 수 있을 것입니다. 그렇게 신경망 따위로 만든 \\(\\mathbf{x}_0\\)의 분포를 \\(p_\\theta(\\mathbf{x}_0)\\)로 쓸 수 있습니다. 이렇게 DDPM에서는 학습으로 만들어가는 분포를 \\(p()\\)로 적고 원래 있는 분포 다시 말해 찾고 싶은 분포를 \\(q()\\)로 적습니다. \\(p\\) 아래 있는 \\(\\theta\\)는 모델이 \\(q(\\mathbf{x}_0)\\)처럼 잘 작동하기 위해 찾아야 하는 파라미터가 됩니다.\n\\(\\mathbf{x}_0, \\mathbf{x}_1, ... , \\mathbf{x}_T\\)들은 서로 연결joint되 있으므로 이 전체 확률변수들의 분포를 \\(p_\\theta(\\mathbf{x}_0, \\mathbf{x}_1, ... , \\mathbf{x}_T)\\) 로 쓸 수 있는데 원문에서는 이를 줄여 \\(p_\\theta(\\mathbf{x}_{0:T})\\)로 쓰고 \\(\\mathbf{x}_{0:T}\\)를 \\(\\mathbf{x}_0, \\mathbf{x}_1, ... , \\mathbf{x}_T\\)들이 모두 결합된 확률변수로 나타냅니다. 최종적으로 관심이 있는 분포는 \\(\\mathbf{x}_0\\)에 대한 분포 이므로 관심 없는 잠재변수는 주변화 시켜 다음처럼 나타낼 수 있습니다.\n\\[\np_{\\theta}(\\mathbf{x}_0) = \\int p_\\theta (\\mathbf{x}_{0:T}) d\\mathbf{x}_{1:T}\n\\]\n갑자기 적분 기호가 나와서 머리가 아플 수 있는데 위 식의 의미는 잠재 변수 \\(\\mathbf{x}_{1:T}\\)를 조건으로 \\(\\mathbf{x}_0\\)의 평균을 구한 것으로 생각하면 됩니다. 따라서 \\(p_{\\theta}(\\mathbf{x}_0)\\)가 구해진다면 이 분포는 우리에게 잠재 변수를 고려한 평균적인 \\(\\mathbf{x}_0\\)를 샘플링할 수 있게 해줄 것입니다.\n만약 \\(p_\\theta (\\mathbf{x}_{0:T})\\)를 완전히 알고 있고 이 분포를 사용해서 샘플링하게 된다면 완전 노이즈 이미지 \\(\\mathbf{x}_T\\)와 여러 단계를 거쳐 노이즈가 조금씩 제거된 이미지 \\(\\mathbf{x}_{T-1}, ... \\mathbf{x}_1\\), 그리고 마지막 깨끗한 이미지 \\(\\mathbf{x}_0\\)를 모두 한 세트로 뽑을 수 있게 될 것입니다. 그런 샘플링이 가능하다면 다음과 같은 샘플은 뽑힐 가능성이 아주 높을 것입니다.\n\n\n반면 다음 같이 빨간 캐릭터가 살짝 보이다가 갑자기 하얀 캐릭터로 바뀌면서 노이즈가 제거되는 샘플은 뽑힐 가능성은 아주 낮겠죠.\n\n이렇게 노이즈가 제거 되어 가는 과정에 대한 변수를 한꺼번에 뽑을 수 있는 분포 \\(p_\\theta (\\mathbf{x}_{0:T})\\)를 리버스 프로세스reverse process라고 합니다. 이 리버스 프로세스에는 고차원의 확률변수들이 너무 많이 결합되어 있으므로 문제를 간단히 하기 위해 마르코프 가정을 하게 됩니다. 원래는 \\(\\mathbf{x}_{0}\\)에 잠재 변수 \\(\\mathbf{x}_{1:T}\\) 모두가 영향을 미치는 것으로 이야기했지만 모델링 과정에서 마르코프 과정을 가정하고 \\(\\mathbf{x}_{0}\\)에는 \\(\\mathbf{x}_{1}\\)만 잠재 변수가 되게 모델링하게 됩니다. 동일하게 \\(\\mathbf{x}_{1}\\)에는 \\(\\mathbf{x}_{2}\\)만이 잠재 변수가 되겠네요. 마르코프 가정을 하고 각 시간 단계에 대한 이미지의 분포를 다음처럼 정의합니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n위 정의에서 각 이미지에 대한 분포를 정규분포로 가정했습니다. 위 식의 의미는 노이즈가 조금 더 제거된 \\(\\mathbf{x}_{t-1}\\)에 대한 분포는 바로 이전 단계인 노이즈가 약간 더 많은 \\(\\mathbf{x}_{t}\\)를 이용해 계산된 어떤 평균 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t)\\)와 분산 \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)을 파라미터로 하는 정규분포로 정의 한다는 것입니다. 이 때 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t)\\), \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)같은 것들은 입력 \\(\\mathbf{x}_t\\)를 보고 네트워크가 추정해야 하는 값들 입니다. 다시말해 노이즈가 많은 \\(\\mathbf{x}_t\\)를 네트워크의 입력으로 넣으면 네트워크는 그 입력을 이용해서 정규분포로 가정된 노이즈가 약간 더 적은 \\(\\mathbf{x}_{t-1}\\)의 분포를 평균과 분산을 추정해서 알아내는 것입니다.\n마지막 단계인 \\(\\mathbf{x}_T\\)는 순수한 가우시안 노이즈라고 보면 \\(p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\)가 되겠고 위 정의와 함께 쓰면 리버스 프로세스 \\(p_\\theta(\\mathbf{x}_{0:T})\\)는 마르코프 가정에 의해 다음처럼 모두 곱해진 형태로 정의될 수 있습니다.\n\\[\np_\\theta(\\mathbf{x}_{0:T}) := p(\\mathbf{x}_T) \\prod^{T}_{t=1}p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) \\tag{1}\n\\]\n마르코프 가정 덕분에 적어도 무시무시한 적분 기호는 사라졌네요. 😁\n\n\n\n\n멀쩡한 이미지에 노이즈가 점점 확산되어 최종적으로 완전히 노이즈 \\(\\mathbf{x}_T\\)가 되는 과정을 포워드 프로세스forward process라고 합니다. 그림에선 오른쪽에서 왼쪽으로 진행되는 과정입니다. 포워드이라고 하면 보통 왼쪽에 오른쪽으로 진행되는 그림을 상상하게 되는데 논문에서는 유독 이를 거꾸로 그렸습니다. \\(\\mathbf{x}\\)에 대한 인덱스도 0에서 \\(T\\)까지가 오른쪽에서 왼쪽으로 진행되도록 그려져서 처음 이 그림을 보면 한동안은 포워드 프로세스가 어느 방향인지 계속 햇갈리게 됩니다. 최종 목적이 노이즈로 부터 이미지를 만들어 가는 과정이므로 논문 저자들은 이렇게 반대로 그려놓는 것이 아마 더 자연스럽다고 생각한것 같습니다.\n리버스 프로세스를 알아보면서 잠재 변수 \\(\\mathbf{x}_{1:T}\\)는 이미지 \\(\\mathbf{x}_0\\)에 대한 원인이고 \\(\\mathbf{x}_0\\)는 결과라고 했습니다. 결과를 조건으로 하는 원인의 확률을 사후 확률posterior이라고 합니다. 그럼 사후 확률 분포는 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)로 쓸 수 있습니다. 이 사후 확률을 포워드 프로세스 또는 디퓨전 프로세스diffusion process라고 합니다. 앞서 이야기한 것처럼 그림에서 오른쪽에서 왼쪽으로 진행되는 과정입니다. 이 확률 분포는 \\(\\mathbf{x}_0\\)가 주어지면 이 이미지를 생성하기 위해 거쳐가야 하는 모든 잠재변수 \\(\\mathbf{x}_{1:T}\\)에 대한 분포를 정의하게 됩니다.\n리버스 프로세스는 직접 수식으로 계산할 수 없지만 포워드 프로세스는 마르코프 과정을 상정하고 각 과정이 정규분포라고 가정하면 직접 계산할 수 있습니다. 리버스 프로세스 때와 같이 마르코프 가정을 하면 그림에서 나타낸 노이즈가 조금 적은 \\(\\mathbf{x}_{t-1}\\)을 조건으로 그 다음 노이즈가 조금 더 많은 이미지 \\(\\mathbf{x}_t\\)에 대한 분포는 다음처럼 정의 할 수 있습니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\]\n이 식에서 \\(\\beta_t\\)는 각 단계에서 노이즈를 얼마나 추가할 지 결정하게 되는 상수입니다. 이렇게 정의된 개별 분포를 모두 곱해서 잠재 변수에 대한 사후 확률, 포워드 프로세스를 정의 합니다.\n\\[\nq(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0):= \\prod^{T}_{t=1} q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) \\tag{2}\n\\]\n앞서 리버스 프로세스에서 노이즈를 제거하는 한 단계 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)를 정의한 바 있습니다. 노이즈가 더해지는 과정 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)로 부터 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 바로 알아 낼 수 있으면 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)를 만들 필요가 없습니다. 하지만 이를 위해 베이즈 정리를 사용한다면 모든 시간 단계에 대한 \\(q(\\mathbf{x}_t)\\)를 다 알아야 하므로 쉽지 않은 일입니다.\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) q(\\mathbf{x}_{t-1})}{q(\\mathbf{x}_t)}\n\\]\n그래서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)의 역과정인 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)로 대신하고자 하는게 DDPM에서 하고자 하는 것입니다.\n그런데 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)을 잘 만들려면 지도 학습 관점에서 비교 대상인 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 알아야 하는데 이 분포는 모른다고 했으니 학습에 사용할 비교 대상이 없습니다. 이 문제를 인식하고 이를 해결하는 과정을 이해하는 것이 DDPM 논문을 이해하는 거의 전부라 할 수 있으니 차차 알아보도록 하겠습니다.\n이제 정의된 포워드 프로세스를 실험해보기 위해 \\(\\beta_t\\), \\(T\\)같은 값들을 정해야 하는데 DDPM 논문에서는 각 설정값을 다음처럼 지정했다고 나와있습니다.\n\n\\(T=1000\\), \\(\\beta_1 = 10^{-4}\\), \\(\\beta_T = 0.02\\)\n\n논문과 동일하게 beta() 함수를 작성합니다.\n\ndef beta(t, T=1000):\n    # t: 1~T\n    # t는 1에서 T까지 이므로 인덱싱할 때는 -1해준다.\n    return np.linspace(1.0e-4, 0.02, T)[t-1]\n\n실험에 사용할 샘플 이미지 \\(\\mathbf{x}_0\\)를 준비합니다. 이때 이미지의 픽셀 값들이 -1, 1사이에 오게 노멀라이즈 합니다.\n\nx0 = (sprites / 255)[10] * 2 - 1\nx0.min(), x0.max()\n\n(-1.0, 1.0)\n\n\n이 이미지의 크기는 (16,16,3)입니다.\n\nx0.shape\n\n(16, 16, 3)\n\n\n분포 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)를 만들어주는 함수 get_q_xt_given_xtm1()을 정의합니다. 이 함수는 내부에서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\) 정의에 따라 정규분포를 하나 만들고 그 분포를 반환합니다.\n\ndef get_q_xt_given_xtm1(xtm1, t):\n    beta_t = beta(t)\n    xtm1_flt = xtm1.reshape(-1)\n\n    q = multivariate_normal(\n        mean=np.sqrt(1-beta_t)*xtm1_flt,\n        cov=beta_t*np.eye(len(xtm1_flt))\n    )\n\n    return q\n\n샘플 x0를 사용해서 \\(\\mathbf{x}_1\\)에 대한 분포를 생성합니다.\n\n# t=1을 지정해 원본 이미지 x0에서 1단계 노이즈 상태인 이미지에 대한 분포를 생성\nq_x1_given_x0 = get_q_xt_given_xtm1(x0, t=1)\n\n이 분포는 \\(\\sqrt{1-\\beta}\\mathbf{x}_0\\)를 평균으로 하는 정규분포이므로 \\(\\mathbf{x}_0\\)에 대한 밀도 함숫값, 다시말해 이 분포에서 \\(\\mathbf{x}_0\\)가 존재할 가능성은 크고 일반 노이즈에 대한 가능성은 작아야 합니다. 밀도 함숫값의 로그값을 계산해주는 logpdf() 함수로 확인해봅시다.\n\n# x0에 대한 확률 밀도값                  그냥 노이즈에 대한 확률 밀도값\nq_x1_given_x0.logpdf( x0.reshape(-1) ), q_x1_given_x0.logpdf( np.random.randn( 16*16*3 ) )\n\n(2831.0183943629117, -6699964.090514234)\n\n\n예상대로 \\(\\mathbf{x}_0\\)에 대한 값은 매우 크고 표준 정규분포로 부터 샘플링된 노이즈에 대한 값은 로그값이 매우 작은 음수이므로 거의 0임을 알 수 있습니다. 이제 정의된 분포로부터 \\(\\mathbf{x}_1\\) 하나를 샘플링합니다.\n\nx1 = q_x1_given_x0.rvs(size=1)\n\n제대로 작동한다면 x1은 x0와 거의 차이가 없어야 할 것입니다.\n\nfig, ax = plt.subplots(nrows=1, ncols=2)\n\nx0_ = ((x0 - x0.min()) / (x0.max() - x0.min())).clip(0,1)\nx1_ = ((x1 - x1.min()) / (x1.max() - x1.min())).clip(0,1)\n\nax[0].imshow(x1_.reshape(16,16,3))\nax[0].set_title(r\"$\\mathbf{x}_1$\")\n\nax[1].imshow(x0_.reshape(16,16,3))\nax[1].set_title(r\"$\\mathbf{x}_0$\")\nplt.show()\n\n\n\n\n한 스탭정도 노이즈를 확산시켜서는 아무런 차이가 없는듯 보입니다. 경우에 따라 흰색 배경부분을 자세히 보면 완전 흰색이 아니라 약간 색이 달라진 것을 미세하게 확인할 수 있을 수도 있습니다(아주 약하게 나타나거나 모니터에 따라 확인되지 않을 수 있음). 이제 \\(T\\) 단계까지 한 단계씩 차례로 노이즈를 확산시킵니다. for루프로 이를 직접 구현해보면 다음과 같습니다. 확산 단계는 30단계까지로 제한했습니다.\n\nx0.min(), x0.max()\n\n(-1.0, 1.0)\n\n\n\n%%time\n\n# 루프 돌면서 x30까지 해보기\nxts = [x0.reshape(-1).copy()]\nxt = xts[0]\nT = 30\n\nfor t in range(1, T+1):\n    # t-1 단계에서 만들어진 이미지로 부터 분포 q(x_t|x_t-1)을 만든다.\n    q_xt_given_xtm1 = get_q_xt_given_xtm1(xt, t)\n    # 만들어진 분포에서 샘플링한다.\n    xt = q_xt_given_xtm1.rvs(size=1)\n    xts.append(xt)\n\nCPU times: user 9.76 s, sys: 3.04 s, total: 12.8 s\nWall time: 3.25 s\n\n\n30단계만 진행했는데도 시간이 상당히 오래 걸립니다. 분포를 정의하고 그로 부터 샘플링하는 과정을 기술적으로 잘 처리해서 속도를 조금 높일 수 있겠지만 원리적으로 시간이 오래 걸리는 과정이라는 사실은 변함이 없습니다. 노이즈가 확산된 30개 이미지와 원본이미지를 담은 리스트를 넘파이 어레이로 변환합니다.\n\nxts = np.array(xts)\nxts.shape\n\n(31, 768)\n\n\n이제 30단계까지 확산된 노이즈를 가진 이미지를 원본 이미지와 비교해보겠습니다.\n\nfig, ax = plt.subplots(nrows=1, ncols=2)\n\nx0_ = ((x0 - x0.min()) / (x0.max() - x0.min())).clip(0,1)\nxt_ = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n\nax[0].imshow(xt_.reshape(16,16,3))\nax[0].set_title(f\"$\\mathbf{{x}}_{{{T}}}$\")\n\nax[1].imshow(x0_.reshape(16,16,3))\nax[1].set_title(r\"$\\mathbf{x}_0$\")\nplt.show()\n\n\n\n\n확실이 점점 노이즈로 뒤덮히기 시작합니다. DDPM에서는 \\(T=1000\\)을 사용하므로 1000단계까지가면 원래 이미지는 완전히 사라지고 노이즈만 있는 이미지가 될 것입니다.\n이제 식(2)에 의해 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)를 직접 계산할 수 있습니다. \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)를 직접 눈으로 확인해보기 위해 \\(\\mathbf{x}_t\\)들을 스칼라로 가정하고 \\(T=2\\)로 두어 \\(x_0\\), \\(x_1\\), \\(x_2\\)로 포워드 프로세스 단계를 제한합니다. 그렇게하면 3차원 그래프로 해당 분포를 그려볼 수 있습니다.\n식(2)에 의해 다음과 같으므로\n\\[\nq(x_{1:2} \\mid x_0) = q(x_2 \\mid x_1) \\times q(x_1 \\mid x_0)\n\\]\n두 일변수 정규분포를 곱하고 정리하면 다음처럼 \\(q(x_{1:2} \\mid x_0)\\)를 \\(x_1\\), \\(x_2\\)에 대해 계산해주는 함수를 만들 수 있습니다.\n\ndef make_q(x0):\n\n    def q(x2, x1):\n        return (1 / (2*np.pi)) * np.exp( -(1/2) * ((x2-x1)**2 + (x1-x0)**2) )\n\n    return q\n\n해당 확률분포가 \\(x_0\\)에 대해서 어떻게 변하는지 확인하기 위해 그림그리는 모듈을 임포트 합니다.\n\\(x_1\\), \\(x_2\\)는 -10, 10 정도 범위로 설정하고 \\(x_0\\)는 -5, 5까지 범위를 설정해 각 \\(x_0\\)에 대해서 \\(q(x_{1:2} \\mid x_0)\\)를 -10, 10로 정의된 정사각 영역에 대해서 함숫값을 모두 계산합니다. 아울러 이렇게 생성된 \\(x_0\\)에 대해서 \\(q(x_{1:2} \\mid x_0)\\)들이 확률분포로써 타당한지 확인하기 위해 수치적분값이 1이 되는지 확인해봅니다.\n\nfrom scipy import integrate\n\n# 정의역 정의\nx_range = [-10, 10]\ny_range = [-10, 10]\n\nx_min, x_max = x_range[0], x_range[1]\ny_min, y_max = y_range[0], y_range[1]\n\nxx = np.linspace(x_min, x_max, 150)\nyy = np.linspace(y_min, y_max, 150)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nx0s = np.linspace(-5, 5, 21)\nZs = []\n\nfor x0_ in x0s:\n    q = make_q(x0=x0_)\n    Zs.append( q(X_grid[:,0], X_grid[:,1]) )\n\n    # 2차원 수치 적분\n    print(f\"{integrate.dblquad(q, x_range[0], x_range[1], lambda x: y_range[0], lambda x: y_range[1])[0]:.4f}\")\n\n0.9998\n0.9999\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9999\n0.9998\n\n\n적분값은 모두 거의 1에 가까운 값임을 확인할 수 있습니다. 이제 앞서 x0s변수에 범위를 잡아둔 \\(x_0\\)에 따른 \\(x_1\\), \\(x_2\\)의 분포를 그림으로 그립니다.\n\nlayout = go.Layout(\n    title='q(x_1:2|x_0)',\n    width=600, height=600,\n    margin=dict(l=0, r=0, b=0, t=25),\n    scene = dict(\n        xaxis = dict(title='x2', range=[x_min, x_max],),\n        yaxis = dict(title='x1', range=[y_min, y_max],),\n        zaxis = dict(title='pdf'),\n        aspectratio=dict(x=1, y=1, z=0.5)\n    )\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\n\n# Add traces, one for each slider step\nfor Z in Zs:\n    fig.add_trace(\n        go.Surface(\n            x=X_grid[:,0].reshape(X1.shape), y=X_grid[:,1].reshape(X1.shape),\n            z=Z.reshape(X1.shape),\n            showscale=False,  opacity=1.,\n            colorscale ='Blues',\n            contours=dict(\n                x=dict(show=True, highlight=True),\n                y=dict(show=True,  highlight=True),\n                z=dict(show=True,  highlight=True),\n            ), visible=False\n        )\n    )\n\nfig.data[0].visible = True\n\n# Create and add slider\nsteps = []\n\nfor i in range(0, len(fig.data)):\n    step = dict(\n        method=\"update\",\n        args=[\n            {\"visible\": [False] * (len(fig.data))},\n        ],  # layout attribute\n        label=f\"{x0s[i]:.2f}\"\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [\n    dict(\n        active=0,\n        currentvalue={\"prefix\": \"x0: \"},\n        pad={\"l\":10, \"t\": 50, \"r\":10, \"b\":10},\n        steps=steps\n    )\n]\n\nfig.update_layout(sliders=sliders)\n\nfig.show()\n\n\n                                                \n\n\n위 그림에서 슬라이드를 이동시키면 확률분포가 변하는 것을 확인할 수 있습니다. 여기서 슬라이드로 조정하는 \\(x_0\\)는 노이즈가 없는 깨끗한 이미지로 생각할 수 있습니다. 즉 슬라이드의 이동으로 깨끗한 이미지가 변하면 이와 연결된 잠재 변수들의 결합 분포가 변하는 것을 눈으로 확인 해볼 수 있는 것입니다. 당연하겠지만 잠재 변수들은 \\(x_0\\) 근처에 분포하게 됩니다.\n이 시각화를 통해서 이미지 \\(\\mathbf{x}_0\\)와 노이즈가 포함된 이미지 \\(\\mathbf{x}_1, ..., \\mathbf{x}_T\\)에 대한 관계가 조금 더 명확히 이해 되었으면 좋겠습니다.\n이제 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)가 무엇을 의미하는지 잘 이해했으므로 이미지 변수에 대해서 이 분포를 직접 계산하는 함수를 만들어 봅시다.\n다음 함수 get_q_x1T_given_x0()는 루프를 돌면서 식(2)를 직접 계산하는 함수 q()를 리턴합니다. 이 때 확률 밀도 함숫값은 매우 큰 값이 되므로 이를 곱하면 오버플로가 생기게 됩니다. 이를 방지하기 위해 로그 확률 밀도값을 더하는 방식을 사용합니다.\n\ndef get_q_x1T_given_x0(x0, T=T, x0_shape=(16,16,3)):\n    # T개의 확률변수를 만든다.\n    def q(x1T):\n        # x1T: T*16*16*3\n        x1_T = x1T.reshape(T, np.prod(x0_shape)) # (T, 16*16*3)\n        x0_T = np.concatenate((x0.reshape(1,-1), x1_T), axis=0) # (T+1, 16*16*3)\n\n        density = 0.0\n        for t in range(1, T+1):\n            q_xt_given_xtm1 = get_q_xt_given_xtm1(x0_T[t-1], t)\n            density += q_xt_given_xtm1.logpdf(x0_T[t])\n\n        return density\n\n    return q\n\nx0를 조건으로 입력하여 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\) 분포를 생성합니다.\n\nq_x130_given_x0 = get_q_x1T_given_x0(x0, T=T)\n\n\\(q(\\mathbf{x}_1 \\mid \\mathbf{x}_{0})\\)에 대해서 \\(\\mathbf{x}_0\\)와 표준 정규분포로 부터 샘플링된 노이즈를 입력하여 각 샘플에 대한 가능성을 logpdf()로 직접 계산해본 실험을 여기서 다시 해보겠습니다.\n\\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)에 대해서 표준 정규분포로 부터 샘플링된 (30, 16, 16, 3) 크기를 가지는 노이즈를 입력합니다.\n\nx_random = np.random.randn( (T+1), 16, 16, 3 )\n\nfig, ax = plt.subplots(figsize=(13,10), nrows=5, ncols=6)\n\nfor i in range(5):\n    for j in range(6):\n        xt = x_random[i*5+j+1]\n        xt = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n        ax[i][j].imshow(xt.reshape(16,16,3))\n        title = f\"$x_{{{i*6+j+1}}}$\"\n        ax[i][j].set_title(title)\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n\nfig.suptitle(r\"$\\mathbf{x}_{1:30}$\", y=0.93)\nplt.show()\n\n\n\n\n위 그림이 \\(\\mathbf{x}_{1:30}\\)을 나타냅니다. 이 샘플은 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)에서 존재할 가능성이 매우 작을 것입니다.\n\nq_x130_given_x0( x_random[1:].reshape(-1) )\n\n-77323292.1886759\n\n\n거의 0이 됩니다. 반면 조금전 \\(\\mathbf{x}_0\\)로부터 30단계까지 노이즈를 확산하면서 얻은 이미지 30장 세트로 구성된 \\(\\mathbf{x}_{1:30}\\)는 분포 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)상에서 큰 가능성을 가질 것입니다. 이 샘플이 xts에 저장되어 있으므로 실험을 해볼 수 있습니다.\n\nfig, ax = plt.subplots(figsize=(13,10), nrows=5, ncols=6)\n\nfor i in range(5):\n    for j in range(6):\n        xt = xts[i*5+j+1]\n        xt = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n        ax[i][j].imshow(xt.reshape(16,16,3))\n        title = f\"$x_{{{i*6+j+1}}} \\sim q(x_{{{i*6+j+1}}} | x_{{{i*6+j}}})$\"\n        ax[i][j].set_title(title)\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n\nfig.suptitle(r\"$\\mathbf{x}_{1:30}$\", y=0.93)\nplt.show()\n\n\n\n\n\nq_x130_given_x0( xts[1:].reshape(-1) )\n\n59104.04837779686\n\n\n예상처럼 꽤 큰 값을 돌려줍니다. 만약 이 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)로 부터 정밀하게 샘플링 할 수 있다면 위 그림과 비슷한 하지만 조금씩 다른 노이즈가 확산된 이미지 세트를 얻을 수 있을 것입니다.\n마지막으로 \\(\\mathbf{x}_{15}\\)를 노이즈 확산의 시작 이미지인 \\(\\mathbf{x}_0\\)로 설정하고 30단계 노이즈를 확산하면 방금 실험한 \\(\\mathbf{x}_{1:30}\\)과는 노이즈 정도가 사뭇 다른 이미지들이 얻어질텐데 이 이미지들의 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)에서 가능성은 어떻게 될지 실험해보겠습니다.\n\n%%time\n\n# xts[15]에 저장된 x_15를 시작이미지 y0로 두고 y1 ~ y30까지 생성\nyts = [xts[15].reshape(-1)]\nyt = yts[0]\n\nfor t in range(1, T+1):\n    # t-1 단계에서 만들어진 이미지로 부터 분포 q(x_t|x_t-1)을 만든다.\n    q_xt_given_xtm1 = get_q_xt_given_xtm1(yt, t)\n    # 만들어진 분포에서 샘플링한다.\n    yt = q_xt_given_xtm1.rvs(size=1)\n    yts.append(yt)\n\nyts = np.array(yts)\nyts.shape\n\nfig, ax = plt.subplots(figsize=(13,10), nrows=5, ncols=6)\n\nfor i in range(5):\n    for j in range(6):\n        xt = yts[i*6+j+1]\n        xt = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n        ax[i][j].imshow(xt.reshape(16,16,3))\n        title = f\"$x_{{{i*6+j+1}}} \\sim q(x_{{{i*6+j+1}}} | x_{{{i*6+j}}})$\"\n        ax[i][j].set_title(title)\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n\nfig.suptitle(r\"$\\mathbf{x}_{1:30}$\", y=0.93)\nplt.show()\n\nq_x130_given_x0( yts[1:].reshape(-1) )\n\n\n\n\nCPU times: user 13.4 s, sys: 3.89 s, total: 17.3 s\nWall time: 4.81 s\n\n\n44704.27845529124\n\n\n그려진 샘플을 확인해보세요. 이번 샘플은 처음부터 노이즈가 존재합니다. 이렇게 확산된 노이즈 정도가 다르기 때문에 확률분포에서 존재할 가능성은 조금 떨어진것을 알 수 있습니다.\n다음에 지금까지 야이기한 리버스 프로세스와 포워드 프로세스를 다시 정리했습니다.\n\n리버스 프로세스와 포워드 프로세스 \\[\n\\begin{aligned}\n&p_\\theta(\\mathbf{x}_{0:T}) := p(\\mathbf{x}_T) \\prod^{T}_{t=1}p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t), \\qquad &p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right), \\quad p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})  \n\\\\\n&q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0):= \\prod^{T}_{t=1} q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}), \\qquad &q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\end{aligned}\n\\]\n\n리버스 프로세스에만 파라미터가 있고 포워드 프로세스에는 없는 점을 유념해야 합니다.\n\n\n\n앞에서 정의한 리버스 프로세스에서 잠재 변수를 주변화시켜 \\(p_\\theta (\\mathbf{x}_0)\\)를 얻을 수 있다는 점을 이야기했습니다.\n\\[\np_{\\theta}(\\mathbf{x}_0) = \\int p_\\theta (\\mathbf{x}_{0:T}) d\\mathbf{x}_{1:T}\n\\]\n이렇게 얻은 \\(p_\\theta (\\mathbf{x}_0)\\)는 \\(\\theta\\)에 대한 \\(\\mathbf{x}_0\\)의 가능도 함수입니다. 파라미터 \\(\\theta\\)를 조정해서 이 가능도를 최대화 시키는 것을 최대 가능도 추정Maximum Likelihood Estimation이라 하고 많은 모델들이 이 MLE 과정으로 학습하게 됩니다. 이처럼 가능도 함수를 최대화 시키는 \\(\\theta\\)를 최적화 알고리즘을 통해 찾는 것이 학습의 목표인데 최적화는 보통 함수의 최소화를 수행하므로 \\(-1\\)을 곱해서 마이너스 로그 가능도로 만듭니다. 또 가능도 함수는 곱하기에 의해 정의되는 경우가 많기 때문에 가능도 함수를 직접 다루기 보다 로그를 씌워서 로그 가능도 함수를 다루는 경우가 많습니다 (실제로 앞서 정의한 \\(p_\\theta (\\mathbf{x}_{0:T})\\)도 많은 곱하기로 이뤄져 있음을 알 수 있습니다).\n\\[\n- \\log p_{\\theta}(\\mathbf{x}_0) = - \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right)\n\\]\n우변은 잠재 변수에 대한 적분입니다. 이 적분을 앞서 알아본 잠재 변수의 \\(\\mathbf{x}_0\\)에 대한 조건부 분포 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)에 대한 기댓값으로 바꾸기 위해 우변을 다음처럼 변형합니다.\n\\[\n\\begin{aligned}\n- \\log p_{\\theta}(\\mathbf{x}_0) &= - \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\int q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0) \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} \\left[ \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}  \\right] \\right)\n\\end{aligned}\n\\]\n위 식은 \\(\\mathbf{x}_0\\)에 대한 가능도는 \\(\\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\)를 \\(\\mathbf{x}_0\\)를 조건으로 하는 모든 존재 가능한 잠재 변수 \\(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0\\)에 대해서 평균을 취한것과 같다는 것을 이야기 합니다.\n여기서 볼록함수convex function에서 성립하는 젠슨 부등식Jensen’s Inequality를 적용하는데 젠슨 부등식을 다음과 같습니다.\n\\[\nf(\\mathbb{E}[x]) \\le \\mathbb{E}[f(x)]\n\\]\n유도하고 있는 식에서 $ -$는 볼록함수이므로 젠슨 부등식이 성립하고 이를 적용하면 다음과 같습니다.\n\\[\n\\begin{aligned}\n- \\log p_{\\theta}(\\mathbf{x}_0) &= - \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\int q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0) \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} \\left[ \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}  \\right] \\right) \\\\[10pt]\n&\\le \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\n\\end{aligned}\n\\]\n방금 유도한 식은 마이너스 로그 가능도 함수의 상한을 알려 줍니다. 그 상한은 잠재 변수에 대한 평균이라는 것도 이해했습니다. 이 상한을 최소화하면 마이너스 로그 가능도도 자연스럽게 작아질 수 밖에 없습니다.\n이제 좌변을 모든 \\(\\mathbf{x}_0\\)에 대한 평균으로 만듭니다. 그럼 다음과 같은 과정을 통해 DDPM 식(3)의 부등식 부분이 유도 됩니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] &= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[- \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right) \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log \\left( \\int q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0) \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}   d\\mathbf{x}_{1:T}\\right) \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log \\left( \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} \\left[ \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}  \\right] \\right) \\right] \\\\[10pt]\n&\\le \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\n\\end{aligned}\n\\]\n마지막 등식은 다음에 제시된 Law of total expectation \\(\\mathbb{E}[X] = \\mathbb{E}\\left[\\mathbb{E}[X \\mid Y]\\right]\\)으로 성립합니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(y)} \\left[ \\mathbb{E}_{q(x \\mid y)} \\left[ f(x,y) \\right] \\right] &= \\int_{y} q(y) \\int_{x} q(x \\mid y) f(x, y) dx dy\\\\[5pt]\n&= \\int_y \\int_x q(x \\mid y) q(y) f(x,y) dx dy \\\\[5pt]\n&= \\int_y \\int_x q(x, y) f(x,y) dx dy \\\\[5pt]\n&= \\mathbb{E}_{q(x,y)} [f(x,y)]\n\\end{aligned}\n\\]\n정리하면 다음처럼 됩니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] \\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] \\tag{3*}\n\\]\n식(3*)는 깨끗한 이미지 \\(\\mathbf{x}_0\\)의 \\(\\theta\\)에 대한 마이너스 로그 가능도는 우변보다 클 수 없다는 것을 나타냅니다. 여기서 우변을 Variational Bound라고 합니다. 왜 variational 이란 단어를 쓰냐면 이 상한은 잠재 변수의 분포로 제안된 함수 \\(q(\\mathbf{x}_{1:T}\\mid \\mathbf{x}_0)\\)에 따라 달라지기 때문입니다.\n지금까지 힘들게 겨우 식(3)의 앞 부분을 유도해봤는데 왜 이런 짓을 하고 있는지 이해하는것이 식을 구체적으로 유도하는 것 보다 더 중요할 것입니다. 이 과정을 조금 살펴보도록 하겠습니다.\n데이터 \\(\\mathbf{x}_0\\)에 대한 로그 가능도는 다음처럼 분해될 수 있습니다.\n\\[\n\\log p_\\theta(\\mathbf{x}_0) = \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right] + \\left( - \\mathbb{E}_{q(\\mathbf{x}_{1:T})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_{0})}{q(\\mathbf{x}_{1:T})} \\right] \\right)\n\\]\n위 분해가 어떻게 가능한지 조금 더 자세히 알고 싶은 독자는 EM 알고리즘에 대해서 정리한 A Step by Step Introduction to EM Algorithm를 참고하시면 좋겠습니다. 좀 긴 분량이 부담이 될 수 있지만 EM 알고리즘에 대해 국내서 가장 자세하고 친철히 설명한 문서라 할 수 있겠습니다.😁\n어쨌거나 이렇게 분해된 우변의 첫 번째 항을 Variational Low Bound 또는 Evidence Lower Bound(앞으로 ELBO로 표기)라고 하고 두 번째 항을 KD Divergence라고 합니다.(앞으로 \\(D_{KL}\\)로 표기) 이 \\(D_{KL}\\)은 두 확률분포의 차이를 측정하는 값으로 해석됩니다. 이 식에서 \\(D_{KL}\\) 항은 \\(p_\\theta(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)\\)와 \\(q(\\mathbf{x}_{1:T})\\)의 차이를 나타내는 값이 됩니다. 그리고 또 중요한 성질은 항상 0과 같거나 큰 값을 가지게 되고 그렇기 때문에 첫 번째 항이 가능도의 하한이 되게 됩니다. \\(D_{KL}\\)을 고려하지 않으면 다음처럼 쓸 수 있습니다.\n\\[\n\\log p_\\theta(\\mathbf{x}_0) \\ge \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right]  \n\\]\n여기서 우변에 \\(q(\\mathbf{x}_{1:T})\\)는 잠재변수 \\(\\mathbf{x}_{1:T}\\)의 분포에 대해서 알지 못하므로 임의로 제안된 분포입니다. 잠재변수에 대한 분포를 어떤 분포로 제안을 해도 위 식이 성립함을 의미합니다. 이 식과 유도된 식 (3*)을 비교하면\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] &\\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\\\\[10pt]\n\\log p_\\theta(\\mathbf{x}_0) &\\ge \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right]  \n\\end{aligned}\n\\]\n가 되는데 마이너스가 곱해져서 부등호의 방향이 반대인 것과 양변에 확률변수 \\(\\mathbf{x}_0\\)에 대한 기댓값 연산이 적용된 것이 차이입니다. 첫번째 식의 우변은 마이너스 로그 가능도 평균의 상한이 되고 두번째 식의 우변은 로그 가능도의 하한이 됩니다. 여기서 양변의 차이를 \\(D_{KL}\\)가 채우게 되는 것으로\n\\[\n\\log p_\\theta(\\mathbf{x}_0) = \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right]  + D_{KL}\n\\]\n처럼 되는데 로그 가능도의 분해식에서 \\(q(\\mathbf{x}_{1:T})\\)를 \\(q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_{0})\\)로 제안하면\n\\[\n\\log p_\\theta(\\mathbf{x}_0) = \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}\\mid \\mathbf{x}_0)} \\right] + D_{KL}\n\\]\n이 됩니다. \\(D_{KL}\\)을 무시하고 양변에 \\(\\mathbb{E}_{q(\\mathbf{x}_0)}[\\cdot]\\)를 적용하면\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\log p_\\theta(\\mathbf{x}_0)\\right] \\ge \\mathbb{E}_{q(\\mathbf{x}_{0:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}\\mid \\mathbf{x}_0)} \\right]\n\\]\n이 되고 모양이 식(3*)와 부등호 방향만 빼고 동일해졌습니다.\n이제 우변을 \\(\\theta\\)에 대해 최대화 시켜 값을 크게 하면 우변은 좌변의 하한이었으므로 좌변은 최대화로 커진 우변보다 항상 조금 더 커지게 되고 그 차이가 다시 0보다 큰 \\(D_{KL}\\)로 나타내게 됩니다.\n식(3*)대해서도 동일한 논리가 적용되는데 다른점은 하한이 상한으로 바뀌어 있기 때문에 우변을 최소화 하면 좌변은 항상 우변보다 더 작아지게 되는 것입니다. 때문에 식(3*)의 우변을 목적함수로 사용할 수 있게 됩니다.\n이제 유도된 부등식의 우변항을 \\(\\theta\\)에 대해서 최적화하는 것이 목적이라는 것을 잘 이해 했으므로 \\(\\theta\\)와 관련있는 항과 관련없는 항으로 분리하도록 합니다. 우변항 [ ] 안쪽 부분을 다음 과정으로 식을 전개할 수 있습니다.\n\\[\n\\begin{aligned}\n-\\log \\left( \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_{0})} \\right) &= - \\log \\left( \\frac{p(\\mathbf{x}_T) \\displaystyle\\prod_{t=1}^{T} p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{\\displaystyle\\prod_{t=1}^T q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right) \\\\[10pt]\n&=- \\log \\left( p(\\mathbf{x}_T) \\cdot \\frac{\\displaystyle\\prod_{t=1}^{T} p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{\\displaystyle\\prod_{t=1}^T q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right) \\\\[10pt]\n&= - \\log p(\\mathbf{x}_T) - \\log \\prod_{t=1}^T \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\\\[10pt]\n&= - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})}\n\\end{aligned}\n\\]\n전개된 부분을 다시 원식에 적용하면 식(3)이 완성됩니다. 최종적으로 가장 오른쪽에 있는 항을 목적함수 \\(L\\)로 정의합니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] \\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right] := L \\tag{3}\n\\]\n식(3)이 목적함수라면 네트워크를 학습시키는데 사용할 수 있다는 말인데 어떻게 사용할 수 있을까요? 식이 너무 복잡해서 뭐가 네트워크의 출력인지 뭐가 타겟인지 지도학습의 손실인지 비지도 학습의 손실인지 분명하게 잘 이해가 안됩니다.\n식(3)의 의미를 이해해보기 위해 간단한 실험을 해보도록 하겠습니다. 계산해야 하는 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)와 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)들은 잠재변수가 결합된 형태가 아니라 특정 시간 단계에서 한 단계 이전 또는 이후의 잠재변수에 대한 조건을 가지는 형태로 바뀌었습니다. 따라서 이전에 정의한 다음 두 식으로 부터 값을 계산할 수 있습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\]\n전술했듯이는 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t)\\), \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)는 네트워크가 출력하는 리버스 프로세스 한 단계에 대한 분포의 평균과 표준편차인데 여기서 실험을 간단히 하기 위해 \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) = \\beta_t \\mathbf{I}\\)로 가정하겠습니다.\n우선 적당한 \\(\\mathbf{x}_0\\)에 대해서 1단계에서 \\(T\\) 단계까지 노이즈가 더해진 이미지를 준비합니다. \\(T=20\\)으로 두도록 하겠습니다.\n\nx0.min(), x0.max()\n\n(-1.0, 1.0)\n\n\n\n%%time\n\nxts = [x0.reshape(-1)]\nxt = xts[0]\n\nT = 20\nL = xts[0].shape[0]\n\nfor t in range(1, T+1):\n    # t-1 단계에서 만들어진 이미지로 부터 분포 q(x_t|x_t-1)을 만든다.\n    q_xt_given_xtm1 = get_q_xt_given_xtm1(xt, t)\n\n    # 만들어진 분포에서 샘플링한다.\n    xt = q_xt_given_xtm1.rvs(size=1)\n    xts.append(xt)\n\nCPU times: user 6.4 s, sys: 1.97 s, total: 8.37 s\nWall time: 2.05 s\n\n\n그리고 현재 가진 네트워크가 잘 학습이 되어 있어서 \\(\\mathbf{x}_{20}\\)을 입력하면 \\(\\mathbf{x}_{19}\\)와 매우 유사한 이미지를 출력한다고 하겠습니다. 이를 그림으로 표현하면 다음처럼 표현할 수 있습니다. 그림을 통해 현재 상황을 직관적으로 이해하도록 합시다.\n\n# 샘플1에 대해서 x20을 입력했을 때 네트워크가 평균 x19를 출력했다고 가정\nx19_flt = xts[-2].copy()\n\n# x19에 약간의 노이즈를 더해 네트워크의 출력을 시뮬레이션합니다.\nx19_flt += 0.001 * np.random.randn(L)\nx19 = ((x19_flt - x19_flt.min()) / (x19_flt.max() - x19_flt.min())).clip(0,1)\n\nx20_flt = xts[-1].copy()\nx20 = ((x20_flt - x20_flt.min()) / (x20_flt.max() - x20_flt.min())).clip(0,1)\n\nfig, ax = plt.subplots(figsize=(10,3), nrows=1, ncols=3)\n\nax[0].imshow(x20.reshape(16,16,3))\nax[0].set_title(f\"$\\mathbf{{x}}_{{{T}}}$\")\nax[0].set_xticks([])\nax[0].set_yticks([])\n\nax[1].text(0.25, 0.47, 'network', fontsize=20)\ncon = ConnectionPatch(\n    xyA=(13, 7.5), coordsA=ax[0].transData,\n    xyB=(0.2, 0.5), coordsB=ax[1].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].set_frame_on(False)\ncon = ConnectionPatch(\n    xyA=(0.8, 0.5), coordsA=ax[1].transData,\n    xyB=(2, 7.5), coordsB=ax[2].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\n\n\nax[2].imshow(x19.reshape(16,16,3))\nax[2].set_title(f\"$\\mathbf{{\\mu}}_\\\\theta(\\mathbf{{x}}_{{{T}}},{T})$\")\nax[2].set_xticks([])\nax[2].set_yticks([])\n\nplt.show()\n\n\n\n\n이제 가상의 네트워크 출력을 평균으로 하는 정규분포를 정의합니다. 이것이 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)에 해당합니다. \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)는 위쪽 루프를 실행할 때 이미 마지막에 만들어졌습니다. 준비된 재료를 사용해 식(3)을 계산합니다.\n\\[\nL=\\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right]\n\\]\n이때 편의상 \\(\\sum_{t=1}^T\\) 부분을 모두 계산하지 않고 \\(T=20\\)에 해당하는 항 하나만 계산합니다. 또 $- p(_T) $ 부분도 \\(\\theta\\)와 무관한 부분이므로 계산하지 않습니다.\n\n# 네트워크의 출력을 평균으로 p(x_{t-1}| x_t)를 정의\np_xtm1_given_xt = multivariate_normal(mean=x19_flt, cov=beta(20)*np.eye(L))\n\n# 손실함수 값을 T=20에 대해서만 계산\n- (p_xtm1_given_xt.logpdf(x19_flt) - q_xt_given_xtm1.logpdf(x20_flt))\n\n-402.80940657034307\n\n\n샘플 \\(\\mathbf{x}_0\\) 하나와 연관된 특정 잠재변수 \\(\\mathbf{x}_{1:T}\\)에 대해서 가상으로 손실함수 값을 계산했습니다.\n이번에는 네트워크가 잘 학습되지 않아 \\(\\mathbf{x}_{20}\\)을 입력하면 엉뚱한 노이즈를 출력한다고 가정해보겠습니다. 다음 그림과 같은 상황입니다.\n\nfig, ax = plt.subplots(figsize=(10,3), nrows=1, ncols=3)\n\nax[0].imshow(x20.reshape(16,16,3))\nax[0].set_title(f\"$\\mathbf{{x}}_{{{T}}}$\")\nax[0].set_xticks([])\nax[0].set_yticks([])\n\nax[1].text(0.25, 0.47, 'network', fontsize=20)\ncon = ConnectionPatch(\n    xyA=(13, 7.5), coordsA=ax[0].transData,\n    xyB=(0.2, 0.5), coordsB=ax[1].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].set_frame_on(False)\ncon = ConnectionPatch(\n    xyA=(0.8, 0.5), coordsA=ax[1].transData,\n    xyB=(2, 7.5), coordsB=ax[2].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\n\nnoise = np.random.randn(L)\nnoise = ((noise - noise.min()) / (noise.max() - noise.min())).clip(0,1)\nax[2].imshow(noise.reshape(16,16,3))\nax[2].set_title(f\"$\\mathbf{{\\mu}}_\\\\theta(\\mathbf{{x}}_{{{T}}},{T})$\")\nax[2].set_xticks([])\nax[2].set_yticks([])\n\nplt.show()\n\n\n\n\n이전과 동일하게 이 노이즈를 평균으로 하는 정규분포를 정의의하고 같은 계산을 반복합니다. 모든 계산이 문제가 없다면 이번 값은 이전 값보다 매우 커야 합니다.\n\n# 네트워크의 출력을 평균으로 p(x_{t-1}| x_t)를 정의\np_xtm1_given_xt = multivariate_normal(mean=noise, cov=beta(20)*np.eye(L))\n\n# 손실함수 값을 T=20에 대해서만 계산\n- (p_xtm1_given_xt.logpdf(x19_flt) - q_xt_given_xtm1.logpdf(x20_flt))\n\n703501.3757760313\n\n\n예상처럼 상당히 큰 값이 얻어졌습니다. 이것으로 식(3)이 손실 함수로 작동할 수 있다는 것을 실험적으로 확인했습니다. 또 손실 함수를 계산하기 위해서 시작 이미지 \\(\\mathbf{x}_0\\)만 있으면 \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_T\\)와 같은 필요한 모든 요소들을 즉석에서 만들어 낼 수 있는 것도 알았습니다. 다시말해 별도의 타겟이 필요없다는 의미입니다.\n지금까지 유도된 손실함수를 직접 계산할 수 있음을 알아봤습니다. 이를 위해서 임의의 샘플 \\(\\mathbf{x}_0\\)를 선택하고 이로부터 노이즈를 확산시켜 \\(\\mathbf{x}_1, \\mathbf{x}_2, ... , \\mathbf{x}_T\\)를 얻습니다. 이 과정에서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)을 단계적으로 계산했습니다. 즉 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)를 계산하기 위해서 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t-2})\\)를 반드시 거쳐야 했습니다. 실험에서 \\(T=20\\)으로 두었음에도 이는 상당한 시간이 걸리는 것을 알 수 있습니다. \\(T=1000\\)정도로 노이즈 확산 단계를 설정한다면 이 과정을 계산하는 시간은 현실적이지 못합니다.\n임의 시간 단계 \\(t\\)에 대해서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)를 바로 정의할 수 있다면 임의 시간 단계의 노이즈가 확산된 샘플을 바로 얻을 수 있어 학습의 효율을 높일 수 있습니다. \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\) 정의와 reparameterization 트릭을 이용하면 가능합니다. 어떻게 하는지 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)에서 인덱스 \\(t\\)를 2로 두고 구체적인 예를 통해 알아봅시다.\n노이즈가 2단계 확산된 확률변수 \\(\\mathbf{x}_2\\)는 다음처점 정의된 정규 분포를 따릅니다.\n\\[\n\\mathbf{x}_2 \\sim q(\\mathbf{x}_2 \\mid \\mathbf{x}_1) := \\mathcal{N}(\\mathbf{x}_2; \\sqrt{1-\\beta_2}\\mathbf{x}_1, \\beta_2\\mathbf{I})\n\\]\n한편 확률변수 \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)에 대한 표준화는 다음과 같습니다.\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\n표준화를 이용하면 확률변수 \\(X\\)는 다음처럼 나타낼 수 있습니다.\n\\[\nX = \\mu + \\sigma Z, \\qquad Z \\sim \\mathcal{N}(0, 1)\n\\]\n이렇게 \\(X\\)가 따르는 분포로 부터 직접 샘플링하지 않고 표준 정규분포에서 샘플링한 샘플을 \\(X\\)로 변환하는 방법을 reparametization 트릭이라고 합니다. 이 트릭을 사용하면 샘플 \\(\\mathbf{x}_2 \\sim q(\\mathbf{x}_2 \\mid \\mathbf{x}_1)\\)는 다음처럼 쓸 수 있습니다.\n\\[\n\\mathbf{x}_2 = \\sqrt{\\alpha_2} \\mathbf{x}_1 + \\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1, \\qquad \\boldsymbol{\\epsilon}_1 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\n여기서 \\(\\alpha_t := 1- \\beta_t\\)로 두었습니다. 같은 방법으로 \\(\\mathbf{x}_1 \\sim q(\\mathbf{x}_1 \\mid \\mathbf{x}_0)\\) 도 다음처럼 쓸 수 있습니다.\n\\[\n\\mathbf{x}_1 = \\sqrt{\\alpha_1} \\mathbf{x}_0 + \\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0, \\qquad \\boldsymbol{\\epsilon}_0 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\n이 두 관계를 이용하면 다음처럼 \\(\\mathbf{x}_2\\)를 정리할 수 있습니다.\n\\[\n\\begin{align}\n\\mathbf{x}_2 &= \\sqrt{\\alpha_2} \\mathbf{x}_1 + \\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1 \\\\[10pt]\n&= \\sqrt{\\alpha_2} \\left( \\sqrt{\\alpha_1} \\mathbf{x}_0 + \\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0 \\right) + \\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1 \\\\[10pt]\n&= \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0 + \\underbrace{\\underbrace{\\sqrt{\\alpha_2} \\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0}_{\\sim \\mathcal{N}(\\mathbf{0}, \\alpha_2(1-\\alpha_1)\\mathbf{I})} + \\underbrace{\\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1}_{\\sim \\mathcal{N}(\\mathbf{0}, (1-\\alpha_2)\\mathbf{I})}}_{\\sim \\mathcal{N}(\\mathbf{0}, (\\alpha_2(1-\\alpha_1)+(1-\\alpha_2))\\mathbf{I})} \\quad (**) \\\\[5pt]\n&= \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0 + \\sqrt{\\alpha_2 (1-\\alpha_1) + (1-\\alpha_2)} \\boldsymbol{\\epsilon}^*_0 \\quad(*) \\\\[10pt]\n&= \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0 + \\sqrt{1- \\alpha_1 \\alpha_2} \\boldsymbol{\\epsilon}^*_0\n\\end{align}\n\\]\n세 번째 식 (**)에서 \\(\\sqrt{\\alpha_2}\\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0\\)는 reparameterization 트릭에 의해 \\(\\mathcal{N}(\\mathbf{0}, \\alpha_2(1-\\alpha_1)\\mathbf{I})\\)에서 샘플링 된 샘플이고 \\(\\sqrt{1-\\alpha_2}\\boldsymbol{\\epsilon}_1\\)은 \\(\\mathcal{N}(\\mathbf{0}, (1-\\alpha_2)\\mathbf{I})\\)에서 샘플링된 샘플입니다. 두 분포에서 샘플링된 샘플 두개를 더하고 있습니다. 서로 독립인 두 정규 분포를 따르는 확률변수를 합한 확률 변수가 따르는 분포는 각 정규분포의 평균과 분산이 더해진 정규분포입니다. 즉 다음이 성립합니다.\n\\[\n\\begin{aligned}\nX &\\sim \\mathcal{N}(\\mu_X, \\sigma^2_X) \\\\[5pt]\nY &\\sim \\mathcal{N}(\\mu_Y, \\sigma^2_Y) \\\\[5pt]\nX+Y &\\sim \\mathcal{N}(\\mu_X+\\mu_Y, \\sigma^2_X+\\sigma^2_Y)\n\\end{aligned}\n\\]\n이에 대한 증명은 sum of two independent Gaussian random variables를 참고하시기 바랍니다.\n이를 이용하면 \\(\\sqrt{\\alpha_2}\\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0 + \\sqrt{1-\\alpha_2}\\boldsymbol{\\epsilon}_1\\)는 \\(\\mathcal{N}(\\mathbf{0}, (\\alpha_2(1-\\alpha_1)+(1-\\alpha_2))\\mathbf{I})\\)를 따르는 샘플이란 것을 알 수 있습니다. 따라서 새로운 표준 정규분포로 부터 샘플링된 샘플 \\(\\boldsymbol{\\epsilon}^*\\)를 사용해서 네 번째 식 (*)처럼 표현 가능합니다.\n정리하면 \\(\\mathbf{x}_2\\)는 평균이 \\(\\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0\\)고 표준편차가 \\(\\sqrt{1- \\alpha_1 \\alpha_2}\\)인 정규분포를 따르게 됩니다.\n이를 다시 쓰면\n\\[\n\\mathbf{x}_2 \\sim  \\mathcal{N}(\\mathbf{x}_2 ; \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0, (1- \\alpha_1 \\alpha_2)\\mathbf{I}) = q(\\mathbf{x}_2 \\mid \\mathbf{x}_0)\n\\]\n이를 \\(\\mathbf{x}_t\\)와 \\(\\mathbf{x}_0\\) 사이의 일반적인 관계로 확장하면 다음을 얻을 수 있습니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, (1- \\bar{\\alpha}_t)\\mathbf{I}), \\qquad \\alpha_t := 1-\\beta_t, \\quad \\bar{\\alpha}_t := \\prod_{s=1}^t \\alpha_s \\tag{4}\n\\]\n유도된 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)로 임의 시간 단계에서 노이즈가 확신된 샘플을 얻어보도록 합시다. 이전에 beta() 함수를 만들어 사용했습니다. 하지만 \\(\\beta_t\\)를 학습하지 않고 일정한 시간 단계에서 상수로 설정할 것이기 때문에 전체 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar{\\alpha}_t\\)를 한꺼번에 만들어 두는 것이 효과적입니다. 노이즈를 확산시키는 스케쥴은 일정하게 고정한다는 의미입니다. 다시 한번 논문에서 밝힌 다음 조건을 이용하여 \\(T\\) 길이를 가지는 beta, alpha, alpha_bar를 계산합니다.\n\n\\(T=1000\\), \\(\\beta_1 = 10^{-4}\\), \\(\\beta_T = 0.02\\)\n\n\nbeta_1 = 1e-4\nbeta_T = 0.02\nT = 1000\n\nbeta = np.concatenate( (np.array([0.]), np.linspace(beta_1, beta_T, T)) )\nalpha = 1 - beta\nalpha_bar = np.exp( np.cumsum(np.log(alpha)) )\n\nbeta[1], beta[-1], alpha[1], alpha[-1], alpha_bar[1], alpha_bar[-1]\n\n(0.0001, 0.02, 0.9999, 0.98, 0.9999, 4.035829765375694e-05)\n\n\n만들어진 alpha_bar를 이용해서 \\(\\sqrt{\\bar{\\alpha}_t}\\), \\(\\sqrt{1-\\bar{\\alpha}_t}\\)가 \\(t\\)에 대해서 어떻게 변하는지 확인해보겠습니다.\n\na = np.array([ [np.sqrt(alpha_bar[t]), np.sqrt(1-alpha_bar[t])] for t in range(1, T+1)])\n\nplt.plot(a[:,0], label=r\"$\\sqrt{\\bar{\\alpha}_t}$\")\nplt.plot(a[:,1], label=r\"$\\sqrt{1-\\bar{\\alpha}_t}$\")\n\nplt.legend()\nplt.show()\n\n\n\n\n시간 단계가 흐를 수록 \\(\\sqrt{\\bar{\\alpha}_t}\\)는 0이 되고, \\(\\sqrt{1-\\bar{\\alpha}_t}\\)는 1이 됩니다. 따라서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, (1- \\bar{\\alpha}_t)\\mathbf{I})\\)는 노이즈가 확산 될 수록 점점 표준 정규분포가 되어 간다는 것을 알 수 있습니다.\n샘플 \\(\\mathbf{x}_0\\)에 \\(\\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\)로 \\(t\\) 단계 노이즈를 확신 시키는 함수를 정의합니다. 이 때 샘플 x의 모양은 채널이 이미지 크기보다 먼저오는 (N, C, H, W)로 입력 받도록 하겠습니다.\n\n# 숫자값이 들어있는 1차원 어레이에 대한 인덱싱 결과가 4차원 어레이가 되도록 인덱싱 합니다.\nalpha_bar[[1], None, None, None].shape\n\n(1, 1, 1, 1)\n\n\n\ndef perturb_x(x, t, eps):\n    # x: (N, C, H, W)\n\n    # x와 alpha_bar[t]가 브로드캐스팅되도록 차원을 늘린수 곱하고 더함\n    # alpha_bar[t, None, None, None]: (N,1,1,1)\n    # x, eps: (N,C,H,W)\n    # 연산결과: (N,C,H,W)\n    return np.sqrt(alpha_bar[t, None, None, None]) * x \\\n            + np.sqrt(1 - alpha_bar[t, None, None, None]) * eps\n\nsprites로부터 적당히 샘플 다섯개를 가져오고 임의 시간 단계 t를 만든 다음 샘플 이미지에 적용해보겠습니다.\n\nnp.random.seed(102)\nidx = np.random.randint(0, sprites.shape[0], 5)\n\nx = (sprites[idx].transpose(0, 3, 1, 2)  / 255) * 2 - 1\n\nt = np.random.randint(1, T+1, (x.shape[0],))\nnoise = np.random.randn(*x.shape)\nxt = perturb_x(x, t, noise)\nxt_prev = perturb_x(x, t-1, noise)\n\n\nfig, ax = plt.subplots(figsize=(10,7), nrows=3, ncols=5)\n\nfor i in range(5):\n    x_i = ((x[i] - x[i].min()) / (x[i].max() - x[i].min())).clip(0,1)\n    ax[0][i].imshow(x_i.transpose(1,2,0))\n    ax[0][i].set_title(f\"t={0}\")\n    ax[0][i].set_xticks([])\n    ax[0][i].set_yticks([])\n\n    x_i = ((xt_prev[i] - xt_prev[i].min()) / (xt_prev[i].max() - xt_prev[i].min())).clip(0,1)\n    ax[1][i].imshow(x_i.transpose(1,2,0))\n    ax[1][i].set_title(f\"t={t[i]-1}\")\n    ax[1][i].set_xticks([])\n    ax[1][i].set_yticks([])\n\n    x_i = ((xt[i] - xt[i].min()) / (xt[i].max() - xt[i].min())).clip(0,1)\n    ax[2][i].imshow(x_i.transpose(1,2,0))\n    ax[2][i].set_title(f\"t={t[i]}\")\n    ax[2][i].set_xticks([])\n    ax[2][i].set_yticks([])\n\n    if i == 0:\n        ax[0][i].set_ylabel(r\"$\\mathbf{x}_0$\", size=15)\n        ax[1][i].set_ylabel(f\"$\\mathbf{{x}}_{{t-1}}$\", size=15)\n        ax[2][i].set_ylabel(f\"$\\mathbf{{x}}_{{t}}$\", size=15)\n\nplt.show()\n\n\n\n\n이미지 위에 시간 단계를 표시했습니다. 시간 단계를 건너뛰면서 빠르게 노이즈가 추가되는것을 확인할 수 있습니다. 샘플링이 훨씬 빠르게 가능해졌으므로 이 샘플들을 이용해서 효율적으로 손실함수에 나타나는 \\(p_\\theta(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t)\\), \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)를 계산할 수 있게 되었습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\]\n지금까지 식(3)에 대해서 자세히 알아봤습니다. 이론적인 부분 뿐만 아니라 실제 학습에 사용할 수 있을 정도로 코드 수준에서 값을 직접 계산까지 해봤습니다.\nDDPM 논문을 보면 이렇게 유도된 손실함수를 바로 사용하지 않고 한번 더 형태를 변형하여 손실함수를 좀 더 간단하게 만들어 사용합니다. DDPM 식(3)에서 식(5)를 유도하는 과정입니다. 논문 부록에 이 과정이 아주 간략하게 소개 되어 있으므로 여기서 자세히 알아보도록 하겠습니다. 다시 식(3)을 보겠습니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] \\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right] := L \\tag{3}\n\\]\n우선 \\(\\sum\\) 이하 로그 분수 부분을 변형하기 위해 분모를 봅시다. 분모에 베이즈 정리를 적용하면 다음처럼 쓸 수 있습니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})=\\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t) q(\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1})}\n\\]\n앞서 가정한 마르코프 성질에 의해 좌변은 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})=q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)\\) 이며 우변의 다른 항들도 마찬가지 성질을 보이므로 다음처럼 써도 무방합니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})= q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)=\\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}\n\\]\n이 결과를 \\(\\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})}\\)에 대입하고 정리합니다.\n\\[\n\\begin{aligned}\n\\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} &=\n\\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{\\dfrac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}}\\\\[10pt]\n&= \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)  \\dfrac{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}} \\\\[10pt]\n&= \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}\n\\end{aligned}\n\\]\n위 결과를 식(3)에 대입하고 정리합니다.\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\quad (20)\n\\end{aligned}\n\\]\n여기까지가 논문 부록에 제시된 식(20)까지 입니다. 로그 곱셈을 덧셈으로 바꾸고 정리합니다.\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\left\\{ \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} + \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)} \\right\\} - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right]\n\\end{aligned}\n\\]\n세 번째 항에 합산 기호를 전개하면 대부분 항이 삭제되고 다음처럼 됩니다.\n\\[\n\\begin{aligned}\n-\\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)} = - &( \\log q(\\mathbf{x}_1\\mid\\mathbf{x}_0) - \\log q(\\mathbf{x}_2\\mid\\mathbf{x}_0) ) \\\\\n- &( \\log q(\\mathbf{x}_2\\mid\\mathbf{x}_0) - \\log q(\\mathbf{x}_3\\mid\\mathbf{x}_0) ) - \\cdots \\\\[10pt]\n- &( \\log q(\\mathbf{x}_{T-1}\\mid\\mathbf{x}_0) - \\log q(\\mathbf{x}_T\\mid\\mathbf{x}_0) ) \\\\[10pt]\n=  - &\\log q(\\mathbf{x}_1 \\mid \\mathbf{x}_0 ) + \\log q(\\mathbf{x}_T \\mid \\mathbf{x}_0)\n\\end{aligned}\n\\]\n이를 원 식에 대입하고 정리하면 논문의 식(21)이 완성됩니다.\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log q(\\mathbf{x}_1 \\mid \\mathbf{x}_0 ) + \\log q(\\mathbf{x}_T \\mid \\mathbf{x}_0) - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) + \\log q(\\mathbf{x}_T \\mid \\mathbf{x}_0) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log q(\\mathbf{x}_1 \\mid \\mathbf{x}_0 )  - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) + \\log q(\\mathbf{x}_1\\mid\\mathbf{x}_0) \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad(21)\n\\end{aligned}\n\\]\n식(21)에서 기댓값 연산을 쪼개고 나서 각 항에 \\(D_{KL}\\)을 적용합니다.\n\\[\n\\begin{aligned}\nL =& \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad(21) \\\\[10pt]\n=& \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)}  \\right] + \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\right] + \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\\\[10pt]\n=& \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} \\right] \\right] \\quad (\\ast) \\\\[10pt]\n+ &\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[  - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\quad (\\ast\\ast)\\\\[10pt]\n+ &\\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad (\\ast\\ast\\ast)\n\\end{aligned}\n\\]\n이렇게 분리된 기댓값 연산 (*), (**), (***)를 \\(D_{KL}\\) 형태로 정리하면 식(5)가 완성됩니다. 여기서 \\(D_{KL}\\)의 정의를 다시 확인하고 이 꼴로 위 식을 변형해야 합니다. \\(D_{KL}\\)은 연속 확률 변수 \\(P \\sim p(x)\\), \\(Q \\sim q(x)\\)에 대해서 다음과 같이 정의 됩니다.\n\\[\nD_{KL}(Q || P) = - \\int  q(x) \\log \\frac{p(x)}{q(x)} dx\n\\]\n\\(D_{KL}\\)의 정의와 (*)를 비교해보면 (*)에서 적분 변수는 \\(\\mathbf{x}_T\\)라는 것을 알 수 있습니다. 나머지 변수들을 주변화 시킬 수 있습니다. 간단한 스칼라 함수에 대한 주변화 과정을 다음에 정리했습니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(x_1, x_2, x_3)} [ f(x_1, x_2) ] &= \\int_{x_3} \\int_{x_2} \\int_{x_1} q(x_1, x_2, x_3) f(x_1, x_2) dx_1 dx_2 dx_3 \\\\[10pt]\n&= \\int_{x_2} \\int_{x_1} \\int_{x_3} q(x_1, x_2, x_3) f(x_1, x_2) dx_3 dx_1 dx_2 \\quad  \\because \\text{Fubini's theorem} \\\\[10pt]\n&=\\int_{x_2} \\int_{x_1} f(x_1, x_2)  \\int_{x_3} q(x_1, x_2, x_3) dx_3 dx_1 dx_2 \\\\[10pt]\n&= \\int_{x_2} \\int_{x_1} f(x_1, x_2)  dx_1 dx_2 \\quad \\because \\text{marginalization}\\\\[10pt]\n&= \\mathbb{E}_{q(x_1,x_2)} [f(x_1, x_2)]\n\\end{aligned}\n\\]\n적분 변수 \\(x_3\\)이 사라졌습니다. 이처럼 (*)에서 \\(\\mathbf{x}_1\\), … \\(\\mathbf{x}_{T-1}\\)을 모두 주변화 해서 없애면 다음처럼 정리할 수 있습니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} \\right] \\right] = \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{T}\\mid\\mathbf{x}_0)} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} \\right] \\right] = \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ D_{KL}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) || p(\\mathbf{x}_T)) \\right] \\quad (*)\n\\]\n(**) 부분도 마찬가지로 주변화 과정을 거치고 다음처럼 \\(D_{KL}\\) 형태로 바꿀 수 있습니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[  - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[   \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t-1},\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[    \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[  - \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[  \\mathbb{E}_{q(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t, \\mathbf{x}_0)}\\left[   \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[   - \\mathbb{E}_{q(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t, \\mathbf{x}_0)}\\left[  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[     D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))  \\right] \\right] \\\\[10pt]\n&=\\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_0, \\mathbf{x}_t)} \\left[      D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))  \\right] \\quad (**)\n\\end{aligned}\n\\]\n(***) 부분도 간단히 주변화 해서 다음처럼 정리한 다음\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[   - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] &= \\mathbb{E}_{q(\\mathbf{x}_{0})} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} [- \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) ] \\right] \\\\[5pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0})} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1} \\mid \\mathbf{x}_0)} [- \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) ] \\right] \\\\[5pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0}, \\mathbf{x}_1)} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad (***)\n\\end{aligned}\n\\]\n유도된 세 부분을 함께 쓰면 다음과 같습니다.\n\\[\nL = \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ D_{KL}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) || p(\\mathbf{x}_T)) \\right] + \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_0, \\mathbf{x}_t)} \\left[      D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))  \\right] + \\mathbb{E}_{q(\\mathbf{x}_{0}, \\mathbf{x}_1)} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right]\n\\]\n이제 각 항에 주변화해서 없앤 적분 변수를 다시 넣어서 \\(\\mathbf{x}_{0:T}\\)에 대한 적분으로 바꾸면 다음처럼 정리 됩니다.\n\\[\nL = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ \\underbrace{D_{KL}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) || p(\\mathbf{x}_T))}_{L_T} +   \\sum_{t=2}^T    \\underbrace{D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))}_{L_{t-1}}   \\underbrace{ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}_{L_0} \\right] \\tag{5}\n\\]\n이렇게 식(5)를 완전히 유도 했습니다.\n식(5)는 손실 함수 식(3)이 \\(L_T\\), \\(L_{t-1}\\), \\(L_0\\) 세 부분으로 변형된 형태이며 앞 두 항은 \\(D_{KL}\\)이고 마지막 항 \\(L_0\\)은 리버스 프로세스에서 마지막 과정에 해당하는 마이너스 로그 가능도 입니다. 이 세 항을 모두 줄여야 하는 사실은 변함이 없고 식(3)에서 형태만 바뀐 상태입니다.\n각 항의 의미를 살펴보도록 합시다.\n\n\n\\(\\mathbf{x}_0\\)로부터 완전히 노이즈 상태가 된 \\(\\mathbf{x}_T\\)의 분포가 표준 정규분포 \\(p(\\mathbf{x}_T)\\)와 얼마나 비슷한지를 나타냅니다. 이 항은 파라미터 \\(\\theta\\)에 독립적인 항이므로 무시할 수 있습니다.\n\n\n\n\n복잡한 이야기를 다 떠나서 우리가 궁극적으로 알고 싶은 것은 \\(q(\\mathbf{x}_0 \\mid \\mathbf{x}_T)\\) 또는 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)입니다. 전자는 모든 시간 단계를 건더뛰고 완전 노이즈와 이미지를 바로 연결하는 조건부 분포이므로 이를 직접 알기는 힘듭니다. 대신 각 시간 단계에 대한 리버스 프로세스를 알려주는 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 알아도 됩니다. \\(q(\\mathbf{x} \\mid \\mathbf{x}_{t-1})\\)를 알고 있기 때문에 여기에 베이즈 정리를 이용해서 이론적으로 유도 할 수 있지만 실제 계산을 위해서 모든 시간 단계에 대한 \\(q(\\mathbf{x}_t)\\)를 알아야 합니다.\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})q(\\mathbf{x}_{t-1})}{q(\\mathbf{x}_t)}\n\\]\n\\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)는 알 수 없지만 식(5)에 나타난 포워드 프로세스의 posterior \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 계산할 수 있습니다. 이를 위해 먼저 다음 베이즈 정리를 이용합니다.\n\\[\n\\begin{aligned}\np(x \\mid y, z) &= \\frac{p(x, y, z)}{p(y,z)} \\\\[5pt]\n&= \\frac{p(y \\mid x, z)p(x,z)}{p(y,z)} \\\\[5pt]\n&= \\frac{p(y \\mid x, z)p(x \\mid z)p(z)}{p(y \\mid z)p(z)} \\\\[5pt]\n&= \\frac{p(y \\mid x, z)p(x \\mid z)}{p(y \\mid z)}\n\\end{aligned}\n\\]\n위 베이즈 정리를 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)에 적용합니다.\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)=\\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)} = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\]\n두 번째 등호는 마르코프 가정을 사용했습니다. 이제 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)가 계산할 수 있는 항 3개로 변환되었습니다. \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)\\), \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)는 식(4)에 의해 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)는 포워드 프로세스의 정의에 의해 계산할 수 있습니다. 이 계산을 마치면 DDPM의 식(6), (7)이 유도됩니다. 이 과정은 어렵진 않지만 상당히 지저분하고 지루한 과정이므로 다음 제시된 계산 과정은 스킵하여도 무방합니다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &=\\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)} \\\\[10pt]\n&= \\frac{\\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) \\,\\, \\mathcal{N}(\\mathbf{x}_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0, (1-\\bar{\\alpha}_{t-1})\\mathbf{I})}{\\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_{t})\\mathbf{I})} \\quad (*)\n\\end{aligned}\n\\]\n한편 \\(\\mathbf{x} \\in \\mathbb{R}^D\\)인 벡터 변수에 대한 정규분포는 다음과 같고\n\\[\n\\mathcal{N}(\\mathbf{x}, \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(\\sqrt{2\\pi})^D \\sqrt{|\\Sigma|}} \\exp \\left\\{-\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}) \\right\\}\n\\]\n지금 다루고 있는 $(t ; {t-1}, _t ) $처럼 공분산 행렬이 \\(\\Sigma = \\beta \\, \\mathbf{I}\\)인 경우 다음처럼 간소화 됩니다.\n\\[\n\\mathcal{N}(\\mathbf{x}, \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(\\sqrt{2\\pi})^D \\sqrt{\\beta^D}} \\exp\\left\\{-\\frac{1}{2\\beta} (\\mathbf{x}-\\boldsymbol{\\mu})^T (\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\n\\]\n그리고 벡터의 행렬곱은 다음처럼 전개할 수 있으므로\n\\[\n\\begin{aligned}\n(\\mathbf{x}-\\boldsymbol{\\mu})^T (\\mathbf{x}-\\boldsymbol{\\mu}) &= (\\mathbf{x}^T-\\boldsymbol{\\mu}^T)(\\mathbf{x}-\\boldsymbol{\\mu}) \\\\[5pt]\n&= \\mathbf{x}^T \\mathbf{x} - \\mathbf{x}^T \\boldsymbol{\\mu} - \\boldsymbol{\\mu}^T \\mathbf{x} + \\boldsymbol{\\mu}^T\\boldsymbol{\\mu} \\\\[5pt]\n&= \\lVert\\mathbf{x}\\rVert^2_2 - 2 \\mathbf{x}^T \\boldsymbol{\\mu} + \\lVert \\boldsymbol{\\mu} \\rVert^2_2\n\\end{aligned}\n\\]\n이를 이용해 (*)를 전개합니다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= \\frac{\\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) \\,\\, \\mathcal{N}(\\mathbf{x}_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0, (1-\\bar{\\alpha}_{t-1})\\mathbf{I})}{\\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_{t})\\mathbf{I})} \\quad (*) \\\\[10pt]\n&= \\frac{ \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\alpha_t)^D}} \\exp\\left\\{-\\frac{1}{2(1-\\alpha_t)} (\\mathbf{x}_t - \\sqrt{\\alpha_t}\\mathbf{x}_{t-1})^T (\\mathbf{x}_t - \\sqrt{\\alpha_t}\\mathbf{x}_{t-1}) \\right\\} \\times \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t-1})^D}} \\exp\\left\\{-\\frac{1}{2(1-\\bar{\\alpha}_{t-1})} (\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0})^T (\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0}) \\right\\}  }{  \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}} \\exp\\left\\{-\\frac{1}{2(1-\\bar{\\alpha}_{t})} (\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0})^T (\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}) \\right\\}  } \\\\[10pt]\n&=\\frac{ \\frac{1}{(2\\pi)^D\\sqrt{(1-\\alpha_t)^D (1-\\bar{\\alpha}_{t-1})^D}} \\exp\\left\\{ -\\left(  \\overbrace{\\frac{\\lVert \\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1} \\rVert^2_2}{2(1-\\alpha_t)}}^{A} + \\overbrace{\\frac{\\lVert \\mathbf{x}_{t-1} - \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_{0} \\rVert^2_2}{2(1-\\bar{\\alpha}_{t-1})} }^{B} \\right) \\right\\}  }{ \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}} \\exp \\left\\{ - \\frac{1}{2(1-\\bar{\\alpha}_{t})} \\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 \\right\\} } \\\\[10pt]\n&= \\underbrace{\\frac{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}}{(2\\pi)^D\\sqrt{(1-\\alpha_t)^D (1-\\bar{\\alpha}_{t-1})^D}}}_{C_1} \\exp \\left\\{ -(A+B) - \\left(- \\frac{1}{2(1-\\bar{\\alpha}_{t})} \\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 \\right)  \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{ - \\left( \\frac{\\lVert \\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1} \\rVert^2_2}{2(1-\\alpha_t)} +  \\frac{\\lVert \\mathbf{x}_{t-1} - \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_{0} \\rVert^2_2}{2(1-\\bar{\\alpha}_{t-1})} - \\frac{\\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 }{2(1-\\bar{\\alpha}_{t})} \\right) \\right\\}\n\\end{aligned}\n\\]\n마지막에 이 상수 \\(C_1\\)이 어떤 정규분포의 정규화 상수와 일치하게 되는 것을 확인할 있습니다.\n이제 \\(||\\cdot||^2_2\\) 부분을 전개하고\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= C_1 \\exp \\left\\{ - \\left( \\frac{\\lVert \\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1} \\rVert^2_2}{2(1-\\alpha_t)} +  \\frac{\\lVert \\mathbf{x}_{t-1} - \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_{0} \\rVert^2_2}{2(1-\\bar{\\alpha}_{t-1})} - \\frac{ \\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 }{2(1-\\bar{\\alpha}_{t})} \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\\\\n& \\left\\{ - \\frac{1}{2} \\left( \\frac{ \\lVert\\mathbf{x}_t \\rVert^2_2 - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert\\mathbf{x}_{t-1} \\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert\\mathbf{x}_{t-1} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{t-1}^T \\mathbf{x}_{0} + \\bar\\alpha_{t-1} \\lVert \\mathbf{x}_{0} \\rVert^2_2  }{(1-\\bar{\\alpha}_{t-1})} - \\frac{ \\lVert \\mathbf{x}_{t} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{t}^T \\mathbf{x}_{0} + \\bar\\alpha_{t} \\lVert \\mathbf{x}_{0} \\rVert^2_2  }{(1-\\bar{\\alpha}_{t})} \\right) \\right\\}\n\\end{aligned}\n\\]\n\\((\\cdot)\\) 안을 \\(\\mathbf{x}_{t-1}\\), \\(\\mathbf{x}_0\\), \\(\\mathbf{x}_t\\) 로 나눠서 정리합니다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= C_1 \\exp \\\\\n& \\left\\{ - \\frac{1}{2} \\left( \\frac{ - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert\\mathbf{x}_{t-1}\\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert\\mathbf{x}_{t-1} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{t-1}^T \\mathbf{x}_{0} }{(1-\\bar{\\alpha}_{t-1})} + \\overbrace{\\frac{ \\lVert\\mathbf{x}_t\\rVert^2_2}{1-\\alpha_t} + \\frac{\\bar\\alpha_{t-1} \\lVert \\mathbf{x}_0 \\rVert^2_2}{1-\\bar\\alpha_{t-1}}- \\frac{ \\lVert \\mathbf{x}_{t} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{t}^T \\mathbf{x}_{0} + \\bar\\alpha_{t} \\lVert \\mathbf{x}_{0} \\rVert^2_2  }{(1-\\bar{\\alpha}_{t})}}^{C_2(\\mathbf{x}_0, \\mathbf{x}_t)} \\right) \\right\\}\n\\end{aligned}\n\\]\n\\(C_2(\\mathbf{x}_0, \\mathbf{x}_t)\\) 부분은 \\(\\mathbf{x}_{t-1}\\)과 아무 상관이 없는 항입니다. 지금 계산하고 있는 분포 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)의 변수는 \\(\\mathbf{x}_{t-1}\\)임을 다시 한번 상기합시다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= C_1 \\exp \\left\\{ - \\frac{1}{2} \\left( \\frac{ - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert\\mathbf{x}_{t-1}\\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert\\mathbf{x}_{t-1}\\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{t-1}^T \\mathbf{x}_{0} }{(1-\\bar{\\alpha}_{t-1})} + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{ - \\frac{1}{2} \\left( \\frac{ - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert \\mathbf{x}_{t-1} \\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0}^T \\mathbf{x}_{t-1} }{(1-\\bar{\\alpha}_{t-1})} + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{ -\\frac{1}{2} \\left( \\frac{\\alpha_t \\lVert \\mathbf{x}_{t-1} \\rVert^2_2}{1-\\alpha_t} + \\frac{ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2}{1-\\bar\\alpha_{t-1}} + \\frac{- 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1}}{1-\\alpha_t} + \\frac{- 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0}^T \\mathbf{x}_{t-1}}{1-\\bar\\alpha_{t-1}} + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2}\\left( \\left( \\frac{\\alpha_t}{1-\\alpha_t} + \\frac{1}{1-\\bar\\alpha_{t-1}} \\right) \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}} \\right) \\mathbf{x}_{t-1}  + C_2(\\mathbf{x}_0, \\mathbf{x}_t)  \\right)  \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2}\\left( \\left( \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} \\right) \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}} \\right) \\mathbf{x}_{t-1}  + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2} \\left( \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} \\right) \\left[ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}}}{\\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}} \\right) \\mathbf{x}_{t-1} + \\underbrace{\\frac{C_2(\\mathbf{x}_0, \\mathbf{x}_t) }{ \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} }}_{C_3} \\right] \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2} \\left( \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} \\right) \\left[ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\left(\\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}}\\right)(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_{t}} \\right) \\mathbf{x}_{t-1} + C_3 \\right] \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2} \\left( \\frac{1}{ \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_{t}}   } \\right) \\left[ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\underbrace{\\left( \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1}) \\mathbf{x}_t^T + \\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\mathbf{x}_0^T}{1-\\bar\\alpha_{t}} \\right)}_{\\mathbf{v}^T}\\mathbf{x}_{t-1} + C_3 \\right] \\right\\}\n\\end{aligned}\n\\]\n두 번째 등호는 \\(\\mathbf{x}_{t-1}^T\\mathbf{x}_0\\)를 \\(\\mathbf{x}_0^T \\mathbf{x}_{t-1}\\)로 바꿔 적은 것입니다. 전개한 결과를 표준적인 정규분포 모양과 비교해보면\n\\[\n\\mathcal{N}(\\mathbf{x}, \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(\\sqrt{2\\pi})^D \\sqrt{\\beta^D}} \\exp\\left\\{-\\frac{1}{2\\beta} (\\mathbf{x}-\\boldsymbol{\\mu})^T (\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\n\\]\n다음 두 등식이 성립하면 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 정규 분포라는 것을 알 수 있습니다.\n\\[\nC_1 = \\frac{1}{ (\\sqrt{2\\pi})^D \\sqrt{\\left( \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\right)^D}}, \\quad C_3 = \\mathbf{v}^T \\mathbf{v}\n\\]\n먼저 \\(C_1\\)부터 정리해보겠습니다.\n\\[\n\\begin{aligned}\nC_1 &= \\frac{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}}{(2\\pi)^D\\sqrt{(1-\\alpha_t)^D (1-\\bar{\\alpha}_{t-1})^D}} \\\\[5pt]\n&= \\frac{\\sqrt{(2\\pi)^D}}{(2\\pi)^D} \\frac{\\left((1-\\bar\\alpha_t)^{\\frac{1}{2}}\\right)^D}{\\left(((1-\\alpha_t)(1-\\bar\\alpha_{t-1}))^{\\frac{1}{2}}\\right)^D} \\\\[5pt]\n&= \\frac{\\left((2\\pi)^D\\right)^{\\frac{1}{2}}}{(2\\pi)^D} \\frac{1}{\\dfrac{\\left((1-\\alpha_t)(1-\\bar\\alpha_{t-1})\\right)^{\\frac{D}{2}}}{(1-\\bar\\alpha_t)^{\\frac{D}{2}}}} \\\\[5pt]\n&= \\left(\\frac{(2\\pi)^{\\frac{1}{2}}}{2\\pi}\\right)^D \\frac{1}{\\sqrt{\\dfrac{(1-\\alpha_t)^D(1-\\bar\\alpha_{t-1})^D}{(1-\\bar\\alpha_t)^D}}} \\\\[5pt]\n&= \\left((2\\pi)^{-\\frac{1}{2}}\\right)^D \\frac{1}{\\sqrt{\\left(\\dfrac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)}\\right)^D}} \\\\[5pt]\n&= \\frac{1}{ (\\sqrt{2\\pi})^D \\sqrt{\\left( \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\right)^D}}\n\\end{aligned}\n\\]\n마지막으로 \\(C_3 = \\mathbf{v}^T \\mathbf{v}\\)를 보이기 위해 먼저 \\(\\mathbf{v}^T \\mathbf{v}\\)를 계산합니다.\n\\[\n\\begin{aligned}\n\\mathbf{v}^T \\mathbf{v} &=  \\left(  \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1}) \\mathbf{x}_t^T }{1-\\bar\\alpha_{t}} + \\frac{ \\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\mathbf{x}_0^T}{1-\\bar\\alpha_{t}} \\right) \\left(  \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1}) \\mathbf{x}_t }{1-\\bar\\alpha_{t}} + \\frac{ \\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\mathbf{x}_0}{1-\\bar\\alpha_{t}}  \\right) \\\\[10pt]\n&= \\frac{\\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_t \\mathbf{x}_t + \\frac{\\sqrt{\\alpha_t}\\sqrt{\\bar\\alpha_{t-1}}(1-\\bar\\alpha_{t-1})(1-\\alpha_t)}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_t \\mathbf{x}_0 + \\frac{\\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_0 \\mathbf{x}_t + \\frac{\\bar\\alpha_{t-1}(1-\\alpha_t)^2}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_0 \\mathbf{x}_0 \\\\[10pt]\n&= \\color{#20639B}{\\frac{\\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t}\n+ \\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1}(1-\\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n다음으로 \\(C_3\\)이 방금 전개한 식과 같음을 보이면 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 정규 분포가 됩니다. 이를 위해 \\(C_3\\) 정의로 부터 식을 전개하도록 하겠습니다.\n\\[\n\\begin{aligned}\nC_3 &= \\frac{C_2(\\mathbf{x}_0, \\mathbf{x}_1)}{\\dfrac{1-\\bar\\alpha_t}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}} \\\\[10pt]\n&= \\frac{ \\dfrac{\\mathbf{x}^T_t \\mathbf{x}_t}{1-\\alpha_t} + \\dfrac{\\bar\\alpha_{t-1}\\mathbf{x}^T_0 \\mathbf{x}_0}{1-\\bar\\alpha_{t-1}} - \\dfrac{\\mathbf{x}^T_t \\mathbf{x}_t - 2 \\sqrt{\\bar\\alpha_t} \\mathbf{x}^T_t \\mathbf{x}_0 + \\bar\\alpha_t \\mathbf{x}^T_0 \\mathbf{x}_0 }{1-\\bar\\alpha_t} }{\\dfrac{1-\\bar\\alpha_t}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}} \\\\[10pt]\n&= \\frac{(1-\\bar\\alpha_{t-1})\\mathbf{x}^T_t \\mathbf{x}_t + (1-\\alpha_t)\\bar\\alpha_{t-1} \\mathbf{x}^T_0 \\mathbf{x}_0 - \\dfrac{(\\mathbf{x}^T_t \\mathbf{x}_t - 2 \\sqrt{\\bar\\alpha_t} \\mathbf{x}^T_t \\mathbf{x}_0 + \\bar\\alpha_t \\mathbf{x}^T_0 \\mathbf{x}_0)(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}}{1-\\bar\\alpha_t} \\\\[10pt]\n&= \\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\mathbf{x}^T_t \\mathbf{x}_t + \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\mathbf{x}^T_0 \\mathbf{x}_0 - \\frac{(\\mathbf{x}^T_t \\mathbf{x}_t - 2 \\sqrt{\\bar\\alpha_t} \\mathbf{x}^T_t \\mathbf{x}_0 + \\bar\\alpha_t \\mathbf{x}^T_0 \\mathbf{x}_0)(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\\\[10pt]\n&=  \\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\mathbf{x}^T_t \\mathbf{x}_t + \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\mathbf{x}^T_0 \\mathbf{x}_0  \\\\[5pt]\n& \\quad - \\left(\n    \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\mathbf{x}^T_t \\mathbf{x}_t  \n    - \\frac{2 \\sqrt{\\bar\\alpha_t}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_t \\mathbf{x}_0 + \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\mathbf{x}^T_0 \\mathbf{x}_0\n\\right) \\\\[10pt]\n&= \\color{#20639B}{\\left[ \\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} - \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\right]}\n\\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t} \\\\[5pt]\n& \\quad + \\color{#3CAEA3}{\\left[ \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} - \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\right]}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n이제 \\(\\left[ \\cdot \\right]\\) 부분을 정리합니다.\n먼저 네이비색 부분입니다.\n\\[\n\\color{#20639B}{\\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} - \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}} \\color{black}{= \\frac{(1-\\bar\\alpha_{t-1})(1-\\bar\\alpha_t)-(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}}\n\\]\n분자만 따로 정리를 하면\n\\[\n\\begin{aligned}\n(1-\\bar\\alpha_{t-1})(1-\\bar\\alpha_t)-(1-\\alpha_t)(1-\\bar\\alpha_{t-1}) &= (1-\\bar\\alpha_{t-1})\\left( (1-\\bar\\alpha_t)-(1-\\alpha_t) \\right) \\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) ( 1- \\bar\\alpha_t - 1 + \\alpha_t) \\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) (\\alpha_t - \\bar\\alpha_t) \\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) (\\alpha_t - (\\alpha_t \\cdot \\bar\\alpha_{t-1}))\\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) \\alpha_t (1 - \\bar\\alpha_{t-1}) \\\\[5pt]\n&= \\alpha_t (1-\\bar\\alpha_{t-1})^2\n\\end{aligned}\n\\]\n분자를 정리된 결과로 바꾸면\n\\[\n\\color{#20639B}{\\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} - \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{=}\n\\color{#20639B}{\\frac{ \\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}}\n\\]\n이어서 민트색 부분도 정리합니다.\n\\[\n\\color{#3CAEA3}{ \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} - \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} } \\color{black}{\n    = \\frac{(1-\\alpha_t)(1-\\bar\\alpha_t)\\bar\\alpha_{t-1} - \\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\n}\n\\]\n분자만 따로 정리를 하면\n\\[\n\\begin{aligned}\n(1-\\alpha_t)(1-\\bar\\alpha_t) \\bar\\alpha_{t-1} - \\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})\n&= (1 - \\alpha_t) \\left( \\bar\\alpha_{t-1} (1-\\bar\\alpha_t) - \\alpha_t(1 - \\bar\\alpha_{t-1}) \\right) \\\\[5pt]\n&=(1 - \\alpha_t) ( \\bar\\alpha_{t-1} - \\bar\\alpha_t\\bar\\alpha_{t-1} - \\bar\\alpha_t + \\bar\\alpha_t\\bar\\alpha_{t-1} ) \\\\[5pt]\n&= (1 - \\alpha_t) (\\bar\\alpha_{t-1} - \\bar\\alpha_t) \\\\[5pt]\n&= (1 - \\alpha_t) (\\bar\\alpha_{t-1} - (\\alpha_t \\cdot \\bar\\alpha_{t-1}))\\\\[5pt]\n&= (1 - \\alpha_t) \\bar\\alpha_{t-1} (1 - \\alpha_t) \\\\[5pt]\n&= \\bar\\alpha_{t-1} (1 - \\alpha_t)^2\n\\end{aligned}\n\\]\n분자를 정리된 결과로 바꾸면\n\\[\n\\color{#3CAEA3}{ \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} - \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} } \\color{black}{=}\n\\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1} (1 - \\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\]\n계산 결과를 \\(C_3\\)이 전개된 식에 다시 대입하고 최종 결과를 \\(\\mathbf{v}^T \\mathbf{v}\\)와 비교하면\n\\[\n\\begin{aligned}\nC_3 &= \\color{#20639B}{\\frac{ \\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}} \\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t} + \\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1} (1 - \\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\mathbf{v}^T \\mathbf{v} &= \\color{#20639B}{\\frac{\\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t}\n+ \\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1}(1-\\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n두 식은 일치하고 이로부터 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 평균과 분산이 다음과 같은 정규 분포라는 것을 알 수 있습니다.\n\\[\n\\begin{aligned}\n\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) &:= \\frac{\\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)}{1-\\bar\\alpha_t} \\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t = \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t} \\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t \\\\[5pt]\n\\tilde{\\beta}_t &:= \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} \\beta_t\n\\end{aligned} \\tag{7}\n\\]\n따라서\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}) \\tag{6}\n\\]\n가 됩니다.\n타겟이 계산되었으니 다음 단계로 만들어야 하는 한 시간 단계에 대한 리버스 프로세스를 다음처럼 정의 합니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\n\\]\n위 식에서 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\), \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)가 뉴럴넷의 출력이 되는 것입니다. 이때 분산은 문제를 간단히 하기 위해 \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)=\\sigma^2_t \\mathbf{I}\\)처럼 대각 행렬로 정의합니다. 그리고 \\(\\sigma^2_t\\)는 식(7)에서 유도된 타겟에 의해 \\(\\sigma^2_t = \\tilde\\beta_t = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} \\beta_t\\)로 두거나 더 간단히 그냥 \\(\\sigma^2_t = \\beta_t\\)로 두게 됩니다. 이 두 결과 모두 실험에서 결과에 큰 영향을 주지 않는다고 합니다. 분산을 대각행렬로 간단히해서 다시 쓰면 아래과 같습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma^2_t \\mathbf{I})\n\\]\n손실 함수의 값은 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)와 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)의 \\(D_{KL}\\)이고 이 두 분포는 모두 정규 분포임을 알고 있으므로 다음처럼 주어지는 정규분포에 대한 \\(D_{KL}\\)을 이용합니다.\n\\[\nD_{KL}(\\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}_x, \\boldsymbol{\\Sigma}_x) \\,\\lVert\\, \\mathcal{N}(\\mathbf{y}; \\boldsymbol{\\mu}_y, \\boldsymbol{\\Sigma}_y)) = \\frac{1}{2}\\left( \\text{tr}\\left(\\boldsymbol{\\Sigma}_y^{-1} \\boldsymbol{\\Sigma}_x \\right) - d + (\\boldsymbol{\\mu}_y - \\boldsymbol{\\mu}_x)^T \\boldsymbol{\\Sigma}^{-1}_y (\\boldsymbol{\\mu}_y - \\boldsymbol{\\mu}_x) + \\log \\left( \\frac{\\det \\boldsymbol{\\Sigma}_y}{\\det \\boldsymbol{\\Sigma}_x} \\right) \\right)\n\\]\n여기서 \\(d\\)는 \\(\\mathbf{x}\\), \\(\\mathbf{y}\\)의 차원입니다.\n이렇게 주어진 정규 분포의 \\(D_{KL}\\)를 식(6)과 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma^2_t \\mathbf{I})\\)를 이용하여 쓰면\n\\[\n\\begin{aligned}\nD_{KL}(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\,\\lVert\\, p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))\n&= D_{KL}(\\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde\\beta_t\\mathbf{I}) \\,\\lVert\\, \\mathcal{N}(\\mathbf{x}_{t-1};\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma^2_t \\mathbf{I})) \\\\[5pt]\n&= \\frac{1}{2} \\left( (\\boldsymbol{\\mu}_\\theta-\\tilde{\\boldsymbol{\\mu}}_t)^T (\\sigma^2_t \\mathbf{I})^{-1} (\\boldsymbol{\\mu}_\\theta-\\tilde{\\boldsymbol{\\mu}}_t)+ \\underbrace{\\text{tr}\\left((\\sigma^2_t \\mathbf{I})^{-1} \\tilde\\beta_t \\mathbf{I} \\right) -d +\\log \\frac{\\det \\sigma^2_t \\mathbf{I}}{\\det \\tilde\\beta\\mathbf{I}}}_{C} \\right) \\\\[5pt]\n&= \\frac{1}{2 \\sigma^2_t} \\lVert  \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) - \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) \\rVert^2_2 + C\n\\end{aligned}\n\\]\n위 식에서 \\(C\\)는 파라미터 \\(\\theta\\)와 무관한 항들입니다. 만약 전술한대로 \\(\\sigma^2_t = \\tilde\\beta_t\\) 또는 \\(\\sigma^2_t = \\tilde\\beta_t = \\beta_t\\)로 두면 \\(C\\)는 사라집니다. 이를 이용하여 식(5)에 \\(L_{t-1}\\)을 써보면 다음과 같게 됩니다.\n\\[\nL_{t-1} = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ \\frac{1}{2 \\sigma^2_t} \\lVert \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)   \\rVert^2_2\\right] + C \\tag{8}\n\\]\n\\(\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)\\)는 \\(t\\) 시간 단계의 노이즈가 많은 \\(\\mathbf{x}_t\\)로 부터 만들어진 노이즈가 약간 작은 \\(\\mathbf{x}_{t-1}\\)의 분포의 평균이 되고 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\)는 \\(\\mathbf{x}_t\\)와 \\(t\\)를 입력으로 받은 뉴럴넷이 출력한 어떤 평균입니다. 이 두 평균의 차이가 작게 되도록 뉴럴넷을 학습시키면 뉴럴넷이 \\(\\mathbf{x}_t\\)를 입력으로 받고 출력하는 출력값은 \\(\\mathbf{x}_{t-1}\\)의 평균이 되고 이는 노이즈가 약간 줄어들어 있어야 합니다.\n식(8)의 \\(\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)\\)부분을 식(4)를 이용해 좀 더 정리를 하도록 하겠습니다. 식(4)는 \\(\\mathbf{x}_0\\)로 부터 임의의 시간 단계 \\(t\\)에 대한 \\(\\mathbf{x}_t\\)에 대한 분포를 다음처럼 바로 정의 한 식입니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, (1- \\bar{\\alpha}_t)\\mathbf{I})\n\\]\n이 식을 이용하면 \\(\\mathbf{x}_t\\)를 다음처럼 표준 정규분포를 이용해 reparameterization 할 수 있습니다.\n\\[\n\\mathbf{x}_t (\\mathbf{x}_0, \\boldsymbol{\\epsilon}) = \\sqrt{\\bar\\alpha_t} \\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon}, \\qquad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\n이로 부터 역으로\n\\[\n\\mathbf{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right)\n\\]\n임을 알 수 있습니다. 이는 추가된 노이즈만 알면 \\(\\mathbf{x}_t\\)에서 \\(\\mathbf{x}_0\\)를 계산할 수 있다는 것을 나타냅니다. 이를 식(8) 대입하면\n\\[\nL_{t-1} - C = \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\tilde{\\boldsymbol{\\mu}}_t \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) ,  \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right)   \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\tag{9}\n\\]\n가 됩니다. 잠재 변수 \\(\\mathbf{x}_t\\)는 \\(\\mathbf{x}_0\\), \\(\\boldsymbol{\\epsilon}\\)의 함수이므로 기댓값을 구하기 위한 적분 변수는 모든 \\(\\mathbf{x}_0\\), \\(\\boldsymbol{\\epsilon}\\)로 바뀌었습니다.\n이제 식(7)로 부터 \\(\\tilde{\\boldsymbol{\\mu}}_t\\)를 정리합니다.\n\\[\n\\begin{aligned}\n\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), \\mathbf{x}_0) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t} \\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) \\\\[10pt]\n&= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t} \\left( \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right) \\right) + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) \\\\[10pt]\n&= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{\\sqrt{\\bar\\alpha_t}(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t\\sqrt{1-\\bar\\alpha_{t}}}{\\sqrt{\\bar\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) \\\\[10pt]\n&=\\left( \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{\\sqrt{\\bar\\alpha_{t-1}}\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)} + \\frac{\\sqrt{\\alpha_t}(1-\\sqrt{\\bar\\alpha_{t-1}})}{1-\\bar\\alpha_t} \\right) \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{\\bar\\alpha_{t-1}}\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\\\[10pt]\n&= \\left( \\frac{\\beta_t}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)} + \\frac{\\alpha_t(1-\\bar\\alpha_{t-1})}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)} \\right) \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})-\\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\\\[10pt]\n&= \\frac{\\beta_t + (\\alpha_t - \\bar\\alpha_t)}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\\\[10pt]\n&= \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\frac{\\beta_t + (\\alpha_t - \\bar\\alpha_t)}{(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\right) \\\\[10pt]\n&= \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\frac{1-\\alpha_t + (\\alpha_t - \\bar\\alpha_t)}{(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) \\\\[10pt]\n&= \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right)\n\\end{aligned}\n\\]\n이 결과를 식(9)에 대입하면 식(10)이 얻어집니다.\n\\[\n\\begin{aligned}\nL_{t-1} - C &= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\tilde{\\boldsymbol{\\mu}}_t \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) ,  \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right)   \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\qquad {(9)} \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\qquad\\qquad \\qquad \\qquad  {(10)}\n\\end{aligned}\n\\]\n종합하면 \\(\\boldsymbol{\\mu}_\\theta\\)는 \\(\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right)\\)를 예측하면 되는데 \\(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})\\)는 가우시안 노이즈 \\(\\boldsymbol{\\epsilon}\\)을 사용하여 \\(t\\) 시간 단계에 해당하는 노이즈가 낀 이미지 이고 이는 \\(\\boldsymbol{\\mu}_\\theta\\)를 추정하는 뉴럴넷의 입력으로 주어질 것입니다. 나머지 \\(\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\) 부분도 \\(t\\)가 주어지면 계산되는 항이므로 결국 예측 해야 하는 것은 \\(\\mathbf{x}_t\\)를 만들때 사용한 노이즈 \\(\\boldsymbol{\\epsilon}\\)이라는 것을 알 수 있습니다. 물론 논문에서도 밝히고 있듯이 \\(\\boldsymbol{\\mu}_\\theta\\)는 \\(\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right)\\)를 바로 예측해도 되는데 실험 결과 생성되는 이미지 퀄리티가 떨어진다고 합니다.\n\\[\n\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\tilde{\\boldsymbol{\\mu}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), \\frac{1}{\\sqrt{\\bar\\alpha_t}}(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t) )\\right)\n\\]\n위 식은 \\(\\boldsymbol{\\mu}_\\theta\\)가 타겟 \\(\\tilde{\\boldsymbol{\\mu}}\\)와 같아지기 위해서 \\(\\mathbf{x}_t\\)를 만들기 위해 추가된 노이즈를 \\(\\boldsymbol{\\epsilon}_\\theta\\)으로 잘 예측해야 한다는 것을 이야기 합니다. 예측하는 뉴럴넷 입장에서는 \\(\\mathbf{x}_t\\)만 알 수 있을 뿐 \\(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})\\)에서 \\(\\boldsymbol{\\epsilon}\\)는 알 수 없으므로 추청해야하는 것입니다. 우변항을 이전과 같은 과정으로 계산하여 정리하면\n\\[\n\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\tilde{\\boldsymbol{\\mu}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), \\frac{1}{\\sqrt{\\bar\\alpha_t}}(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t) )\\right) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right) \\tag{11}\n\\]\n를 얻게 됩니다. 식(11) 마지막 표현에서 \\(\\mathbf{x}_t\\)를 만들때 사용된 노이즈 \\(\\boldsymbol{\\epsilon}\\)를 모른다는 것을 명확히 하기 위해 노이즈 낀 이미지를 \\(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})\\)로 쓰지 않고 \\(\\mathbf{x}_t\\)로 표시 했습니다.\n식(10)에 식(11)을 대입하고 정리하면 식(12)를 얻게 됩니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} &\\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) - \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right) \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert  - \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}  + \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)  \\right\\rVert^2_2 \\right] \\\\[10pt]\n\\end{aligned}\n\\]\n벡터 노름에 대한 다음 성질을 이용하여\n\\[\n\\begin{aligned}\n\\lVert c \\mathbf{x} - c \\mathbf{y} \\rVert^2_2 &= \\sum_i (cx_i - cy_i)^2 \\\\\n&= \\sum_i c^2 x_i^2 - 2 c^2 x_i y_i + c^2 y_i^2 \\\\\n&= \\sum_i c^2 y_i^2 - 2 c^2 x_i y_i + c^2 x_i^2 \\\\\n&= c^2 \\sum_i y_i^2 - 2 x_i y_i + x_i^2 \\\\\n&= c^2 \\lVert  \\mathbf{y} - \\mathbf{x} \\rVert^2_2\n\\end{aligned}\n\\]\n식을 계속 정리합니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} & \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert  - \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}  + \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)  \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\left\\lVert  -\\boldsymbol{\\epsilon}  + \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\left\\lVert   \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t)-\\boldsymbol{\\epsilon}  \\right\\rVert^2_2 \\right] \\qquad \\because \\text{eq.} (4)\\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\left\\lVert   \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t)  \\right\\rVert^2_2 \\right]\n\\end{aligned}\n\\]\n이렇게 \\(t=1\\)에서 \\(t={T-1}\\) 단계에 해당하는 손실을 유도 했습니다. 식(12)를 볼때 한가지 주의해야할 점은 $_(_0 + , t) $에 보이는 \\(\\boldsymbol{\\epsilon}\\)은 타겟인데 이를 예측하는 뉴럴넷 입장에서는 \\(\\boldsymbol{\\epsilon}\\)를 알 수 없고 \\(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}\\)에 의해 노이즈가 낀 \\(\\mathbf{x}_t\\)만 입력으로 받는다는 점입니다. 정리하면 \\(\\mathbf{x}_t\\)와 \\(t\\)를 입력으로 받고 \\(\\boldsymbol{\\epsilon}\\)을 예측하는 것입니다. \\(\\boldsymbol{\\epsilon}\\)에 색을 넣어보면 다음과 같고 빨간색은 타겟, 파란색은 예측을 나타냅니다.\n\\[\nL_{t-1} - C = \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}{(}\n    \\underbrace{\n        \\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\color{red}{\\boldsymbol{\\epsilon}}\n    }_{\\mathbf{x}_t}\n    \\color{black}{ , t)}  \\rVert^2_2 \\right] \\tag{12}\n\\]\n\n\n\n꽤 긴 내용으로 \\(t=1 \\sim T-1\\)까지 적용되는 손실 함수에 대해서 알아봤습니다. 마지막으로 \\(L_0\\)에 대해서 알아볼 차례인데 그전에 식(5)의 마지막 항인 \\(- \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)\\)의 의미를 다시 한번 생각해봅시다.\n이미지를 생성하는 마지막 과정에서 \\(\\mathbf{x}_1\\)을 입력으로 받고 \\(\\mathbf{x}_0\\)를 출력으로 내는 모델은 노이즈가 있는 이미지에서 노이즈가 없는 이미지를 만들어내는 모델이므로 디코더라고 할 수 있습니다. 이 디코더에 입력으로 \\(\\mathbf{x}_1\\), 출력으로 \\(\\mathbf{x}_0\\)를 세팅하고 그때 가능도가 최대가 되도록 \\(\\theta\\)를 설정하는 과정이 바로 디코더를 학습시키는 과정이 됩니다. \\(p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)\\)이 바로 \\(\\mathbf{x}_1\\)을 조건으로 하는 \\(\\mathbf{x}_0\\)의 가능도 함수입니다. 따라서 식(5)에서 마지막 항은 최종적으로 깨끗한 이미지를 생성하는 디코더를 위한 손실 항이 됩니다. 이 손실 항을 식(13)과 같이 정의 합니다.\n\\[\n\\begin{aligned}\np_\\theta(\\mathbf{x}_0 \\mid \\mathbf{x}_1) &= \\prod_{i=1}^D \\int^{\\delta_+(x^i_0)}_{\\delta_-(x^i_0)} \\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 ) dx \\\\[5pt]\n\\delta_+(x) &= \\begin{cases}\n\\infty & \\mbox{if }x=1 \\\\\nx+\\frac{1}{255} & \\mbox{if } x &lt; 1\n\\end{cases} \\quad\n\\delta_-(x) = \\begin{cases}\n-\\infty & \\mbox{if }x = -1 \\\\\nx-\\frac{1}{255} & \\mbox{if } x &gt; -1\n\\end{cases}\n\\end{aligned} \\tag{13}\n\\]\n식(13)은 보기에도 골치가 아파오게 생겼습니다. 그런데 의미를 잘 생각해보면 크게 복잡한 식이 아니란 것을 알 수 있습니다.\n먼저 \\(\\mathbf{x}_0\\)와 \\(\\mathbf{x}_1\\)은 픽셀수가 똑같은 이미지라는 점을 상기합시다. 식(13)에서 상첨자 \\(i\\)는 픽셀의 인덱스를 나타냅니다. 그리고 앞서 시간 단계에 대한 리버스 프로세스를 다음처럼 정규분포로 정의 했습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n노이즈가 제거된 이미지 \\(\\mathbf{x}_{t-1}\\)는 정규분포 하는데 그 분포의 평균과 분산은 노이즈가 조금 더 많은 \\(\\mathbf{x}_t\\)와 시간 단계 \\(t\\)를 입력으로 받은 모델이 예측한 \\(\\boldsymbol{\\mu}_\\theta\\)와 \\(\\boldsymbol{\\Sigma}_\\theta\\)가 된다는 이야기입니다.\n이 정의를 그대로 식(13)에서 다시 사용했는데 두가지 다른 점이 있습니다.\n첫 번째 다른 점은 모든 픽셀이 독립이라는 가정을 한 것입니다. 그래서 \\(\\mathbf{x}_0\\)가 따르는 분포의 평균에 해당하는 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_1, 1)\\)을 각 픽을 별로 \\(\\mu^i_{\\theta}(\\mathbf{x}_1, 1)\\)라고 따로 적고 있습니다. 분산은 예측하지 않을 것이므로 \\(\\sigma^2_1\\)처럼 상수로 나타냈습니다. 이미지에서 픽셀들은 서로 독립이 아닙니다. 특정 픽셀의 색은 인접한 픽셀의 색에 영향을 매우 강하게 받습니다. 그렇기에 픽셀간 종속성을 고려하면 더 뛰어난 디코더가 모델링 되지만 논문에선 향후 작업으로 남겨둔다고 밝히고 있습니다. 이런 독립 가정으로 인해 가능도는 각 픽셀에 대한 확률값의 곱이 됩니다. \\(\\prod\\) 기호가 식에 등장해 모든 확률값을 곱하고 있는 것입니다. 그럼 \\(D\\)는 픽셀 개수가 되고 뒤 적분항은 확률이 됩니다. 확률밀도 함수를 특정 구간에서 적분하므로 확률이 맞습니다.\n두번째 다른 점은 최종 이미지 \\(\\mathbf{x}_0\\)의 각 픽셀 \\(x^i_0\\)는 이산확률변수라는 점입니다. 이 변수들은 \\(0\\) ~ \\(255\\)까지 값을 가지는 이산 확률 변수이고 선형 변환을 통해 \\(-1\\) ~ \\(1\\)로 변환됩니다. 리버스 프로세스 정의에 의해 \\(\\mathbf{x}_0\\)의 \\(i\\)번째 픽셀 \\(x_0^i\\)은 \\(\\mathbf{x}_1\\)로 부터 예측된 평균 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_1, 1)\\)의 \\(i\\)번째 픽셀인 \\(\\mu^i_{\\theta}(\\mathbf{x}_1, 1)\\)를 평균으로 하는 정규분포를 따르게 됩니다. 그런데 \\(x_0^i\\)는 이산 확률변수기 때문에 이 확률변수의 특정 값에 대한 확률을 구하기 위해서는 \\(\\mathcal{N}(x_0^i; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )\\)를 \\(x_0^i\\) 주변에서 적분하여야 합니다. 이때 적분 구간을 설정해야 하는데 \\(x_0^i\\)를 중심으로 좌우 \\(\\frac{1}{255}\\)로 구간을 설정합니다. 이렇게 설정한 이유는 픽셀이 가질 수 있는 값이 총 256개며 각 값 사이의 구간이 255개 있기 때문입니다. 따라서 적분 구간의 크기는 \\(\\frac{2}{255}\\)가 됩니다. 만약 \\(x_0^i\\)가 경계값인 \\(-1\\)이면 때 적분 구간의 왼쪽값을 \\(-\\infty\\), 오른쪽 값을 \\(x_0^i + \\frac{1}{255}\\)로 설정합니다. 반대쪽 경계값인 \\(1\\)이면 왼쪽 값은 \\(x_0^i - \\frac{1}{255}\\), 오른쪽 값을 \\(\\infty\\)로 설정합니다. 이는 \\(\\mathcal{N}(x_0^i; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )\\)의 전 정의역을 적분구간에 포함시키기 위함입니다.\n이렇게 정의된 가능도 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)를 이용해 학습된 디코더는 어떤 \\(\\mathbf{x}_0\\)를 만들어내게 되는지 생각해봅시다. 이를 이해하기 아래코드로 식(13)을 시각화 하겠습니다.\n\nfrom scipy.stats import norm\n\nplotly.io.renderers.default = \"notebook\"\n\n\nmu_theta_i = 0.25\nsigma2_i = 0.1\nxmin = -1.3\nxmax = 1.6\nN = norm(loc=mu_theta_i, scale=np.sqrt(sigma2_i))\nx = np.linspace(-1.3, 1.6, 150)\ny = N.pdf(x)\n\n# 0~10을 -1~1로 매핑, 각 값은 25단계로 discrete\nx0s = np.linspace(0, 10, 25) / 10 * 2 - 1\n\n# 적분 구간의 절반\nbin_width_half = 1 / (len(x0s)-1)\n\n\nlayout = go.Layout(\n    # title=r'$\\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )$',\n    title=r'N(mu^i_theta(x; x_1, 1), sigma^2_1)',\n    width=800, height=400,\n    margin=dict(l=0, r=0, b=0, t=25),\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\n\nprobs = []\nfor x0 in x0s:\n    if x0 == -1:\n        int_left = xmin\n        int_right = x0 + bin_width_half\n    elif x0 == 1:\n        int_left = x0 - bin_width_half\n        int_right = xmax\n    else:\n        int_left = x0 - bin_width_half\n        int_right = x0 + bin_width_half\n\n    x_interval = np.linspace(int_left, int_right, 50)\n    y_interval = N.pdf(x_interval)\n    probs.append([x_interval, y_interval, np.trapz(y=y_interval, x=x_interval)])\n\n# 적분 합이 거의 1이 되는지 테스트\n# P = 0.0\n# for i in probs:\n#     P += i[2]\n# print(P)\n\n# Add traces, one for each slider step\nfig.add_trace(go.Scatter(x=x, y=y,\n                    mode='lines',\n                    name='lines',\n                    line_color='indigo'\n                    ))\n\nfor i, prob in enumerate(probs):\n    x_part = prob[0]\n    y_part = prob[1]\n    # https://plotly.com/python/filled-area-plots/\n    fig.add_trace(\n        go.Scatter(\n            x=x_part,\n            y=y_part,\n            fill='tozeroy',\n            mode='lines',\n            line_color='orange',\n            visible=False,\n            name=f\"area:{prob[2]:.3f}\"\n        )\n    )\n\nfig.data[0].visible = True\nfig.data[1].visible = True\n\nsteps = []\nfor i in range(len(probs)):\n    step = dict(\n        method=\"update\",\n        args=[\n            # 일단 모든 적분 영역 안보이게, 가우시안만 보이게\n            {\"visible\": [False] * (len(fig.data))},\n            # {\"visible\": [False] * (len(fig.data)-1) + [True]},\n        ],\n        label=f\"{x0s[i]:.2f}\"\n    )\n    # 해당 i번째 적분 영역만 보이게\n    step[\"args\"][0][\"visible\"][i+1] = True\n    step[\"args\"][0][\"visible\"][0] = True\n    steps.append(step)\n\nsliders = [\n    dict(\n        active=0,\n        currentvalue={\"prefix\": \"x0: \"},\n        pad={\"l\":10, \"t\": 50, \"r\":10, \"b\":10},\n        steps=steps\n    )\n]\n\nfig.update_layout(sliders=sliders)\n\nfig.show()\n\n\n                                                \n\n\n위 그래프는 특정 \\(i\\)번째 픽셀의 평균을 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)=0.25\\)로 가정했을 때 \\(x^i_0\\)값에 따른 적분 구간과 적분값을 보여줍니다. 단 적절한 시각화를 위해 \\(x_0\\)는 0에서 10사이를 24등분해서 25개 값을 가지는 것으로 가정했습니다. 그럼 적분구간의 너비는 \\(\\frac{2}{24}\\)가 되는 상황입니다.\n슬라이드바를 이동하면 \\(x^i_0\\)의 값을 변경할 수 있습니다. 편의상 그래프에서 상첨자 \\(i\\)는 생략했습니다.\n먼저 x0값을 \\(-1\\)로 설정해보세요. 그래프는 아래 그림처럼 보입니다.\n\n적분 구간은 \\(-\\infty\\)에서 \\(-1+\\frac{1}{24}\\)으로 설정됩니다. 하지만 이 구간에서 확률밀도값이 거의 모두 0이기 때문에 적분 구간이 잘 확인되지 않습니다. 그래프 오른쪽에 area값을 보면 적분값 역시 거의 0임을 알 수 있습니다.\n이제 슬라이드를 오른쪽으로 이동시키면 적분 구간이 그려지게 됩니다. x0값을 \\(1\\)로 설정했을 때 그래프는 다음처럼 보이게 됩니다.\n\n설정된 값 \\(1\\)약간 왼쪽(정확히는 \\(1-\\frac{1}{24}\\))에서 부터 \\(\\infty\\)까지 적분되는것을 확인할 수 있고 그 값은 0.013정도가 됩니다.\n마지막으로 x0값을 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)\\)와 같은 값인 0.25로 설정하면 적분 구간은 정규분포의 가운데 위치하게 됩니다.\n\n이때 적분값은 0.105이며 슬라이드를 좌우로 이동해보면 알 수 있겠지만 이 값은 최대 적분값이 됩니다.\n이 실험은 식(13)처럼 가능도를 정의하면 디코더에 의해 생성될 \\(\\mathbf{x}_0\\)의 \\(i\\)번째 픽셀 값이 \\(\\mathbf{x}_1\\)으로 부터 예측된 평균의 \\(i\\)번째 픽셀값 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)\\)과 동일하면 가능도가 최대가 된다는 것을 이야기합니다.\n이 실험을 통해 디코더가 평균 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)\\)을 그대로 출력하면 가능도는 최대가 된다는 것을 알 수 있습니다!\n여기서 평균은 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1)\\), 다시말해 \\(\\mathbf{x}_1\\)을 입력받고 모델이 예측한 출력입니다. 이런 이유로 논문에서는 다음처럼 이야기하고 있으며\n\nwe display \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1)\\) noiselessly.\n\n이 문장의 의미는 샘플링 알고리즘에서 마지막 단계인 \\(\\mathbf{x}_0\\)를 샘플링할 때 표준 정규분포로 부터 샘플링된 노이즈를 추가하지 않는 다는 뜻입니다. (샘플링 알고리즘을 이야기할 때 알아보겠지만 확률적 다양성을 위해서 예측된 이미지 평균에 정규분포로 부터 샘플링된 노이즈를 더하는 과정을 거치게 됩니다.)\n이렇게 \\(L_0\\)에 대해서 자세히 알아봤습니다. 하지만 구현에서 마지막 단계에만 적용되는 디코더를 따로 모델링하지 않기 때문에 실제로 학습 과정에서 \\(L_0\\)를 사용하지는 않습니다. 그 이유에 대해서는 다음 절에서 자세히 알아보도록 하겠습니다.\n\n\n\n지금까지 정의한 손실함수는 모두 미분 가능하지만 그대로 사용하지 않고 식(14)처럼 간략화시켜 사용하게 됩니다. 식(14)는 식(12)에서 노름 앞부분에 곱해지는 상수를 제거한것과 동일합니다. 최소화를 위해서 상수는 제거해도 상관없기 때문에 식(14)를 제안한것은 타당합니다. 기댓값 기호에 \\(t\\)가 추가된 이유는 식(12)가 \\(t-1\\)에 대한 손실이므로 모든 시간 단계에 대해서 기댓값을 구하기 위해 추가되었습니다. 그런데 이렇게 제시된 식(14)가 앞서 따로 정의한 \\(L_0\\)도 대신할 수 있는지는 따져볼 필요가 있습니다.\n\\[\nL_{\\text{simple}}(\\theta) := \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\left\\lVert \\boldsymbol{\\epsilon}- \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t) \\right\\rVert^2_2 \\right] \\tag{14}\n\\]\n\\(x=1\\), \\(x=-1\\)일 때 무한대까지 적분하는 경우(edge effects)를 무시하면 식(13)의 적분은 다음처럼 근사될 수 있습니다.\n\\[\n\\int^{\\delta_+(x^i_0)}_{\\delta_-(x^i_0)} \\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 ) dx \\approx \\frac{1}{\\sqrt{2 \\pi \\sigma^2_1}} \\exp \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_i, 1))^2}{2 \\sigma_1^2} \\right) \\times \\frac{2}{255}\n\\]\n근사된 적분식은 아래 그림을 통해 직관적으로 이해할 수 있습니다.\n\nmu_theta_i = 0.0\nsigma2_i = 0.1\nN = norm(loc=mu_theta_i, scale=np.sqrt(sigma2_i))\nx = np.linspace(-1.5, 1.5, 150)\ny = N.pdf(x)\n\nlayout = go.Layout(\n    title=r'Approximated integrals',\n    width=800, height=400,\n    margin=dict(l=0, r=0, b=0, t=25),\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\n\nhalf = 0.1\nx_part = np.linspace(0.25-half, 0.25+half, 100)\ny_part = N.pdf(x_part)\nfig.add_trace(\n    go.Scatter(\n        x=x_part,\n        y=y_part,\n        fill='tozeroy',\n        line_color='orange',\n        name=\"Exact Prob.\",\n        showlegend=False,\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=x, y=y,\n        mode='lines',\n        # name=r'$\\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )$',\n        name = r\"N(mu_theta(x; x_1, 1), sigma^2_1)\",\n        line_color='indigo'\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=[0.25, 0.25], y=[0, N.pdf(0.25)],\n        mode='lines',\n        line_color='indigo',\n        line_width=1,\n        line_dash='dash',\n        showlegend=False,\n    ),\n)\n\nfig.add_trace(go.Scatter(\n    x=[0.25],\n    y=[-0.1],\n    mode=\"text\",\n    #name=\"Lines, Markers and Text\",\n    text=[r\"x=0.25\"],\n    textposition=\"top center\",\n    textfont=dict(\n        size=15,\n        color=\"black\"\n    ),\n    showlegend=False,\n))\n\nfig.add_shape(\n    type=\"rect\",\n    xref=\"x\", yref=\"y\",\n    x0=0.25-half, y0=0,\n    x1=0.25+half, y1=N.pdf(0.25),\n    line = dict(\n        color=\"RoyalBlue\",\n        width=2,\n        # dash=\"dash\"\n    )\n)\n\nfig.add_annotation(\n    x=0.35, y=N.pdf(0.25), ax=50,\n    text=\"Approx. Prob.\", align=\"left\",\n    showarrow=True,\n    arrowhead=2, arrowsize=2,\n    arrowcolor=\"RoyalBlue\",\n    font=dict(\n        size=18,\n        color=\"RoyalBlue\"\n    )\n)\n\nfig.add_annotation(\n    x=0.21, y=0.5, ax=-66,\n    text=\"Exact Prob.\", align=\"left\",\n    showarrow=True,\n    arrowhead=2, arrowsize=2,\n    arrowcolor=\"orange\",\n    font=dict(\n        size=18,\n        color=\"orange\"\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\n위 그림은 \\(x^i_0=0.25\\)일 때 적당한 구간에서 정확한 적분값과 위 식으로 근사된 적분값을 나타냅니다. 이 근사치를 이용하여 \\(\\mathbf{x}_1\\)의 로그 가능도를 전개하면 다음과 같습니다.\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x}_1 \\mid \\mathbf{x}_0) &\\approx \\log \\left\\{ \\prod_{i=1}^D \\frac{1}{\\sqrt{2 \\pi \\sigma^2_1}} \\exp \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_i, 1))^2}{2 \\sigma_1^2} \\right) \\times \\frac{2}{255} \\right\\} \\\\[10pt]\n&= \\sum_{i=1}^D \\log \\left\\{ \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2}{2 \\sigma_1^2} \\right) \\times \\frac{2}{255} \\right\\} \\\\[10pt]\n&= \\sum_{i=1}^D \\left\\{  \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2_1}} + \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2}{2\\sigma_1^2} \\right) + \\log \\frac{2}{255}  \\right\\} \\\\[10pt]\n&= \\sum_{i=1}^D \\log \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} + \\left( - \\frac{1}{2\\sigma_1^2} \\sum_{i=1}^D (x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2 \\right) + \\sum_{i=1}^D \\log \\frac{2}{255} \\\\[10pt]\n&= -\\frac{1}{2\\sigma_1^2} \\sum_{i=1}^D (x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2 + C \\\\\n&= -\\frac{1}{2\\sigma_1^2} \\lVert \\mathbf{x}_0 - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2 + C\n\\end{aligned}\n\\]\n최소화 문제로 바꾸기 위해 마이너스를 곱하고 상수를 무시하면\n\\[\nL_0 = -\\log p_\\theta(\\mathbf{x}_1 \\mid \\mathbf{x}_0) \\approx \\frac{1}{2\\sigma_1^2}\\lVert \\mathbf{x}_0 - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2\n\\]\n가 됩니다. 이를 \\(L_0\\)에 대한 근사값으로 사용할 수 있습니다.\n위 식에서 \\(\\mathbf{x}_0\\)는 식(4)에 reparameterization을 적용하는 과정에 의해\n\\[\n\\mathbf{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha_1}} \\left( \\mathbf{x}_1 -  \\sqrt{1-\\bar\\alpha_1} \\boldsymbol{\\epsilon} \\right)\n\\]\n로 표현됨을 앞서 알아봤고 또한 식(11)에 의해\n\\[\n\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) = \\frac{1}{\\sqrt{\\alpha_1}}\\left( \\mathbf{x}_1 - \\frac{\\beta_1}{\\sqrt{1-\\bar\\alpha_1}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\right)\n\\]\n이므로 이를 \\(L_0\\)에 대입합니다.\n\\[\n\\begin{aligned}\nL_0 &\\approx \\frac{1}{2\\sigma_1^2} \\lVert \\mathbf{x}_0 - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2 \\\\[10pt]\n&= \\frac{1}{2\\sigma_1^2} \\left\\lVert \\frac{1}{\\sqrt{\\bar\\alpha_1}} \\left( \\mathbf{x}_1 -  \\sqrt{1-\\bar\\alpha_1} \\boldsymbol{\\epsilon} \\right) - \\frac{1}{\\sqrt{\\alpha_1}}\\left( \\mathbf{x}_1 - \\frac{\\beta_1}{\\sqrt{1-\\bar\\alpha_1}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\right)  \\right\\rVert^2_2 \\\\[10pt]\n&= \\frac{1}{2\\sigma_1^2} \\left\\lVert - \\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}} \\boldsymbol{\\epsilon} + \\frac{1-\\alpha_1}{\\sqrt{\\alpha_1}\\sqrt{1-\\alpha_1}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\right\\rVert^2_2 \\qquad \\because \\bar\\alpha_1 = \\alpha_1 \\\\[10pt]\n&= \\frac{1}{2\\sigma_1^2} \\cdot \\frac{1-\\alpha_1}{\\alpha_1} \\lVert \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2\n\\end{aligned}\n\\]\n얻어진 \\(t=1\\)에 대한 손실은 식(12)에 \\(t=1\\)을 대입하고 상수를 무시한 결과와 일치 합니다.\n\\[\n\\begin{aligned}\nL_{0} &= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_1}\\cdot\\frac{\\beta_1^2}{\\alpha_1(1-\\bar\\alpha_1)} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}(\\mathbf{x}_1 , 1)  \\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_1}\\cdot\\frac{(1-\\alpha_1)^2}{\\alpha_1(1-\\bar\\alpha_1)} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}(\\mathbf{x}_1 , 1)  \\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_1}\\cdot\\frac{1-\\alpha_1}{\\alpha_1} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}(\\mathbf{x}_1 , 1)  \\rVert^2_2 \\right]\n\\end{aligned}\n\\]\n따라서 \\(L_0\\)를 계산하기 위해 적분항의 곱으로 정의된 별도의 손실 함수 식(13)을 따로 계산하지 않고 식(14)를 모든 시간 단계에 대해서 사용할 수 있습니다.\n\n\n\n\n지금까지 내용을 잘 이해했다면 다음 제시된 논문의 알고리즘을 선명하게 이해 할 수 있습니다.\n\n한줄 씩 따져보겠습니다.\n\n1: 학습의 반복을 나타냅니다.\n2: 준비된 학습 이미지중 하나를 선택합니다. 이 이미지가 \\(\\mathbf{x}_0\\)입니다.\n3: 임의의 시간 단계를 선택합니다.\n4: 표준 정규분포로 부터 노이즈를 샘플링합니다.\n5: 2, 3, 4단계에서 선택된 자료를 이용해서 \\(\\mathbf{x}_t\\)를 만들고 모델이 입력하여 출력으로 \\(\\boldsymbol{\\epsilon}_\\theta\\)를 생성합니다. 이를 4단계에서 샘플링한 노이즈와 비교하여 그래디언트를 구하고 경사하강법을 수행합니다.\n6: 이 단계를 손실이 충분히 줄어들 때까지 반복합니다.\n\n알고리즘 1을 수행함에 있어 이미지는 배치로 처리되므로 2, 3, 4단계에서 선택되는 이미지, 시간 단계, 노이즈는 배치수만큼 선택되어 일괄처리 될 것입니다.\n\n\n\n처음 DDPM 모델을 공부할 때 혼란스런 점은 학습된 모델이 깨끗한 이미지를 직접 출력하지 않는다는 점입니다. 학습 알고리즘을 보면 알 수 있듯이 모델이 출력하는 것은 입력 이미지에 포워드 프로세스를 적용할 때 사용한 노이즈입니다. 처음 이를 보면 “쓸데없이 노이즈를 예측해서 뭘 어쩌자는거지?” 라는 생각이 들게 됩니다. 하지만 식(11)에 의해 예측된 노이즈로 부터 이전 시간 단계에 대한 이미지의 평균을 구할 수 있습니다. 이를 이용해 다음처럼 반복적으로 이미지를 샘플링하게 됩니다.\n\n\n1: 모델에 입력될 완전 노이즈 \\(\\mathbf{x}_T\\)를 표준 정규분포로 부터 샘플링합니다.\n2: 시간 단계를 \\(T\\)에서 \\(1\\)까지 거슬러 오르면서 for 루프를 돕니다.\n3: 4단계에서 에측될 노이즈가 살짝 제거된 평균이미지에 추가할 노이즈 \\(\\mathbf{z}\\)를 샘플링합니다. 이 때 \\(t=1\\)인 마지막 시간 단계라면 노이즈를 0으로 둡니다.\n4: 식에 보이는 \\(\\frac{1}{\\sqrt{\\alpha}_t}(\\cdot)\\)부분은 식(11)에 의해 \\(\\mathbf{x}_t\\)로 부터 예측된 \\(\\mathbf{x}_{t-1}\\)의 평균 이미지이며 이 평균에 \\(\\sigma_t\\mathbf{z}\\)만큼의 노이즈를 추가하여 \\(\\mathbf{x}_{t-1}\\)을 만들어냅니다. 식에의해 계산된 평균을 샘플로 취해도 되지만 노이즈를 추가하면 확률적 다양성이 확보되고 최종 이미지 퀄리티도 더 좋아진다는 것이 경험적으로 알려져 있습니다. 단, 여기서 주목해야 하는 부분은 3 단계에 의해 \\(t=1\\)일때, 다시말해 \\(\\mathbf{x}_1\\)으로 부터 최종 이미지 \\(\\mathbf{x}_0\\)를 샘플링 할때는 노이즈를 추가하지 않습니다. 그 이유는 \\(L_0\\)를 설명하면서 이미 알아봤습니다.\n5: for루프를 계속 수행합니다.\n6: 생성된 이미지 \\(\\mathbf{x}_0\\)를 반환합니다.\n\n\n\n\n지금까지 DDPM의 이론적인 부분을 가능한 자세히 살펴봤습니다. 식이 많이 등장 하지만 계산만 복잡할 뿐 차근 차근 따라 간다면 누구나 이해할 수 있는 부분이고 다른 글들에서 풀어쓰기 귀찮은 이유로 생략된 부분들을 모두 다뤘습니다. 이 블로그가 존재하는 이유입니다. 이 내용을 바탕으로 실제 모델을 구현하는 내용을 다음 글에서 알아보도록 하겠습니다. 구현은 파이토치, 텐서플로 두가지 모두 제공될 예정이며 먼저 텐서플로 구현으로 다시 찾아오겠습니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part1.html#리버스-프로세스",
    "href": "posts/diffusion/ddpm_part1.html#리버스-프로세스",
    "title": "A Gentle Introduction to Diffusion Model: Part 1 - DDPM",
    "section": "",
    "text": "아래 그림은 Denoising Diffusion Probabilistic Models 논문(줄여서 DDPM)을 대표하는 그림입니다. 그림에서 \\(\\mathbf{x}_0\\)는 노이즈 없는 깨끗한 이미지 데이터를 의미합니다. 학습을 위해 모은 데이터 셋에서 샘플 하나입니다. 그림은 이 이미지 \\(\\mathbf{x}_0\\)에 단계적으로 노이즈가 확산되는 과정을 오른쪽에서 왼쪽으로 나타내고 있고 반대로 왼쪽에서 오른쪽으로 노이즈가 제거되는 과정을 나타내고 있습니다.\n\n이미지 \\(\\mathbf{x}_0\\)는 숫자 여러 개로 이뤄진 벡터 데이터입니다. 이미지의 픽셀이 숫자 하나에 해당합니다(컬러 이미지라면 픽셀당 숫자 세개). 개별 숫자가 어떤 값을 가지느냐에 따라 우리 눈에 보이는 모습이 달라지게 됩니다. 각 픽셀의 값이 특정 확률 분포를 따른다고 보면 \\(\\mathbf{x}_0\\)는 개별 숫자가 확률 변수인 다차원 확률 변수라 할 수 있습니다. 깨끗한 이미지 \\(\\mathbf{x}_0\\)가 확률 변수라면 이 변수가 따르는 분포가 있을 것이고 이를 다음처럼 표시할 수 있습니다.\n\\[\n\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)\n\\]\n분포 \\(q\\)는 이미지 공간에 원래 존재하는 분포입니다. 하지만 이 분포가 어떤 것인지 알지 못합니다. 만약 \\(q(\\mathbf{x}_0)\\)를 정확히 알고 있다면 이 분포에서 샘플링을 하기만하면 데이터 셋과 동일한 종류의 깨끗한 이미지를 다양하게 얻을 수 있을 것입니다. DDPM 논문에서 \\(q\\)라고 적는 분포는 원래 존재하는 분포, 다시말해 알아내고 싶은 분포를 의미합니다.\n이미지가 확률변수고 이 확률변수가 따르는 분포로 부터 샘플링해서 또 다른 이미지를 만들어 낸다는 이야기가 너무 이상하게 들릴 수 있습니다. 구체적인 이해를 위해 실험을 해보겠습니다.\n다음 셀을 실행해서 16x16x3 크기를 가지는 스프라이트 이미지 데이터 셋을 다운 받습니다. 해당 데이터 셋은 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work에서 사용하는 데이터 셋입니다.\n\n!gdown 1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\nTo: /home/metamath/etc/repo/blog/posts/diffusion/sprites_1788_16x16.npy\n100%|██████████████████████████████████████| 68.7M/68.7M [00:06&lt;00:00, 10.8MB/s]\n\n\n다운 받은 데이터 파일을 로딩하고 255로 나눠 픽셀값을 0~1사이로 노멀라이즈 합니다.\n\nsprites = np.load('sprites_1788_16x16.npy')\n\n\n# 준비된 (64,64)크기의 나비 이미지를 (28,28)로 리사이즈하고\n# 픽셀값을 0~1로 노멀라이즈\nx0 = (sprites / 255)[10]\n\n# 노멀라이즈 확인\nprint(x0.min(), x0.max())\n\n0.0 1.0\n\n\n다운받은 이미지를 화면에 출력해봅니다.\n\nplt.imshow(x0)\nplt.show()\n\n\n\n\n이제 x0가 확률변수라는 것을 실험해보기 위해 scipy에서 제공하는 다변수 정규분포 multivariate_normal를 임포트 합니다.\n\n# x0가 확률변수라는 것을 실험해보기 위해 다변수 정규분포를 임포트\nfrom scipy.stats import multivariate_normal\n\nx0를 16x16x3=768개 숫자를 가지는 벡터 변수로 보고 방금 준비한 x0을 평균으로 하는 정규분포 q_x0를 생성합니다.\n\n# 이미지를 768차원 확률 벡터 변수로 만들고\nx0_flt = x0.reshape(-1)\n\n# 이 이미지 x0_flt를 평균으로 하고\n# 0.1정도되는 적당한 수를 곱해서 공분산을 만들어 정규분포를 정의\n# 이 수가 커지면 평균으로 부터 멀리 떨어진 이미지까지 샘플링되고\n# 작으면 평균과 거의 비슷한 이미지들만 샘플링됨\nq_x0 = multivariate_normal(mean=x0_flt, cov=0.1 * np.eye(len(x0_flt)))\n\n생성된 분포 q_x0에서 값 3개를 샘플링해서 화면이 그려봅니다.\n\n# 3개만 샘플링 해서\nsamples = q_x0.rvs(size=3)\n# 크기를 보면 (3,768)\nprint(samples.shape)\n\n(3, 768)\n\n\n768차원 벡터 변수 3개가 샘플링 되었고 이를 적당히 모양 조정해서 화면에 그리면 다음처럼 그려집니다.\n\n# x0 주변에서 임의로 선택된 샘플들\nplt.imshow(samples.reshape(3,16,16,3).transpose(1,0,2,3).reshape(16,-1,3).clip(0,1))\nplt.show()\n\n\n\n\n나타나는 그림은 샘플링을 시도할 때마다 조금씩 달라지게 되고 q_x0를 만들 때 설정한 분산 cov에 곱하는 숫자 0.1을 더 크게 할 수록 점점 더 노이즈가 많은 이미지가 샘플링 될 수 있습니다. x0 주변에서 완전히 엉뚱한 노이즈 이미지가 샘플링되는 것이 아니라 평균 이미지를 중심으로 노이즈가 낀 이미지가 샘플링되는 것은 분명히 확인할 수 있습니다.\n이미지를 나타내는 변수 \\(\\mathbf{x}_0\\)가 확률변수라는 점을 실험을 통해 분명히 했으므로 나머지 내용을 계속 이어가도록 합시다.\nDDPM의 Fig.2는 완전한 노이즈 \\(\\mathbf{x}_T\\)로 부터 깨끗한 이미지 \\(\\mathbf{x}_0\\)가 만들어지는 과정을 그리고 있습니다. 즉, \\(\\mathbf{x}_T, \\mathbf{x}_{T-1}, \\mathbf{x}_{T-2}, ... , \\mathbf{x}_1\\)이 어떻게 선택되냐에 따라서 \\(\\mathbf{x}_0\\)가 결정된다는 것이고 DDPM 논문에서 이미지가 생성되는 과정을 이렇게 모델링하는 것입니다. 원인과 결과를 따져보자면 \\(\\mathbf{x}_0\\)는 결과가 되고 \\(\\mathbf{x}_T, \\mathbf{x}_{T-1}, \\mathbf{x}_{T-2}, ... , \\mathbf{x}_1\\)들은 \\(\\mathbf{x}_0\\)라는 결과를 만들어낸 원인이 되는 것입니다.\n하지만 오직 \\(\\mathbf{x}_0\\)만 관찰될 수 있고 어떤 \\(\\mathbf{x}_T\\) ~ \\(\\mathbf{x}_1\\)이 선택되어서 지금 보고 있는 \\(\\mathbf{x}_0\\)가 결정되었는지 알 수 없습니다. 이렇게 관찰되는 변수observable variable와 관계되어 영향을 미치지만 직접 관찰되지 않는 변수를 잠재 변수latent variable라 합니다.\n앞서 알아본것 처럼 \\(\\mathbf{x}_0\\)의 분포 \\(q(\\mathbf{x}_0)\\)를 알면 이 분포로 부터 \\(\\mathbf{x}_0\\)를 샘플링할 수 있습니다. 찾고 싶은 \\(q(\\mathbf{x}_0)\\)를 신경망 같은 모델로 만들어볼 수 있을 것입니다. 그렇게 신경망 따위로 만든 \\(\\mathbf{x}_0\\)의 분포를 \\(p_\\theta(\\mathbf{x}_0)\\)로 쓸 수 있습니다. 이렇게 DDPM에서는 학습으로 만들어가는 분포를 \\(p()\\)로 적고 원래 있는 분포 다시 말해 찾고 싶은 분포를 \\(q()\\)로 적습니다. \\(p\\) 아래 있는 \\(\\theta\\)는 모델이 \\(q(\\mathbf{x}_0)\\)처럼 잘 작동하기 위해 찾아야 하는 파라미터가 됩니다.\n\\(\\mathbf{x}_0, \\mathbf{x}_1, ... , \\mathbf{x}_T\\)들은 서로 연결joint되 있으므로 이 전체 확률변수들의 분포를 \\(p_\\theta(\\mathbf{x}_0, \\mathbf{x}_1, ... , \\mathbf{x}_T)\\) 로 쓸 수 있는데 원문에서는 이를 줄여 \\(p_\\theta(\\mathbf{x}_{0:T})\\)로 쓰고 \\(\\mathbf{x}_{0:T}\\)를 \\(\\mathbf{x}_0, \\mathbf{x}_1, ... , \\mathbf{x}_T\\)들이 모두 결합된 확률변수로 나타냅니다. 최종적으로 관심이 있는 분포는 \\(\\mathbf{x}_0\\)에 대한 분포 이므로 관심 없는 잠재변수는 주변화 시켜 다음처럼 나타낼 수 있습니다.\n\\[\np_{\\theta}(\\mathbf{x}_0) = \\int p_\\theta (\\mathbf{x}_{0:T}) d\\mathbf{x}_{1:T}\n\\]\n갑자기 적분 기호가 나와서 머리가 아플 수 있는데 위 식의 의미는 잠재 변수 \\(\\mathbf{x}_{1:T}\\)를 조건으로 \\(\\mathbf{x}_0\\)의 평균을 구한 것으로 생각하면 됩니다. 따라서 \\(p_{\\theta}(\\mathbf{x}_0)\\)가 구해진다면 이 분포는 우리에게 잠재 변수를 고려한 평균적인 \\(\\mathbf{x}_0\\)를 샘플링할 수 있게 해줄 것입니다.\n만약 \\(p_\\theta (\\mathbf{x}_{0:T})\\)를 완전히 알고 있고 이 분포를 사용해서 샘플링하게 된다면 완전 노이즈 이미지 \\(\\mathbf{x}_T\\)와 여러 단계를 거쳐 노이즈가 조금씩 제거된 이미지 \\(\\mathbf{x}_{T-1}, ... \\mathbf{x}_1\\), 그리고 마지막 깨끗한 이미지 \\(\\mathbf{x}_0\\)를 모두 한 세트로 뽑을 수 있게 될 것입니다. 그런 샘플링이 가능하다면 다음과 같은 샘플은 뽑힐 가능성이 아주 높을 것입니다.\n\n\n반면 다음 같이 빨간 캐릭터가 살짝 보이다가 갑자기 하얀 캐릭터로 바뀌면서 노이즈가 제거되는 샘플은 뽑힐 가능성은 아주 낮겠죠.\n\n이렇게 노이즈가 제거 되어 가는 과정에 대한 변수를 한꺼번에 뽑을 수 있는 분포 \\(p_\\theta (\\mathbf{x}_{0:T})\\)를 리버스 프로세스reverse process라고 합니다. 이 리버스 프로세스에는 고차원의 확률변수들이 너무 많이 결합되어 있으므로 문제를 간단히 하기 위해 마르코프 가정을 하게 됩니다. 원래는 \\(\\mathbf{x}_{0}\\)에 잠재 변수 \\(\\mathbf{x}_{1:T}\\) 모두가 영향을 미치는 것으로 이야기했지만 모델링 과정에서 마르코프 과정을 가정하고 \\(\\mathbf{x}_{0}\\)에는 \\(\\mathbf{x}_{1}\\)만 잠재 변수가 되게 모델링하게 됩니다. 동일하게 \\(\\mathbf{x}_{1}\\)에는 \\(\\mathbf{x}_{2}\\)만이 잠재 변수가 되겠네요. 마르코프 가정을 하고 각 시간 단계에 대한 이미지의 분포를 다음처럼 정의합니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n위 정의에서 각 이미지에 대한 분포를 정규분포로 가정했습니다. 위 식의 의미는 노이즈가 조금 더 제거된 \\(\\mathbf{x}_{t-1}\\)에 대한 분포는 바로 이전 단계인 노이즈가 약간 더 많은 \\(\\mathbf{x}_{t}\\)를 이용해 계산된 어떤 평균 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t)\\)와 분산 \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)을 파라미터로 하는 정규분포로 정의 한다는 것입니다. 이 때 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t)\\), \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)같은 것들은 입력 \\(\\mathbf{x}_t\\)를 보고 네트워크가 추정해야 하는 값들 입니다. 다시말해 노이즈가 많은 \\(\\mathbf{x}_t\\)를 네트워크의 입력으로 넣으면 네트워크는 그 입력을 이용해서 정규분포로 가정된 노이즈가 약간 더 적은 \\(\\mathbf{x}_{t-1}\\)의 분포를 평균과 분산을 추정해서 알아내는 것입니다.\n마지막 단계인 \\(\\mathbf{x}_T\\)는 순수한 가우시안 노이즈라고 보면 \\(p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\)가 되겠고 위 정의와 함께 쓰면 리버스 프로세스 \\(p_\\theta(\\mathbf{x}_{0:T})\\)는 마르코프 가정에 의해 다음처럼 모두 곱해진 형태로 정의될 수 있습니다.\n\\[\np_\\theta(\\mathbf{x}_{0:T}) := p(\\mathbf{x}_T) \\prod^{T}_{t=1}p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) \\tag{1}\n\\]\n마르코프 가정 덕분에 적어도 무시무시한 적분 기호는 사라졌네요. 😁"
  },
  {
    "objectID": "posts/diffusion/ddpm_part1.html#포워드-프로세스",
    "href": "posts/diffusion/ddpm_part1.html#포워드-프로세스",
    "title": "A Gentle Introduction to Diffusion Model: Part 1 - DDPM",
    "section": "",
    "text": "멀쩡한 이미지에 노이즈가 점점 확산되어 최종적으로 완전히 노이즈 \\(\\mathbf{x}_T\\)가 되는 과정을 포워드 프로세스forward process라고 합니다. 그림에선 오른쪽에서 왼쪽으로 진행되는 과정입니다. 포워드이라고 하면 보통 왼쪽에 오른쪽으로 진행되는 그림을 상상하게 되는데 논문에서는 유독 이를 거꾸로 그렸습니다. \\(\\mathbf{x}\\)에 대한 인덱스도 0에서 \\(T\\)까지가 오른쪽에서 왼쪽으로 진행되도록 그려져서 처음 이 그림을 보면 한동안은 포워드 프로세스가 어느 방향인지 계속 햇갈리게 됩니다. 최종 목적이 노이즈로 부터 이미지를 만들어 가는 과정이므로 논문 저자들은 이렇게 반대로 그려놓는 것이 아마 더 자연스럽다고 생각한것 같습니다.\n리버스 프로세스를 알아보면서 잠재 변수 \\(\\mathbf{x}_{1:T}\\)는 이미지 \\(\\mathbf{x}_0\\)에 대한 원인이고 \\(\\mathbf{x}_0\\)는 결과라고 했습니다. 결과를 조건으로 하는 원인의 확률을 사후 확률posterior이라고 합니다. 그럼 사후 확률 분포는 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)로 쓸 수 있습니다. 이 사후 확률을 포워드 프로세스 또는 디퓨전 프로세스diffusion process라고 합니다. 앞서 이야기한 것처럼 그림에서 오른쪽에서 왼쪽으로 진행되는 과정입니다. 이 확률 분포는 \\(\\mathbf{x}_0\\)가 주어지면 이 이미지를 생성하기 위해 거쳐가야 하는 모든 잠재변수 \\(\\mathbf{x}_{1:T}\\)에 대한 분포를 정의하게 됩니다.\n리버스 프로세스는 직접 수식으로 계산할 수 없지만 포워드 프로세스는 마르코프 과정을 상정하고 각 과정이 정규분포라고 가정하면 직접 계산할 수 있습니다. 리버스 프로세스 때와 같이 마르코프 가정을 하면 그림에서 나타낸 노이즈가 조금 적은 \\(\\mathbf{x}_{t-1}\\)을 조건으로 그 다음 노이즈가 조금 더 많은 이미지 \\(\\mathbf{x}_t\\)에 대한 분포는 다음처럼 정의 할 수 있습니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\]\n이 식에서 \\(\\beta_t\\)는 각 단계에서 노이즈를 얼마나 추가할 지 결정하게 되는 상수입니다. 이렇게 정의된 개별 분포를 모두 곱해서 잠재 변수에 대한 사후 확률, 포워드 프로세스를 정의 합니다.\n\\[\nq(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0):= \\prod^{T}_{t=1} q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) \\tag{2}\n\\]\n앞서 리버스 프로세스에서 노이즈를 제거하는 한 단계 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)를 정의한 바 있습니다. 노이즈가 더해지는 과정 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)로 부터 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 바로 알아 낼 수 있으면 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)를 만들 필요가 없습니다. 하지만 이를 위해 베이즈 정리를 사용한다면 모든 시간 단계에 대한 \\(q(\\mathbf{x}_t)\\)를 다 알아야 하므로 쉽지 않은 일입니다.\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) q(\\mathbf{x}_{t-1})}{q(\\mathbf{x}_t)}\n\\]\n그래서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)의 역과정인 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)로 대신하고자 하는게 DDPM에서 하고자 하는 것입니다.\n그런데 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)을 잘 만들려면 지도 학습 관점에서 비교 대상인 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 알아야 하는데 이 분포는 모른다고 했으니 학습에 사용할 비교 대상이 없습니다. 이 문제를 인식하고 이를 해결하는 과정을 이해하는 것이 DDPM 논문을 이해하는 거의 전부라 할 수 있으니 차차 알아보도록 하겠습니다.\n이제 정의된 포워드 프로세스를 실험해보기 위해 \\(\\beta_t\\), \\(T\\)같은 값들을 정해야 하는데 DDPM 논문에서는 각 설정값을 다음처럼 지정했다고 나와있습니다.\n\n\\(T=1000\\), \\(\\beta_1 = 10^{-4}\\), \\(\\beta_T = 0.02\\)\n\n논문과 동일하게 beta() 함수를 작성합니다.\n\ndef beta(t, T=1000):\n    # t: 1~T\n    # t는 1에서 T까지 이므로 인덱싱할 때는 -1해준다.\n    return np.linspace(1.0e-4, 0.02, T)[t-1]\n\n실험에 사용할 샘플 이미지 \\(\\mathbf{x}_0\\)를 준비합니다. 이때 이미지의 픽셀 값들이 -1, 1사이에 오게 노멀라이즈 합니다.\n\nx0 = (sprites / 255)[10] * 2 - 1\nx0.min(), x0.max()\n\n(-1.0, 1.0)\n\n\n이 이미지의 크기는 (16,16,3)입니다.\n\nx0.shape\n\n(16, 16, 3)\n\n\n분포 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)를 만들어주는 함수 get_q_xt_given_xtm1()을 정의합니다. 이 함수는 내부에서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\) 정의에 따라 정규분포를 하나 만들고 그 분포를 반환합니다.\n\ndef get_q_xt_given_xtm1(xtm1, t):\n    beta_t = beta(t)\n    xtm1_flt = xtm1.reshape(-1)\n\n    q = multivariate_normal(\n        mean=np.sqrt(1-beta_t)*xtm1_flt,\n        cov=beta_t*np.eye(len(xtm1_flt))\n    )\n\n    return q\n\n샘플 x0를 사용해서 \\(\\mathbf{x}_1\\)에 대한 분포를 생성합니다.\n\n# t=1을 지정해 원본 이미지 x0에서 1단계 노이즈 상태인 이미지에 대한 분포를 생성\nq_x1_given_x0 = get_q_xt_given_xtm1(x0, t=1)\n\n이 분포는 \\(\\sqrt{1-\\beta}\\mathbf{x}_0\\)를 평균으로 하는 정규분포이므로 \\(\\mathbf{x}_0\\)에 대한 밀도 함숫값, 다시말해 이 분포에서 \\(\\mathbf{x}_0\\)가 존재할 가능성은 크고 일반 노이즈에 대한 가능성은 작아야 합니다. 밀도 함숫값의 로그값을 계산해주는 logpdf() 함수로 확인해봅시다.\n\n# x0에 대한 확률 밀도값                  그냥 노이즈에 대한 확률 밀도값\nq_x1_given_x0.logpdf( x0.reshape(-1) ), q_x1_given_x0.logpdf( np.random.randn( 16*16*3 ) )\n\n(2831.0183943629117, -6699964.090514234)\n\n\n예상대로 \\(\\mathbf{x}_0\\)에 대한 값은 매우 크고 표준 정규분포로 부터 샘플링된 노이즈에 대한 값은 로그값이 매우 작은 음수이므로 거의 0임을 알 수 있습니다. 이제 정의된 분포로부터 \\(\\mathbf{x}_1\\) 하나를 샘플링합니다.\n\nx1 = q_x1_given_x0.rvs(size=1)\n\n제대로 작동한다면 x1은 x0와 거의 차이가 없어야 할 것입니다.\n\nfig, ax = plt.subplots(nrows=1, ncols=2)\n\nx0_ = ((x0 - x0.min()) / (x0.max() - x0.min())).clip(0,1)\nx1_ = ((x1 - x1.min()) / (x1.max() - x1.min())).clip(0,1)\n\nax[0].imshow(x1_.reshape(16,16,3))\nax[0].set_title(r\"$\\mathbf{x}_1$\")\n\nax[1].imshow(x0_.reshape(16,16,3))\nax[1].set_title(r\"$\\mathbf{x}_0$\")\nplt.show()\n\n\n\n\n한 스탭정도 노이즈를 확산시켜서는 아무런 차이가 없는듯 보입니다. 경우에 따라 흰색 배경부분을 자세히 보면 완전 흰색이 아니라 약간 색이 달라진 것을 미세하게 확인할 수 있을 수도 있습니다(아주 약하게 나타나거나 모니터에 따라 확인되지 않을 수 있음). 이제 \\(T\\) 단계까지 한 단계씩 차례로 노이즈를 확산시킵니다. for루프로 이를 직접 구현해보면 다음과 같습니다. 확산 단계는 30단계까지로 제한했습니다.\n\nx0.min(), x0.max()\n\n(-1.0, 1.0)\n\n\n\n%%time\n\n# 루프 돌면서 x30까지 해보기\nxts = [x0.reshape(-1).copy()]\nxt = xts[0]\nT = 30\n\nfor t in range(1, T+1):\n    # t-1 단계에서 만들어진 이미지로 부터 분포 q(x_t|x_t-1)을 만든다.\n    q_xt_given_xtm1 = get_q_xt_given_xtm1(xt, t)\n    # 만들어진 분포에서 샘플링한다.\n    xt = q_xt_given_xtm1.rvs(size=1)\n    xts.append(xt)\n\nCPU times: user 9.76 s, sys: 3.04 s, total: 12.8 s\nWall time: 3.25 s\n\n\n30단계만 진행했는데도 시간이 상당히 오래 걸립니다. 분포를 정의하고 그로 부터 샘플링하는 과정을 기술적으로 잘 처리해서 속도를 조금 높일 수 있겠지만 원리적으로 시간이 오래 걸리는 과정이라는 사실은 변함이 없습니다. 노이즈가 확산된 30개 이미지와 원본이미지를 담은 리스트를 넘파이 어레이로 변환합니다.\n\nxts = np.array(xts)\nxts.shape\n\n(31, 768)\n\n\n이제 30단계까지 확산된 노이즈를 가진 이미지를 원본 이미지와 비교해보겠습니다.\n\nfig, ax = plt.subplots(nrows=1, ncols=2)\n\nx0_ = ((x0 - x0.min()) / (x0.max() - x0.min())).clip(0,1)\nxt_ = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n\nax[0].imshow(xt_.reshape(16,16,3))\nax[0].set_title(f\"$\\mathbf{{x}}_{{{T}}}$\")\n\nax[1].imshow(x0_.reshape(16,16,3))\nax[1].set_title(r\"$\\mathbf{x}_0$\")\nplt.show()\n\n\n\n\n확실이 점점 노이즈로 뒤덮히기 시작합니다. DDPM에서는 \\(T=1000\\)을 사용하므로 1000단계까지가면 원래 이미지는 완전히 사라지고 노이즈만 있는 이미지가 될 것입니다.\n이제 식(2)에 의해 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)를 직접 계산할 수 있습니다. \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)를 직접 눈으로 확인해보기 위해 \\(\\mathbf{x}_t\\)들을 스칼라로 가정하고 \\(T=2\\)로 두어 \\(x_0\\), \\(x_1\\), \\(x_2\\)로 포워드 프로세스 단계를 제한합니다. 그렇게하면 3차원 그래프로 해당 분포를 그려볼 수 있습니다.\n식(2)에 의해 다음과 같으므로\n\\[\nq(x_{1:2} \\mid x_0) = q(x_2 \\mid x_1) \\times q(x_1 \\mid x_0)\n\\]\n두 일변수 정규분포를 곱하고 정리하면 다음처럼 \\(q(x_{1:2} \\mid x_0)\\)를 \\(x_1\\), \\(x_2\\)에 대해 계산해주는 함수를 만들 수 있습니다.\n\ndef make_q(x0):\n\n    def q(x2, x1):\n        return (1 / (2*np.pi)) * np.exp( -(1/2) * ((x2-x1)**2 + (x1-x0)**2) )\n\n    return q\n\n해당 확률분포가 \\(x_0\\)에 대해서 어떻게 변하는지 확인하기 위해 그림그리는 모듈을 임포트 합니다.\n\\(x_1\\), \\(x_2\\)는 -10, 10 정도 범위로 설정하고 \\(x_0\\)는 -5, 5까지 범위를 설정해 각 \\(x_0\\)에 대해서 \\(q(x_{1:2} \\mid x_0)\\)를 -10, 10로 정의된 정사각 영역에 대해서 함숫값을 모두 계산합니다. 아울러 이렇게 생성된 \\(x_0\\)에 대해서 \\(q(x_{1:2} \\mid x_0)\\)들이 확률분포로써 타당한지 확인하기 위해 수치적분값이 1이 되는지 확인해봅니다.\n\nfrom scipy import integrate\n\n# 정의역 정의\nx_range = [-10, 10]\ny_range = [-10, 10]\n\nx_min, x_max = x_range[0], x_range[1]\ny_min, y_max = y_range[0], y_range[1]\n\nxx = np.linspace(x_min, x_max, 150)\nyy = np.linspace(y_min, y_max, 150)\nX1, X2 = np.meshgrid(xx, yy)\nX_grid = np.c_[X1.ravel(), X2.ravel()]\n\nx0s = np.linspace(-5, 5, 21)\nZs = []\n\nfor x0_ in x0s:\n    q = make_q(x0=x0_)\n    Zs.append( q(X_grid[:,0], X_grid[:,1]) )\n\n    # 2차원 수치 적분\n    print(f\"{integrate.dblquad(q, x_range[0], x_range[1], lambda x: y_range[0], lambda x: y_range[1])[0]:.4f}\")\n\n0.9998\n0.9999\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9999\n0.9998\n\n\n적분값은 모두 거의 1에 가까운 값임을 확인할 수 있습니다. 이제 앞서 x0s변수에 범위를 잡아둔 \\(x_0\\)에 따른 \\(x_1\\), \\(x_2\\)의 분포를 그림으로 그립니다.\n\nlayout = go.Layout(\n    title='q(x_1:2|x_0)',\n    width=600, height=600,\n    margin=dict(l=0, r=0, b=0, t=25),\n    scene = dict(\n        xaxis = dict(title='x2', range=[x_min, x_max],),\n        yaxis = dict(title='x1', range=[y_min, y_max],),\n        zaxis = dict(title='pdf'),\n        aspectratio=dict(x=1, y=1, z=0.5)\n    )\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\n\n# Add traces, one for each slider step\nfor Z in Zs:\n    fig.add_trace(\n        go.Surface(\n            x=X_grid[:,0].reshape(X1.shape), y=X_grid[:,1].reshape(X1.shape),\n            z=Z.reshape(X1.shape),\n            showscale=False,  opacity=1.,\n            colorscale ='Blues',\n            contours=dict(\n                x=dict(show=True, highlight=True),\n                y=dict(show=True,  highlight=True),\n                z=dict(show=True,  highlight=True),\n            ), visible=False\n        )\n    )\n\nfig.data[0].visible = True\n\n# Create and add slider\nsteps = []\n\nfor i in range(0, len(fig.data)):\n    step = dict(\n        method=\"update\",\n        args=[\n            {\"visible\": [False] * (len(fig.data))},\n        ],  # layout attribute\n        label=f\"{x0s[i]:.2f}\"\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [\n    dict(\n        active=0,\n        currentvalue={\"prefix\": \"x0: \"},\n        pad={\"l\":10, \"t\": 50, \"r\":10, \"b\":10},\n        steps=steps\n    )\n]\n\nfig.update_layout(sliders=sliders)\n\nfig.show()\n\n\n                                                \n\n\n위 그림에서 슬라이드를 이동시키면 확률분포가 변하는 것을 확인할 수 있습니다. 여기서 슬라이드로 조정하는 \\(x_0\\)는 노이즈가 없는 깨끗한 이미지로 생각할 수 있습니다. 즉 슬라이드의 이동으로 깨끗한 이미지가 변하면 이와 연결된 잠재 변수들의 결합 분포가 변하는 것을 눈으로 확인 해볼 수 있는 것입니다. 당연하겠지만 잠재 변수들은 \\(x_0\\) 근처에 분포하게 됩니다.\n이 시각화를 통해서 이미지 \\(\\mathbf{x}_0\\)와 노이즈가 포함된 이미지 \\(\\mathbf{x}_1, ..., \\mathbf{x}_T\\)에 대한 관계가 조금 더 명확히 이해 되었으면 좋겠습니다.\n이제 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)가 무엇을 의미하는지 잘 이해했으므로 이미지 변수에 대해서 이 분포를 직접 계산하는 함수를 만들어 봅시다.\n다음 함수 get_q_x1T_given_x0()는 루프를 돌면서 식(2)를 직접 계산하는 함수 q()를 리턴합니다. 이 때 확률 밀도 함숫값은 매우 큰 값이 되므로 이를 곱하면 오버플로가 생기게 됩니다. 이를 방지하기 위해 로그 확률 밀도값을 더하는 방식을 사용합니다.\n\ndef get_q_x1T_given_x0(x0, T=T, x0_shape=(16,16,3)):\n    # T개의 확률변수를 만든다.\n    def q(x1T):\n        # x1T: T*16*16*3\n        x1_T = x1T.reshape(T, np.prod(x0_shape)) # (T, 16*16*3)\n        x0_T = np.concatenate((x0.reshape(1,-1), x1_T), axis=0) # (T+1, 16*16*3)\n\n        density = 0.0\n        for t in range(1, T+1):\n            q_xt_given_xtm1 = get_q_xt_given_xtm1(x0_T[t-1], t)\n            density += q_xt_given_xtm1.logpdf(x0_T[t])\n\n        return density\n\n    return q\n\nx0를 조건으로 입력하여 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\) 분포를 생성합니다.\n\nq_x130_given_x0 = get_q_x1T_given_x0(x0, T=T)\n\n\\(q(\\mathbf{x}_1 \\mid \\mathbf{x}_{0})\\)에 대해서 \\(\\mathbf{x}_0\\)와 표준 정규분포로 부터 샘플링된 노이즈를 입력하여 각 샘플에 대한 가능성을 logpdf()로 직접 계산해본 실험을 여기서 다시 해보겠습니다.\n\\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)에 대해서 표준 정규분포로 부터 샘플링된 (30, 16, 16, 3) 크기를 가지는 노이즈를 입력합니다.\n\nx_random = np.random.randn( (T+1), 16, 16, 3 )\n\nfig, ax = plt.subplots(figsize=(13,10), nrows=5, ncols=6)\n\nfor i in range(5):\n    for j in range(6):\n        xt = x_random[i*5+j+1]\n        xt = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n        ax[i][j].imshow(xt.reshape(16,16,3))\n        title = f\"$x_{{{i*6+j+1}}}$\"\n        ax[i][j].set_title(title)\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n\nfig.suptitle(r\"$\\mathbf{x}_{1:30}$\", y=0.93)\nplt.show()\n\n\n\n\n위 그림이 \\(\\mathbf{x}_{1:30}\\)을 나타냅니다. 이 샘플은 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)에서 존재할 가능성이 매우 작을 것입니다.\n\nq_x130_given_x0( x_random[1:].reshape(-1) )\n\n-77323292.1886759\n\n\n거의 0이 됩니다. 반면 조금전 \\(\\mathbf{x}_0\\)로부터 30단계까지 노이즈를 확산하면서 얻은 이미지 30장 세트로 구성된 \\(\\mathbf{x}_{1:30}\\)는 분포 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)상에서 큰 가능성을 가질 것입니다. 이 샘플이 xts에 저장되어 있으므로 실험을 해볼 수 있습니다.\n\nfig, ax = plt.subplots(figsize=(13,10), nrows=5, ncols=6)\n\nfor i in range(5):\n    for j in range(6):\n        xt = xts[i*5+j+1]\n        xt = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n        ax[i][j].imshow(xt.reshape(16,16,3))\n        title = f\"$x_{{{i*6+j+1}}} \\sim q(x_{{{i*6+j+1}}} | x_{{{i*6+j}}})$\"\n        ax[i][j].set_title(title)\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n\nfig.suptitle(r\"$\\mathbf{x}_{1:30}$\", y=0.93)\nplt.show()\n\n\n\n\n\nq_x130_given_x0( xts[1:].reshape(-1) )\n\n59104.04837779686\n\n\n예상처럼 꽤 큰 값을 돌려줍니다. 만약 이 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)로 부터 정밀하게 샘플링 할 수 있다면 위 그림과 비슷한 하지만 조금씩 다른 노이즈가 확산된 이미지 세트를 얻을 수 있을 것입니다.\n마지막으로 \\(\\mathbf{x}_{15}\\)를 노이즈 확산의 시작 이미지인 \\(\\mathbf{x}_0\\)로 설정하고 30단계 노이즈를 확산하면 방금 실험한 \\(\\mathbf{x}_{1:30}\\)과는 노이즈 정도가 사뭇 다른 이미지들이 얻어질텐데 이 이미지들의 \\(q(\\mathbf{x}_{1:30} \\mid \\mathbf{x}_0)\\)에서 가능성은 어떻게 될지 실험해보겠습니다.\n\n%%time\n\n# xts[15]에 저장된 x_15를 시작이미지 y0로 두고 y1 ~ y30까지 생성\nyts = [xts[15].reshape(-1)]\nyt = yts[0]\n\nfor t in range(1, T+1):\n    # t-1 단계에서 만들어진 이미지로 부터 분포 q(x_t|x_t-1)을 만든다.\n    q_xt_given_xtm1 = get_q_xt_given_xtm1(yt, t)\n    # 만들어진 분포에서 샘플링한다.\n    yt = q_xt_given_xtm1.rvs(size=1)\n    yts.append(yt)\n\nyts = np.array(yts)\nyts.shape\n\nfig, ax = plt.subplots(figsize=(13,10), nrows=5, ncols=6)\n\nfor i in range(5):\n    for j in range(6):\n        xt = yts[i*6+j+1]\n        xt = ((xt - xt.min()) / (xt.max() - xt.min())).clip(0,1)\n        ax[i][j].imshow(xt.reshape(16,16,3))\n        title = f\"$x_{{{i*6+j+1}}} \\sim q(x_{{{i*6+j+1}}} | x_{{{i*6+j}}})$\"\n        ax[i][j].set_title(title)\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n\nfig.suptitle(r\"$\\mathbf{x}_{1:30}$\", y=0.93)\nplt.show()\n\nq_x130_given_x0( yts[1:].reshape(-1) )\n\n\n\n\nCPU times: user 13.4 s, sys: 3.89 s, total: 17.3 s\nWall time: 4.81 s\n\n\n44704.27845529124\n\n\n그려진 샘플을 확인해보세요. 이번 샘플은 처음부터 노이즈가 존재합니다. 이렇게 확산된 노이즈 정도가 다르기 때문에 확률분포에서 존재할 가능성은 조금 떨어진것을 알 수 있습니다.\n다음에 지금까지 야이기한 리버스 프로세스와 포워드 프로세스를 다시 정리했습니다.\n\n리버스 프로세스와 포워드 프로세스 \\[\n\\begin{aligned}\n&p_\\theta(\\mathbf{x}_{0:T}) := p(\\mathbf{x}_T) \\prod^{T}_{t=1}p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t), \\qquad &p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right), \\quad p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})  \n\\\\\n&q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0):= \\prod^{T}_{t=1} q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}), \\qquad &q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\end{aligned}\n\\]\n\n리버스 프로세스에만 파라미터가 있고 포워드 프로세스에는 없는 점을 유념해야 합니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part1.html#손실-함수",
    "href": "posts/diffusion/ddpm_part1.html#손실-함수",
    "title": "A Gentle Introduction to Diffusion Model: Part 1 - DDPM",
    "section": "",
    "text": "앞에서 정의한 리버스 프로세스에서 잠재 변수를 주변화시켜 \\(p_\\theta (\\mathbf{x}_0)\\)를 얻을 수 있다는 점을 이야기했습니다.\n\\[\np_{\\theta}(\\mathbf{x}_0) = \\int p_\\theta (\\mathbf{x}_{0:T}) d\\mathbf{x}_{1:T}\n\\]\n이렇게 얻은 \\(p_\\theta (\\mathbf{x}_0)\\)는 \\(\\theta\\)에 대한 \\(\\mathbf{x}_0\\)의 가능도 함수입니다. 파라미터 \\(\\theta\\)를 조정해서 이 가능도를 최대화 시키는 것을 최대 가능도 추정Maximum Likelihood Estimation이라 하고 많은 모델들이 이 MLE 과정으로 학습하게 됩니다. 이처럼 가능도 함수를 최대화 시키는 \\(\\theta\\)를 최적화 알고리즘을 통해 찾는 것이 학습의 목표인데 최적화는 보통 함수의 최소화를 수행하므로 \\(-1\\)을 곱해서 마이너스 로그 가능도로 만듭니다. 또 가능도 함수는 곱하기에 의해 정의되는 경우가 많기 때문에 가능도 함수를 직접 다루기 보다 로그를 씌워서 로그 가능도 함수를 다루는 경우가 많습니다 (실제로 앞서 정의한 \\(p_\\theta (\\mathbf{x}_{0:T})\\)도 많은 곱하기로 이뤄져 있음을 알 수 있습니다).\n\\[\n- \\log p_{\\theta}(\\mathbf{x}_0) = - \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right)\n\\]\n우변은 잠재 변수에 대한 적분입니다. 이 적분을 앞서 알아본 잠재 변수의 \\(\\mathbf{x}_0\\)에 대한 조건부 분포 \\(q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)\\)에 대한 기댓값으로 바꾸기 위해 우변을 다음처럼 변형합니다.\n\\[\n\\begin{aligned}\n- \\log p_{\\theta}(\\mathbf{x}_0) &= - \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\int q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0) \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} \\left[ \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}  \\right] \\right)\n\\end{aligned}\n\\]\n위 식은 \\(\\mathbf{x}_0\\)에 대한 가능도는 \\(\\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\)를 \\(\\mathbf{x}_0\\)를 조건으로 하는 모든 존재 가능한 잠재 변수 \\(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0\\)에 대해서 평균을 취한것과 같다는 것을 이야기 합니다.\n여기서 볼록함수convex function에서 성립하는 젠슨 부등식Jensen’s Inequality를 적용하는데 젠슨 부등식을 다음과 같습니다.\n\\[\nf(\\mathbb{E}[x]) \\le \\mathbb{E}[f(x)]\n\\]\n유도하고 있는 식에서 $ -$는 볼록함수이므로 젠슨 부등식이 성립하고 이를 적용하면 다음과 같습니다.\n\\[\n\\begin{aligned}\n- \\log p_{\\theta}(\\mathbf{x}_0) &= - \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\int q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0) \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}   d\\mathbf{x}_{1:T}\\right) \\\\[10pt]\n&= - \\log \\left( \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} \\left[ \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}  \\right] \\right) \\\\[10pt]\n&\\le \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\n\\end{aligned}\n\\]\n방금 유도한 식은 마이너스 로그 가능도 함수의 상한을 알려 줍니다. 그 상한은 잠재 변수에 대한 평균이라는 것도 이해했습니다. 이 상한을 최소화하면 마이너스 로그 가능도도 자연스럽게 작아질 수 밖에 없습니다.\n이제 좌변을 모든 \\(\\mathbf{x}_0\\)에 대한 평균으로 만듭니다. 그럼 다음과 같은 과정을 통해 DDPM 식(3)의 부등식 부분이 유도 됩니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] &= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[- \\log \\left( \\int p_\\theta (\\mathbf{x}_{0:T})   d\\mathbf{x}_{1:T}\\right) \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log \\left( \\int q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0) \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}   d\\mathbf{x}_{1:T}\\right) \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log \\left( \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} \\left[ \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}  \\right] \\right) \\right] \\\\[10pt]\n&\\le \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\n\\end{aligned}\n\\]\n마지막 등식은 다음에 제시된 Law of total expectation \\(\\mathbb{E}[X] = \\mathbb{E}\\left[\\mathbb{E}[X \\mid Y]\\right]\\)으로 성립합니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(y)} \\left[ \\mathbb{E}_{q(x \\mid y)} \\left[ f(x,y) \\right] \\right] &= \\int_{y} q(y) \\int_{x} q(x \\mid y) f(x, y) dx dy\\\\[5pt]\n&= \\int_y \\int_x q(x \\mid y) q(y) f(x,y) dx dy \\\\[5pt]\n&= \\int_y \\int_x q(x, y) f(x,y) dx dy \\\\[5pt]\n&= \\mathbb{E}_{q(x,y)} [f(x,y)]\n\\end{aligned}\n\\]\n정리하면 다음처럼 됩니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] \\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] \\tag{3*}\n\\]\n식(3*)는 깨끗한 이미지 \\(\\mathbf{x}_0\\)의 \\(\\theta\\)에 대한 마이너스 로그 가능도는 우변보다 클 수 없다는 것을 나타냅니다. 여기서 우변을 Variational Bound라고 합니다. 왜 variational 이란 단어를 쓰냐면 이 상한은 잠재 변수의 분포로 제안된 함수 \\(q(\\mathbf{x}_{1:T}\\mid \\mathbf{x}_0)\\)에 따라 달라지기 때문입니다.\n지금까지 힘들게 겨우 식(3)의 앞 부분을 유도해봤는데 왜 이런 짓을 하고 있는지 이해하는것이 식을 구체적으로 유도하는 것 보다 더 중요할 것입니다. 이 과정을 조금 살펴보도록 하겠습니다.\n데이터 \\(\\mathbf{x}_0\\)에 대한 로그 가능도는 다음처럼 분해될 수 있습니다.\n\\[\n\\log p_\\theta(\\mathbf{x}_0) = \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right] + \\left( - \\mathbb{E}_{q(\\mathbf{x}_{1:T})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_{0})}{q(\\mathbf{x}_{1:T})} \\right] \\right)\n\\]\n위 분해가 어떻게 가능한지 조금 더 자세히 알고 싶은 독자는 EM 알고리즘에 대해서 정리한 A Step by Step Introduction to EM Algorithm를 참고하시면 좋겠습니다. 좀 긴 분량이 부담이 될 수 있지만 EM 알고리즘에 대해 국내서 가장 자세하고 친철히 설명한 문서라 할 수 있겠습니다.😁\n어쨌거나 이렇게 분해된 우변의 첫 번째 항을 Variational Low Bound 또는 Evidence Lower Bound(앞으로 ELBO로 표기)라고 하고 두 번째 항을 KD Divergence라고 합니다.(앞으로 \\(D_{KL}\\)로 표기) 이 \\(D_{KL}\\)은 두 확률분포의 차이를 측정하는 값으로 해석됩니다. 이 식에서 \\(D_{KL}\\) 항은 \\(p_\\theta(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)\\)와 \\(q(\\mathbf{x}_{1:T})\\)의 차이를 나타내는 값이 됩니다. 그리고 또 중요한 성질은 항상 0과 같거나 큰 값을 가지게 되고 그렇기 때문에 첫 번째 항이 가능도의 하한이 되게 됩니다. \\(D_{KL}\\)을 고려하지 않으면 다음처럼 쓸 수 있습니다.\n\\[\n\\log p_\\theta(\\mathbf{x}_0) \\ge \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right]  \n\\]\n여기서 우변에 \\(q(\\mathbf{x}_{1:T})\\)는 잠재변수 \\(\\mathbf{x}_{1:T}\\)의 분포에 대해서 알지 못하므로 임의로 제안된 분포입니다. 잠재변수에 대한 분포를 어떤 분포로 제안을 해도 위 식이 성립함을 의미합니다. 이 식과 유도된 식 (3*)을 비교하면\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] &\\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\\\\[10pt]\n\\log p_\\theta(\\mathbf{x}_0) &\\ge \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right]  \n\\end{aligned}\n\\]\n가 되는데 마이너스가 곱해져서 부등호의 방향이 반대인 것과 양변에 확률변수 \\(\\mathbf{x}_0\\)에 대한 기댓값 연산이 적용된 것이 차이입니다. 첫번째 식의 우변은 마이너스 로그 가능도 평균의 상한이 되고 두번째 식의 우변은 로그 가능도의 하한이 됩니다. 여기서 양변의 차이를 \\(D_{KL}\\)가 채우게 되는 것으로\n\\[\n\\log p_\\theta(\\mathbf{x}_0) = \\mathbb{E}_{q(\\mathbf{x}_{1:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T})} \\right]  + D_{KL}\n\\]\n처럼 되는데 로그 가능도의 분해식에서 \\(q(\\mathbf{x}_{1:T})\\)를 \\(q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_{0})\\)로 제안하면\n\\[\n\\log p_\\theta(\\mathbf{x}_0) = \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}\\mid \\mathbf{x}_0)} \\right] + D_{KL}\n\\]\n이 됩니다. \\(D_{KL}\\)을 무시하고 양변에 \\(\\mathbb{E}_{q(\\mathbf{x}_0)}[\\cdot]\\)를 적용하면\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\log p_\\theta(\\mathbf{x}_0)\\right] \\ge \\mathbb{E}_{q(\\mathbf{x}_{0:T})}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}\\mid \\mathbf{x}_0)} \\right]\n\\]\n이 되고 모양이 식(3*)와 부등호 방향만 빼고 동일해졌습니다.\n이제 우변을 \\(\\theta\\)에 대해 최대화 시켜 값을 크게 하면 우변은 좌변의 하한이었으므로 좌변은 최대화로 커진 우변보다 항상 조금 더 커지게 되고 그 차이가 다시 0보다 큰 \\(D_{KL}\\)로 나타내게 됩니다.\n식(3*)대해서도 동일한 논리가 적용되는데 다른점은 하한이 상한으로 바뀌어 있기 때문에 우변을 최소화 하면 좌변은 항상 우변보다 더 작아지게 되는 것입니다. 때문에 식(3*)의 우변을 목적함수로 사용할 수 있게 됩니다.\n이제 유도된 부등식의 우변항을 \\(\\theta\\)에 대해서 최적화하는 것이 목적이라는 것을 잘 이해 했으므로 \\(\\theta\\)와 관련있는 항과 관련없는 항으로 분리하도록 합니다. 우변항 [ ] 안쪽 부분을 다음 과정으로 식을 전개할 수 있습니다.\n\\[\n\\begin{aligned}\n-\\log \\left( \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_{0})} \\right) &= - \\log \\left( \\frac{p(\\mathbf{x}_T) \\displaystyle\\prod_{t=1}^{T} p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{\\displaystyle\\prod_{t=1}^T q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right) \\\\[10pt]\n&=- \\log \\left( p(\\mathbf{x}_T) \\cdot \\frac{\\displaystyle\\prod_{t=1}^{T} p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{\\displaystyle\\prod_{t=1}^T q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right) \\\\[10pt]\n&= - \\log p(\\mathbf{x}_T) - \\log \\prod_{t=1}^T \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\\\[10pt]\n&= - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})}\n\\end{aligned}\n\\]\n전개된 부분을 다시 원식에 적용하면 식(3)이 완성됩니다. 최종적으로 가장 오른쪽에 있는 항을 목적함수 \\(L\\)로 정의합니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] \\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right] := L \\tag{3}\n\\]\n식(3)이 목적함수라면 네트워크를 학습시키는데 사용할 수 있다는 말인데 어떻게 사용할 수 있을까요? 식이 너무 복잡해서 뭐가 네트워크의 출력인지 뭐가 타겟인지 지도학습의 손실인지 비지도 학습의 손실인지 분명하게 잘 이해가 안됩니다.\n식(3)의 의미를 이해해보기 위해 간단한 실험을 해보도록 하겠습니다. 계산해야 하는 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)와 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)들은 잠재변수가 결합된 형태가 아니라 특정 시간 단계에서 한 단계 이전 또는 이후의 잠재변수에 대한 조건을 가지는 형태로 바뀌었습니다. 따라서 이전에 정의한 다음 두 식으로 부터 값을 계산할 수 있습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\]\n전술했듯이는 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t)\\), \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)는 네트워크가 출력하는 리버스 프로세스 한 단계에 대한 분포의 평균과 표준편차인데 여기서 실험을 간단히 하기 위해 \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) = \\beta_t \\mathbf{I}\\)로 가정하겠습니다.\n우선 적당한 \\(\\mathbf{x}_0\\)에 대해서 1단계에서 \\(T\\) 단계까지 노이즈가 더해진 이미지를 준비합니다. \\(T=20\\)으로 두도록 하겠습니다.\n\nx0.min(), x0.max()\n\n(-1.0, 1.0)\n\n\n\n%%time\n\nxts = [x0.reshape(-1)]\nxt = xts[0]\n\nT = 20\nL = xts[0].shape[0]\n\nfor t in range(1, T+1):\n    # t-1 단계에서 만들어진 이미지로 부터 분포 q(x_t|x_t-1)을 만든다.\n    q_xt_given_xtm1 = get_q_xt_given_xtm1(xt, t)\n\n    # 만들어진 분포에서 샘플링한다.\n    xt = q_xt_given_xtm1.rvs(size=1)\n    xts.append(xt)\n\nCPU times: user 6.4 s, sys: 1.97 s, total: 8.37 s\nWall time: 2.05 s\n\n\n그리고 현재 가진 네트워크가 잘 학습이 되어 있어서 \\(\\mathbf{x}_{20}\\)을 입력하면 \\(\\mathbf{x}_{19}\\)와 매우 유사한 이미지를 출력한다고 하겠습니다. 이를 그림으로 표현하면 다음처럼 표현할 수 있습니다. 그림을 통해 현재 상황을 직관적으로 이해하도록 합시다.\n\n# 샘플1에 대해서 x20을 입력했을 때 네트워크가 평균 x19를 출력했다고 가정\nx19_flt = xts[-2].copy()\n\n# x19에 약간의 노이즈를 더해 네트워크의 출력을 시뮬레이션합니다.\nx19_flt += 0.001 * np.random.randn(L)\nx19 = ((x19_flt - x19_flt.min()) / (x19_flt.max() - x19_flt.min())).clip(0,1)\n\nx20_flt = xts[-1].copy()\nx20 = ((x20_flt - x20_flt.min()) / (x20_flt.max() - x20_flt.min())).clip(0,1)\n\nfig, ax = plt.subplots(figsize=(10,3), nrows=1, ncols=3)\n\nax[0].imshow(x20.reshape(16,16,3))\nax[0].set_title(f\"$\\mathbf{{x}}_{{{T}}}$\")\nax[0].set_xticks([])\nax[0].set_yticks([])\n\nax[1].text(0.25, 0.47, 'network', fontsize=20)\ncon = ConnectionPatch(\n    xyA=(13, 7.5), coordsA=ax[0].transData,\n    xyB=(0.2, 0.5), coordsB=ax[1].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].set_frame_on(False)\ncon = ConnectionPatch(\n    xyA=(0.8, 0.5), coordsA=ax[1].transData,\n    xyB=(2, 7.5), coordsB=ax[2].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\n\n\nax[2].imshow(x19.reshape(16,16,3))\nax[2].set_title(f\"$\\mathbf{{\\mu}}_\\\\theta(\\mathbf{{x}}_{{{T}}},{T})$\")\nax[2].set_xticks([])\nax[2].set_yticks([])\n\nplt.show()\n\n\n\n\n이제 가상의 네트워크 출력을 평균으로 하는 정규분포를 정의합니다. 이것이 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)에 해당합니다. \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)는 위쪽 루프를 실행할 때 이미 마지막에 만들어졌습니다. 준비된 재료를 사용해 식(3)을 계산합니다.\n\\[\nL=\\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right]\n\\]\n이때 편의상 \\(\\sum_{t=1}^T\\) 부분을 모두 계산하지 않고 \\(T=20\\)에 해당하는 항 하나만 계산합니다. 또 $- p(_T) $ 부분도 \\(\\theta\\)와 무관한 부분이므로 계산하지 않습니다.\n\n# 네트워크의 출력을 평균으로 p(x_{t-1}| x_t)를 정의\np_xtm1_given_xt = multivariate_normal(mean=x19_flt, cov=beta(20)*np.eye(L))\n\n# 손실함수 값을 T=20에 대해서만 계산\n- (p_xtm1_given_xt.logpdf(x19_flt) - q_xt_given_xtm1.logpdf(x20_flt))\n\n-402.80940657034307\n\n\n샘플 \\(\\mathbf{x}_0\\) 하나와 연관된 특정 잠재변수 \\(\\mathbf{x}_{1:T}\\)에 대해서 가상으로 손실함수 값을 계산했습니다.\n이번에는 네트워크가 잘 학습되지 않아 \\(\\mathbf{x}_{20}\\)을 입력하면 엉뚱한 노이즈를 출력한다고 가정해보겠습니다. 다음 그림과 같은 상황입니다.\n\nfig, ax = plt.subplots(figsize=(10,3), nrows=1, ncols=3)\n\nax[0].imshow(x20.reshape(16,16,3))\nax[0].set_title(f\"$\\mathbf{{x}}_{{{T}}}$\")\nax[0].set_xticks([])\nax[0].set_yticks([])\n\nax[1].text(0.25, 0.47, 'network', fontsize=20)\ncon = ConnectionPatch(\n    xyA=(13, 7.5), coordsA=ax[0].transData,\n    xyB=(0.2, 0.5), coordsB=ax[1].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].set_frame_on(False)\ncon = ConnectionPatch(\n    xyA=(0.8, 0.5), coordsA=ax[1].transData,\n    xyB=(2, 7.5), coordsB=ax[2].transData,\n    arrowstyle=\"-|&gt;\", mutation_scale=20,\n    color='k', fc='red', lw=2)\nfig.add_artist(con)\n\nnoise = np.random.randn(L)\nnoise = ((noise - noise.min()) / (noise.max() - noise.min())).clip(0,1)\nax[2].imshow(noise.reshape(16,16,3))\nax[2].set_title(f\"$\\mathbf{{\\mu}}_\\\\theta(\\mathbf{{x}}_{{{T}}},{T})$\")\nax[2].set_xticks([])\nax[2].set_yticks([])\n\nplt.show()\n\n\n\n\n이전과 동일하게 이 노이즈를 평균으로 하는 정규분포를 정의의하고 같은 계산을 반복합니다. 모든 계산이 문제가 없다면 이번 값은 이전 값보다 매우 커야 합니다.\n\n# 네트워크의 출력을 평균으로 p(x_{t-1}| x_t)를 정의\np_xtm1_given_xt = multivariate_normal(mean=noise, cov=beta(20)*np.eye(L))\n\n# 손실함수 값을 T=20에 대해서만 계산\n- (p_xtm1_given_xt.logpdf(x19_flt) - q_xt_given_xtm1.logpdf(x20_flt))\n\n703501.3757760313\n\n\n예상처럼 상당히 큰 값이 얻어졌습니다. 이것으로 식(3)이 손실 함수로 작동할 수 있다는 것을 실험적으로 확인했습니다. 또 손실 함수를 계산하기 위해서 시작 이미지 \\(\\mathbf{x}_0\\)만 있으면 \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_T\\)와 같은 필요한 모든 요소들을 즉석에서 만들어 낼 수 있는 것도 알았습니다. 다시말해 별도의 타겟이 필요없다는 의미입니다.\n지금까지 유도된 손실함수를 직접 계산할 수 있음을 알아봤습니다. 이를 위해서 임의의 샘플 \\(\\mathbf{x}_0\\)를 선택하고 이로부터 노이즈를 확산시켜 \\(\\mathbf{x}_1, \\mathbf{x}_2, ... , \\mathbf{x}_T\\)를 얻습니다. 이 과정에서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)을 단계적으로 계산했습니다. 즉 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)를 계산하기 위해서 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t-2})\\)를 반드시 거쳐야 했습니다. 실험에서 \\(T=20\\)으로 두었음에도 이는 상당한 시간이 걸리는 것을 알 수 있습니다. \\(T=1000\\)정도로 노이즈 확산 단계를 설정한다면 이 과정을 계산하는 시간은 현실적이지 못합니다.\n임의 시간 단계 \\(t\\)에 대해서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)를 바로 정의할 수 있다면 임의 시간 단계의 노이즈가 확산된 샘플을 바로 얻을 수 있어 학습의 효율을 높일 수 있습니다. \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\) 정의와 reparameterization 트릭을 이용하면 가능합니다. 어떻게 하는지 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)에서 인덱스 \\(t\\)를 2로 두고 구체적인 예를 통해 알아봅시다.\n노이즈가 2단계 확산된 확률변수 \\(\\mathbf{x}_2\\)는 다음처점 정의된 정규 분포를 따릅니다.\n\\[\n\\mathbf{x}_2 \\sim q(\\mathbf{x}_2 \\mid \\mathbf{x}_1) := \\mathcal{N}(\\mathbf{x}_2; \\sqrt{1-\\beta_2}\\mathbf{x}_1, \\beta_2\\mathbf{I})\n\\]\n한편 확률변수 \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)에 대한 표준화는 다음과 같습니다.\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\n표준화를 이용하면 확률변수 \\(X\\)는 다음처럼 나타낼 수 있습니다.\n\\[\nX = \\mu + \\sigma Z, \\qquad Z \\sim \\mathcal{N}(0, 1)\n\\]\n이렇게 \\(X\\)가 따르는 분포로 부터 직접 샘플링하지 않고 표준 정규분포에서 샘플링한 샘플을 \\(X\\)로 변환하는 방법을 reparametization 트릭이라고 합니다. 이 트릭을 사용하면 샘플 \\(\\mathbf{x}_2 \\sim q(\\mathbf{x}_2 \\mid \\mathbf{x}_1)\\)는 다음처럼 쓸 수 있습니다.\n\\[\n\\mathbf{x}_2 = \\sqrt{\\alpha_2} \\mathbf{x}_1 + \\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1, \\qquad \\boldsymbol{\\epsilon}_1 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\n여기서 \\(\\alpha_t := 1- \\beta_t\\)로 두었습니다. 같은 방법으로 \\(\\mathbf{x}_1 \\sim q(\\mathbf{x}_1 \\mid \\mathbf{x}_0)\\) 도 다음처럼 쓸 수 있습니다.\n\\[\n\\mathbf{x}_1 = \\sqrt{\\alpha_1} \\mathbf{x}_0 + \\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0, \\qquad \\boldsymbol{\\epsilon}_0 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\n이 두 관계를 이용하면 다음처럼 \\(\\mathbf{x}_2\\)를 정리할 수 있습니다.\n\\[\n\\begin{align}\n\\mathbf{x}_2 &= \\sqrt{\\alpha_2} \\mathbf{x}_1 + \\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1 \\\\[10pt]\n&= \\sqrt{\\alpha_2} \\left( \\sqrt{\\alpha_1} \\mathbf{x}_0 + \\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0 \\right) + \\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1 \\\\[10pt]\n&= \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0 + \\underbrace{\\underbrace{\\sqrt{\\alpha_2} \\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0}_{\\sim \\mathcal{N}(\\mathbf{0}, \\alpha_2(1-\\alpha_1)\\mathbf{I})} + \\underbrace{\\sqrt{1-\\alpha_2} \\boldsymbol{\\epsilon}_1}_{\\sim \\mathcal{N}(\\mathbf{0}, (1-\\alpha_2)\\mathbf{I})}}_{\\sim \\mathcal{N}(\\mathbf{0}, (\\alpha_2(1-\\alpha_1)+(1-\\alpha_2))\\mathbf{I})} \\quad (**) \\\\[5pt]\n&= \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0 + \\sqrt{\\alpha_2 (1-\\alpha_1) + (1-\\alpha_2)} \\boldsymbol{\\epsilon}^*_0 \\quad(*) \\\\[10pt]\n&= \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0 + \\sqrt{1- \\alpha_1 \\alpha_2} \\boldsymbol{\\epsilon}^*_0\n\\end{align}\n\\]\n세 번째 식 (**)에서 \\(\\sqrt{\\alpha_2}\\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0\\)는 reparameterization 트릭에 의해 \\(\\mathcal{N}(\\mathbf{0}, \\alpha_2(1-\\alpha_1)\\mathbf{I})\\)에서 샘플링 된 샘플이고 \\(\\sqrt{1-\\alpha_2}\\boldsymbol{\\epsilon}_1\\)은 \\(\\mathcal{N}(\\mathbf{0}, (1-\\alpha_2)\\mathbf{I})\\)에서 샘플링된 샘플입니다. 두 분포에서 샘플링된 샘플 두개를 더하고 있습니다. 서로 독립인 두 정규 분포를 따르는 확률변수를 합한 확률 변수가 따르는 분포는 각 정규분포의 평균과 분산이 더해진 정규분포입니다. 즉 다음이 성립합니다.\n\\[\n\\begin{aligned}\nX &\\sim \\mathcal{N}(\\mu_X, \\sigma^2_X) \\\\[5pt]\nY &\\sim \\mathcal{N}(\\mu_Y, \\sigma^2_Y) \\\\[5pt]\nX+Y &\\sim \\mathcal{N}(\\mu_X+\\mu_Y, \\sigma^2_X+\\sigma^2_Y)\n\\end{aligned}\n\\]\n이에 대한 증명은 sum of two independent Gaussian random variables를 참고하시기 바랍니다.\n이를 이용하면 \\(\\sqrt{\\alpha_2}\\sqrt{1-\\alpha_1} \\boldsymbol{\\epsilon}_0 + \\sqrt{1-\\alpha_2}\\boldsymbol{\\epsilon}_1\\)는 \\(\\mathcal{N}(\\mathbf{0}, (\\alpha_2(1-\\alpha_1)+(1-\\alpha_2))\\mathbf{I})\\)를 따르는 샘플이란 것을 알 수 있습니다. 따라서 새로운 표준 정규분포로 부터 샘플링된 샘플 \\(\\boldsymbol{\\epsilon}^*\\)를 사용해서 네 번째 식 (*)처럼 표현 가능합니다.\n정리하면 \\(\\mathbf{x}_2\\)는 평균이 \\(\\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0\\)고 표준편차가 \\(\\sqrt{1- \\alpha_1 \\alpha_2}\\)인 정규분포를 따르게 됩니다.\n이를 다시 쓰면\n\\[\n\\mathbf{x}_2 \\sim  \\mathcal{N}(\\mathbf{x}_2 ; \\sqrt{\\alpha_1 \\alpha_2} \\mathbf{x}_0, (1- \\alpha_1 \\alpha_2)\\mathbf{I}) = q(\\mathbf{x}_2 \\mid \\mathbf{x}_0)\n\\]\n이를 \\(\\mathbf{x}_t\\)와 \\(\\mathbf{x}_0\\) 사이의 일반적인 관계로 확장하면 다음을 얻을 수 있습니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, (1- \\bar{\\alpha}_t)\\mathbf{I}), \\qquad \\alpha_t := 1-\\beta_t, \\quad \\bar{\\alpha}_t := \\prod_{s=1}^t \\alpha_s \\tag{4}\n\\]\n유도된 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)로 임의 시간 단계에서 노이즈가 확신된 샘플을 얻어보도록 합시다. 이전에 beta() 함수를 만들어 사용했습니다. 하지만 \\(\\beta_t\\)를 학습하지 않고 일정한 시간 단계에서 상수로 설정할 것이기 때문에 전체 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar{\\alpha}_t\\)를 한꺼번에 만들어 두는 것이 효과적입니다. 노이즈를 확산시키는 스케쥴은 일정하게 고정한다는 의미입니다. 다시 한번 논문에서 밝힌 다음 조건을 이용하여 \\(T\\) 길이를 가지는 beta, alpha, alpha_bar를 계산합니다.\n\n\\(T=1000\\), \\(\\beta_1 = 10^{-4}\\), \\(\\beta_T = 0.02\\)\n\n\nbeta_1 = 1e-4\nbeta_T = 0.02\nT = 1000\n\nbeta = np.concatenate( (np.array([0.]), np.linspace(beta_1, beta_T, T)) )\nalpha = 1 - beta\nalpha_bar = np.exp( np.cumsum(np.log(alpha)) )\n\nbeta[1], beta[-1], alpha[1], alpha[-1], alpha_bar[1], alpha_bar[-1]\n\n(0.0001, 0.02, 0.9999, 0.98, 0.9999, 4.035829765375694e-05)\n\n\n만들어진 alpha_bar를 이용해서 \\(\\sqrt{\\bar{\\alpha}_t}\\), \\(\\sqrt{1-\\bar{\\alpha}_t}\\)가 \\(t\\)에 대해서 어떻게 변하는지 확인해보겠습니다.\n\na = np.array([ [np.sqrt(alpha_bar[t]), np.sqrt(1-alpha_bar[t])] for t in range(1, T+1)])\n\nplt.plot(a[:,0], label=r\"$\\sqrt{\\bar{\\alpha}_t}$\")\nplt.plot(a[:,1], label=r\"$\\sqrt{1-\\bar{\\alpha}_t}$\")\n\nplt.legend()\nplt.show()\n\n\n\n\n시간 단계가 흐를 수록 \\(\\sqrt{\\bar{\\alpha}_t}\\)는 0이 되고, \\(\\sqrt{1-\\bar{\\alpha}_t}\\)는 1이 됩니다. 따라서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, (1- \\bar{\\alpha}_t)\\mathbf{I})\\)는 노이즈가 확산 될 수록 점점 표준 정규분포가 되어 간다는 것을 알 수 있습니다.\n샘플 \\(\\mathbf{x}_0\\)에 \\(\\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\)로 \\(t\\) 단계 노이즈를 확신 시키는 함수를 정의합니다. 이 때 샘플 x의 모양은 채널이 이미지 크기보다 먼저오는 (N, C, H, W)로 입력 받도록 하겠습니다.\n\n# 숫자값이 들어있는 1차원 어레이에 대한 인덱싱 결과가 4차원 어레이가 되도록 인덱싱 합니다.\nalpha_bar[[1], None, None, None].shape\n\n(1, 1, 1, 1)\n\n\n\ndef perturb_x(x, t, eps):\n    # x: (N, C, H, W)\n\n    # x와 alpha_bar[t]가 브로드캐스팅되도록 차원을 늘린수 곱하고 더함\n    # alpha_bar[t, None, None, None]: (N,1,1,1)\n    # x, eps: (N,C,H,W)\n    # 연산결과: (N,C,H,W)\n    return np.sqrt(alpha_bar[t, None, None, None]) * x \\\n            + np.sqrt(1 - alpha_bar[t, None, None, None]) * eps\n\nsprites로부터 적당히 샘플 다섯개를 가져오고 임의 시간 단계 t를 만든 다음 샘플 이미지에 적용해보겠습니다.\n\nnp.random.seed(102)\nidx = np.random.randint(0, sprites.shape[0], 5)\n\nx = (sprites[idx].transpose(0, 3, 1, 2)  / 255) * 2 - 1\n\nt = np.random.randint(1, T+1, (x.shape[0],))\nnoise = np.random.randn(*x.shape)\nxt = perturb_x(x, t, noise)\nxt_prev = perturb_x(x, t-1, noise)\n\n\nfig, ax = plt.subplots(figsize=(10,7), nrows=3, ncols=5)\n\nfor i in range(5):\n    x_i = ((x[i] - x[i].min()) / (x[i].max() - x[i].min())).clip(0,1)\n    ax[0][i].imshow(x_i.transpose(1,2,0))\n    ax[0][i].set_title(f\"t={0}\")\n    ax[0][i].set_xticks([])\n    ax[0][i].set_yticks([])\n\n    x_i = ((xt_prev[i] - xt_prev[i].min()) / (xt_prev[i].max() - xt_prev[i].min())).clip(0,1)\n    ax[1][i].imshow(x_i.transpose(1,2,0))\n    ax[1][i].set_title(f\"t={t[i]-1}\")\n    ax[1][i].set_xticks([])\n    ax[1][i].set_yticks([])\n\n    x_i = ((xt[i] - xt[i].min()) / (xt[i].max() - xt[i].min())).clip(0,1)\n    ax[2][i].imshow(x_i.transpose(1,2,0))\n    ax[2][i].set_title(f\"t={t[i]}\")\n    ax[2][i].set_xticks([])\n    ax[2][i].set_yticks([])\n\n    if i == 0:\n        ax[0][i].set_ylabel(r\"$\\mathbf{x}_0$\", size=15)\n        ax[1][i].set_ylabel(f\"$\\mathbf{{x}}_{{t-1}}$\", size=15)\n        ax[2][i].set_ylabel(f\"$\\mathbf{{x}}_{{t}}$\", size=15)\n\nplt.show()\n\n\n\n\n이미지 위에 시간 단계를 표시했습니다. 시간 단계를 건너뛰면서 빠르게 노이즈가 추가되는것을 확인할 수 있습니다. 샘플링이 훨씬 빠르게 가능해졌으므로 이 샘플들을 이용해서 효율적으로 손실함수에 나타나는 \\(p_\\theta(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t)\\), \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)를 계산할 수 있게 되었습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} , \\beta_t \\mathbf{I})\n\\]\n지금까지 식(3)에 대해서 자세히 알아봤습니다. 이론적인 부분 뿐만 아니라 실제 학습에 사용할 수 있을 정도로 코드 수준에서 값을 직접 계산까지 해봤습니다.\nDDPM 논문을 보면 이렇게 유도된 손실함수를 바로 사용하지 않고 한번 더 형태를 변형하여 손실함수를 좀 더 간단하게 만들어 사용합니다. DDPM 식(3)에서 식(5)를 유도하는 과정입니다. 논문 부록에 이 과정이 아주 간략하게 소개 되어 있으므로 여기서 자세히 알아보도록 하겠습니다. 다시 식(3)을 보겠습니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\log p_{\\theta}(\\mathbf{x}_0) \\right] \\le \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right] = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right] := L \\tag{3}\n\\]\n우선 \\(\\sum\\) 이하 로그 분수 부분을 변형하기 위해 분모를 봅시다. 분모에 베이즈 정리를 적용하면 다음처럼 쓸 수 있습니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})=\\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t) q(\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1})}\n\\]\n앞서 가정한 마르코프 성질에 의해 좌변은 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})=q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)\\) 이며 우변의 다른 항들도 마찬가지 성질을 보이므로 다음처럼 써도 무방합니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})= q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)=\\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}\n\\]\n이 결과를 \\(\\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})}\\)에 대입하고 정리합니다.\n\\[\n\\begin{aligned}\n\\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} &=\n\\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{\\dfrac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}}\\\\[10pt]\n&= \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)  \\dfrac{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}} \\\\[10pt]\n&= \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}\n\\end{aligned}\n\\]\n위 결과를 식(3)에 대입하고 정리합니다.\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ -\\log \\left( \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)}\\right) \\right]\\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=1}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\log  \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\quad (20)\n\\end{aligned}\n\\]\n여기까지가 논문 부록에 제시된 식(20)까지 입니다. 로그 곱셈을 덧셈으로 바꾸고 정리합니다.\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T \\left\\{ \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} + \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)} \\right\\} - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right]\n\\end{aligned}\n\\]\n세 번째 항에 합산 기호를 전개하면 대부분 항이 삭제되고 다음처럼 됩니다.\n\\[\n\\begin{aligned}\n-\\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)} = - &( \\log q(\\mathbf{x}_1\\mid\\mathbf{x}_0) - \\log q(\\mathbf{x}_2\\mid\\mathbf{x}_0) ) \\\\\n- &( \\log q(\\mathbf{x}_2\\mid\\mathbf{x}_0) - \\log q(\\mathbf{x}_3\\mid\\mathbf{x}_0) ) - \\cdots \\\\[10pt]\n- &( \\log q(\\mathbf{x}_{T-1}\\mid\\mathbf{x}_0) - \\log q(\\mathbf{x}_T\\mid\\mathbf{x}_0) ) \\\\[10pt]\n=  - &\\log q(\\mathbf{x}_1 \\mid \\mathbf{x}_0 ) + \\log q(\\mathbf{x}_T \\mid \\mathbf{x}_0)\n\\end{aligned}\n\\]\n이를 원 식에 대입하고 정리하면 논문의 식(21)이 완성됩니다.\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t\\mid \\mathbf{x}_0)}  - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log q(\\mathbf{x}_1 \\mid \\mathbf{x}_0 ) + \\log q(\\mathbf{x}_T \\mid \\mathbf{x}_0) - \\log \\frac{p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}{q(\\mathbf{x}_1\\mid\\mathbf{x}_0)} \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log p(\\mathbf{x}_T) + \\log q(\\mathbf{x}_T \\mid \\mathbf{x}_0) - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log q(\\mathbf{x}_1 \\mid \\mathbf{x}_0 )  - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) + \\log q(\\mathbf{x}_1\\mid\\mathbf{x}_0) \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad(21)\n\\end{aligned}\n\\]\n식(21)에서 기댓값 연산을 쪼개고 나서 각 항에 \\(D_{KL}\\)을 적용합니다.\n\\[\n\\begin{aligned}\nL =& \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[  - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad(21) \\\\[10pt]\n=& \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)}  \\right] + \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)} \\right] + \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\\\[10pt]\n=& \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} \\right] \\right] \\quad (\\ast) \\\\[10pt]\n+ &\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[  - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\quad (\\ast\\ast)\\\\[10pt]\n+ &\\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad (\\ast\\ast\\ast)\n\\end{aligned}\n\\]\n이렇게 분리된 기댓값 연산 (*), (**), (***)를 \\(D_{KL}\\) 형태로 정리하면 식(5)가 완성됩니다. 여기서 \\(D_{KL}\\)의 정의를 다시 확인하고 이 꼴로 위 식을 변형해야 합니다. \\(D_{KL}\\)은 연속 확률 변수 \\(P \\sim p(x)\\), \\(Q \\sim q(x)\\)에 대해서 다음과 같이 정의 됩니다.\n\\[\nD_{KL}(Q || P) = - \\int  q(x) \\log \\frac{p(x)}{q(x)} dx\n\\]\n\\(D_{KL}\\)의 정의와 (*)를 비교해보면 (*)에서 적분 변수는 \\(\\mathbf{x}_T\\)라는 것을 알 수 있습니다. 나머지 변수들을 주변화 시킬 수 있습니다. 간단한 스칼라 함수에 대한 주변화 과정을 다음에 정리했습니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(x_1, x_2, x_3)} [ f(x_1, x_2) ] &= \\int_{x_3} \\int_{x_2} \\int_{x_1} q(x_1, x_2, x_3) f(x_1, x_2) dx_1 dx_2 dx_3 \\\\[10pt]\n&= \\int_{x_2} \\int_{x_1} \\int_{x_3} q(x_1, x_2, x_3) f(x_1, x_2) dx_3 dx_1 dx_2 \\quad  \\because \\text{Fubini's theorem} \\\\[10pt]\n&=\\int_{x_2} \\int_{x_1} f(x_1, x_2)  \\int_{x_3} q(x_1, x_2, x_3) dx_3 dx_1 dx_2 \\\\[10pt]\n&= \\int_{x_2} \\int_{x_1} f(x_1, x_2)  dx_1 dx_2 \\quad \\because \\text{marginalization}\\\\[10pt]\n&= \\mathbb{E}_{q(x_1,x_2)} [f(x_1, x_2)]\n\\end{aligned}\n\\]\n적분 변수 \\(x_3\\)이 사라졌습니다. 이처럼 (*)에서 \\(\\mathbf{x}_1\\), … \\(\\mathbf{x}_{T-1}\\)을 모두 주변화 해서 없애면 다음처럼 정리할 수 있습니다.\n\\[\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} \\right] \\right] = \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{T}\\mid\\mathbf{x}_0)} \\left[ - \\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T \\mid \\mathbf{x}_0)} \\right] \\right] = \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ D_{KL}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) || p(\\mathbf{x}_T)) \\right] \\quad (*)\n\\]\n(**) 부분도 마찬가지로 주변화 과정을 거치고 다음처럼 \\(D_{KL}\\) 형태로 바꿀 수 있습니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[  - \\sum_{t=2}^T  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0)} \\left[   \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ - \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t-1},\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[    \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[  - \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[  \\mathbb{E}_{q(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t, \\mathbf{x}_0)}\\left[   \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[   - \\mathbb{E}_{q(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t, \\mathbf{x}_0)}\\left[  \\log \\frac{p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0)}  \\right] \\right] \\right] \\\\[10pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_{t}\\mid\\mathbf{x}_0)} \\left[     D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))  \\right] \\right] \\\\[10pt]\n&=\\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_0, \\mathbf{x}_t)} \\left[      D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))  \\right] \\quad (**)\n\\end{aligned}\n\\]\n(***) 부분도 간단히 주변화 해서 다음처럼 정리한 다음\n\\[\n\\begin{aligned}\n\\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[   - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] &= \\mathbb{E}_{q(\\mathbf{x}_{0})} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} [- \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) ] \\right] \\\\[5pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0})} \\left[ \\mathbb{E}_{q(\\mathbf{x}_{1} \\mid \\mathbf{x}_0)} [- \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) ] \\right] \\\\[5pt]\n&= \\mathbb{E}_{q(\\mathbf{x}_{0}, \\mathbf{x}_1)} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right] \\quad (***)\n\\end{aligned}\n\\]\n유도된 세 부분을 함께 쓰면 다음과 같습니다.\n\\[\nL = \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[ D_{KL}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) || p(\\mathbf{x}_T)) \\right] + \\sum_{t=2}^T \\mathbb{E}_{q(\\mathbf{x}_0, \\mathbf{x}_t)} \\left[      D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))  \\right] + \\mathbb{E}_{q(\\mathbf{x}_{0}, \\mathbf{x}_1)} \\left[ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1) \\right]\n\\]\n이제 각 항에 주변화해서 없앤 적분 변수를 다시 넣어서 \\(\\mathbf{x}_{0:T}\\)에 대한 적분으로 바꾸면 다음처럼 정리 됩니다.\n\\[\nL = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ \\underbrace{D_{KL}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) || p(\\mathbf{x}_T))}_{L_T} +   \\sum_{t=2}^T    \\underbrace{D_{KL}(q(\\mathbf{x}_{t-1}\\mid \\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))}_{L_{t-1}}   \\underbrace{ - \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)}_{L_0} \\right] \\tag{5}\n\\]\n이렇게 식(5)를 완전히 유도 했습니다.\n식(5)는 손실 함수 식(3)이 \\(L_T\\), \\(L_{t-1}\\), \\(L_0\\) 세 부분으로 변형된 형태이며 앞 두 항은 \\(D_{KL}\\)이고 마지막 항 \\(L_0\\)은 리버스 프로세스에서 마지막 과정에 해당하는 마이너스 로그 가능도 입니다. 이 세 항을 모두 줄여야 하는 사실은 변함이 없고 식(3)에서 형태만 바뀐 상태입니다.\n각 항의 의미를 살펴보도록 합시다.\n\n\n\\(\\mathbf{x}_0\\)로부터 완전히 노이즈 상태가 된 \\(\\mathbf{x}_T\\)의 분포가 표준 정규분포 \\(p(\\mathbf{x}_T)\\)와 얼마나 비슷한지를 나타냅니다. 이 항은 파라미터 \\(\\theta\\)에 독립적인 항이므로 무시할 수 있습니다.\n\n\n\n\n복잡한 이야기를 다 떠나서 우리가 궁극적으로 알고 싶은 것은 \\(q(\\mathbf{x}_0 \\mid \\mathbf{x}_T)\\) 또는 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)입니다. 전자는 모든 시간 단계를 건더뛰고 완전 노이즈와 이미지를 바로 연결하는 조건부 분포이므로 이를 직접 알기는 힘듭니다. 대신 각 시간 단계에 대한 리버스 프로세스를 알려주는 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)를 알아도 됩니다. \\(q(\\mathbf{x} \\mid \\mathbf{x}_{t-1})\\)를 알고 있기 때문에 여기에 베이즈 정리를 이용해서 이론적으로 유도 할 수 있지만 실제 계산을 위해서 모든 시간 단계에 대한 \\(q(\\mathbf{x}_t)\\)를 알아야 합니다.\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})q(\\mathbf{x}_{t-1})}{q(\\mathbf{x}_t)}\n\\]\n\\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)는 알 수 없지만 식(5)에 나타난 포워드 프로세스의 posterior \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 계산할 수 있습니다. 이를 위해 먼저 다음 베이즈 정리를 이용합니다.\n\\[\n\\begin{aligned}\np(x \\mid y, z) &= \\frac{p(x, y, z)}{p(y,z)} \\\\[5pt]\n&= \\frac{p(y \\mid x, z)p(x,z)}{p(y,z)} \\\\[5pt]\n&= \\frac{p(y \\mid x, z)p(x \\mid z)p(z)}{p(y \\mid z)p(z)} \\\\[5pt]\n&= \\frac{p(y \\mid x, z)p(x \\mid z)}{p(y \\mid z)}\n\\end{aligned}\n\\]\n위 베이즈 정리를 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)에 적용합니다.\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)=\\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)} = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\]\n두 번째 등호는 마르코프 가정을 사용했습니다. 이제 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)가 계산할 수 있는 항 3개로 변환되었습니다. \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)\\), \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_0)\\)는 식(4)에 의해 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\)는 포워드 프로세스의 정의에 의해 계산할 수 있습니다. 이 계산을 마치면 DDPM의 식(6), (7)이 유도됩니다. 이 과정은 어렵진 않지만 상당히 지저분하고 지루한 과정이므로 다음 제시된 계산 과정은 스킵하여도 무방합니다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &=\\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)} \\\\[10pt]\n&= \\frac{\\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) \\,\\, \\mathcal{N}(\\mathbf{x}_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0, (1-\\bar{\\alpha}_{t-1})\\mathbf{I})}{\\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_{t})\\mathbf{I})} \\quad (*)\n\\end{aligned}\n\\]\n한편 \\(\\mathbf{x} \\in \\mathbb{R}^D\\)인 벡터 변수에 대한 정규분포는 다음과 같고\n\\[\n\\mathcal{N}(\\mathbf{x}, \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(\\sqrt{2\\pi})^D \\sqrt{|\\Sigma|}} \\exp \\left\\{-\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}) \\right\\}\n\\]\n지금 다루고 있는 $(t ; {t-1}, _t ) $처럼 공분산 행렬이 \\(\\Sigma = \\beta \\, \\mathbf{I}\\)인 경우 다음처럼 간소화 됩니다.\n\\[\n\\mathcal{N}(\\mathbf{x}, \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(\\sqrt{2\\pi})^D \\sqrt{\\beta^D}} \\exp\\left\\{-\\frac{1}{2\\beta} (\\mathbf{x}-\\boldsymbol{\\mu})^T (\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\n\\]\n그리고 벡터의 행렬곱은 다음처럼 전개할 수 있으므로\n\\[\n\\begin{aligned}\n(\\mathbf{x}-\\boldsymbol{\\mu})^T (\\mathbf{x}-\\boldsymbol{\\mu}) &= (\\mathbf{x}^T-\\boldsymbol{\\mu}^T)(\\mathbf{x}-\\boldsymbol{\\mu}) \\\\[5pt]\n&= \\mathbf{x}^T \\mathbf{x} - \\mathbf{x}^T \\boldsymbol{\\mu} - \\boldsymbol{\\mu}^T \\mathbf{x} + \\boldsymbol{\\mu}^T\\boldsymbol{\\mu} \\\\[5pt]\n&= \\lVert\\mathbf{x}\\rVert^2_2 - 2 \\mathbf{x}^T \\boldsymbol{\\mu} + \\lVert \\boldsymbol{\\mu} \\rVert^2_2\n\\end{aligned}\n\\]\n이를 이용해 (*)를 전개합니다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= \\frac{\\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) \\,\\, \\mathcal{N}(\\mathbf{x}_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0, (1-\\bar{\\alpha}_{t-1})\\mathbf{I})}{\\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_{t})\\mathbf{I})} \\quad (*) \\\\[10pt]\n&= \\frac{ \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\alpha_t)^D}} \\exp\\left\\{-\\frac{1}{2(1-\\alpha_t)} (\\mathbf{x}_t - \\sqrt{\\alpha_t}\\mathbf{x}_{t-1})^T (\\mathbf{x}_t - \\sqrt{\\alpha_t}\\mathbf{x}_{t-1}) \\right\\} \\times \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t-1})^D}} \\exp\\left\\{-\\frac{1}{2(1-\\bar{\\alpha}_{t-1})} (\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0})^T (\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0}) \\right\\}  }{  \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}} \\exp\\left\\{-\\frac{1}{2(1-\\bar{\\alpha}_{t})} (\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0})^T (\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}) \\right\\}  } \\\\[10pt]\n&=\\frac{ \\frac{1}{(2\\pi)^D\\sqrt{(1-\\alpha_t)^D (1-\\bar{\\alpha}_{t-1})^D}} \\exp\\left\\{ -\\left(  \\overbrace{\\frac{\\lVert \\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1} \\rVert^2_2}{2(1-\\alpha_t)}}^{A} + \\overbrace{\\frac{\\lVert \\mathbf{x}_{t-1} - \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_{0} \\rVert^2_2}{2(1-\\bar{\\alpha}_{t-1})} }^{B} \\right) \\right\\}  }{ \\frac{1}{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}} \\exp \\left\\{ - \\frac{1}{2(1-\\bar{\\alpha}_{t})} \\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 \\right\\} } \\\\[10pt]\n&= \\underbrace{\\frac{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}}{(2\\pi)^D\\sqrt{(1-\\alpha_t)^D (1-\\bar{\\alpha}_{t-1})^D}}}_{C_1} \\exp \\left\\{ -(A+B) - \\left(- \\frac{1}{2(1-\\bar{\\alpha}_{t})} \\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 \\right)  \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{ - \\left( \\frac{\\lVert \\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1} \\rVert^2_2}{2(1-\\alpha_t)} +  \\frac{\\lVert \\mathbf{x}_{t-1} - \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_{0} \\rVert^2_2}{2(1-\\bar{\\alpha}_{t-1})} - \\frac{\\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 }{2(1-\\bar{\\alpha}_{t})} \\right) \\right\\}\n\\end{aligned}\n\\]\n마지막에 이 상수 \\(C_1\\)이 어떤 정규분포의 정규화 상수와 일치하게 되는 것을 확인할 있습니다.\n이제 \\(||\\cdot||^2_2\\) 부분을 전개하고\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= C_1 \\exp \\left\\{ - \\left( \\frac{\\lVert \\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1} \\rVert^2_2}{2(1-\\alpha_t)} +  \\frac{\\lVert \\mathbf{x}_{t-1} - \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_{0} \\rVert^2_2}{2(1-\\bar{\\alpha}_{t-1})} - \\frac{ \\lVert \\mathbf{x}_{t} - \\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_{0} \\rVert^2_2 }{2(1-\\bar{\\alpha}_{t})} \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\\\\n& \\left\\{ - \\frac{1}{2} \\left( \\frac{ \\lVert\\mathbf{x}_t \\rVert^2_2 - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert\\mathbf{x}_{t-1} \\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert\\mathbf{x}_{t-1} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{t-1}^T \\mathbf{x}_{0} + \\bar\\alpha_{t-1} \\lVert \\mathbf{x}_{0} \\rVert^2_2  }{(1-\\bar{\\alpha}_{t-1})} - \\frac{ \\lVert \\mathbf{x}_{t} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{t}^T \\mathbf{x}_{0} + \\bar\\alpha_{t} \\lVert \\mathbf{x}_{0} \\rVert^2_2  }{(1-\\bar{\\alpha}_{t})} \\right) \\right\\}\n\\end{aligned}\n\\]\n\\((\\cdot)\\) 안을 \\(\\mathbf{x}_{t-1}\\), \\(\\mathbf{x}_0\\), \\(\\mathbf{x}_t\\) 로 나눠서 정리합니다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= C_1 \\exp \\\\\n& \\left\\{ - \\frac{1}{2} \\left( \\frac{ - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert\\mathbf{x}_{t-1}\\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert\\mathbf{x}_{t-1} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{t-1}^T \\mathbf{x}_{0} }{(1-\\bar{\\alpha}_{t-1})} + \\overbrace{\\frac{ \\lVert\\mathbf{x}_t\\rVert^2_2}{1-\\alpha_t} + \\frac{\\bar\\alpha_{t-1} \\lVert \\mathbf{x}_0 \\rVert^2_2}{1-\\bar\\alpha_{t-1}}- \\frac{ \\lVert \\mathbf{x}_{t} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{t}^T \\mathbf{x}_{0} + \\bar\\alpha_{t} \\lVert \\mathbf{x}_{0} \\rVert^2_2  }{(1-\\bar{\\alpha}_{t})}}^{C_2(\\mathbf{x}_0, \\mathbf{x}_t)} \\right) \\right\\}\n\\end{aligned}\n\\]\n\\(C_2(\\mathbf{x}_0, \\mathbf{x}_t)\\) 부분은 \\(\\mathbf{x}_{t-1}\\)과 아무 상관이 없는 항입니다. 지금 계산하고 있는 분포 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)의 변수는 \\(\\mathbf{x}_{t-1}\\)임을 다시 한번 상기합시다.\n\\[\n\\begin{aligned}\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) &= C_1 \\exp \\left\\{ - \\frac{1}{2} \\left( \\frac{ - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert\\mathbf{x}_{t-1}\\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert\\mathbf{x}_{t-1}\\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{t-1}^T \\mathbf{x}_{0} }{(1-\\bar{\\alpha}_{t-1})} + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{ - \\frac{1}{2} \\left( \\frac{ - 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1} + \\alpha_t \\lVert \\mathbf{x}_{t-1} \\rVert^2_2 }{(1-\\alpha_t)} +  \\frac{ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0}^T \\mathbf{x}_{t-1} }{(1-\\bar{\\alpha}_{t-1})} + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{ -\\frac{1}{2} \\left( \\frac{\\alpha_t \\lVert \\mathbf{x}_{t-1} \\rVert^2_2}{1-\\alpha_t} + \\frac{ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2}{1-\\bar\\alpha_{t-1}} + \\frac{- 2 \\sqrt{\\alpha_t}\\mathbf{x}_t^T \\mathbf{x}_{t-1}}{1-\\alpha_t} + \\frac{- 2 \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0}^T \\mathbf{x}_{t-1}}{1-\\bar\\alpha_{t-1}} + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2}\\left( \\left( \\frac{\\alpha_t}{1-\\alpha_t} + \\frac{1}{1-\\bar\\alpha_{t-1}} \\right) \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}} \\right) \\mathbf{x}_{t-1}  + C_2(\\mathbf{x}_0, \\mathbf{x}_t)  \\right)  \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2}\\left( \\left( \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} \\right) \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}} \\right) \\mathbf{x}_{t-1}  + C_2(\\mathbf{x}_0, \\mathbf{x}_t) \\right) \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2} \\left( \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} \\right) \\left[ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}}}{\\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}} \\right) \\mathbf{x}_{t-1} + \\underbrace{\\frac{C_2(\\mathbf{x}_0, \\mathbf{x}_t) }{ \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} }}_{C_3} \\right] \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2} \\left( \\frac{1-\\bar\\alpha_{t}}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} \\right) \\left[ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\left( \\frac{\\left(\\frac{\\sqrt{\\alpha_t}\\mathbf{x}_t^T}{1-\\alpha_t} + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\mathbf{x}_0^T}{1-\\bar\\alpha_{t-1}}\\right)(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_{t}} \\right) \\mathbf{x}_{t-1} + C_3 \\right] \\right\\} \\\\[10pt]\n&= C_1 \\exp \\left\\{  -\\frac{1}{2} \\left( \\frac{1}{ \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_{t}}   } \\right) \\left[ \\lVert \\mathbf{x}_{t-1} \\rVert^2_2  -2 \\underbrace{\\left( \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1}) \\mathbf{x}_t^T + \\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\mathbf{x}_0^T}{1-\\bar\\alpha_{t}} \\right)}_{\\mathbf{v}^T}\\mathbf{x}_{t-1} + C_3 \\right] \\right\\}\n\\end{aligned}\n\\]\n두 번째 등호는 \\(\\mathbf{x}_{t-1}^T\\mathbf{x}_0\\)를 \\(\\mathbf{x}_0^T \\mathbf{x}_{t-1}\\)로 바꿔 적은 것입니다. 전개한 결과를 표준적인 정규분포 모양과 비교해보면\n\\[\n\\mathcal{N}(\\mathbf{x}, \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(\\sqrt{2\\pi})^D \\sqrt{\\beta^D}} \\exp\\left\\{-\\frac{1}{2\\beta} (\\mathbf{x}-\\boldsymbol{\\mu})^T (\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\n\\]\n다음 두 등식이 성립하면 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 정규 분포라는 것을 알 수 있습니다.\n\\[\nC_1 = \\frac{1}{ (\\sqrt{2\\pi})^D \\sqrt{\\left( \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\right)^D}}, \\quad C_3 = \\mathbf{v}^T \\mathbf{v}\n\\]\n먼저 \\(C_1\\)부터 정리해보겠습니다.\n\\[\n\\begin{aligned}\nC_1 &= \\frac{\\sqrt{(2\\pi)^D}\\sqrt{(1-\\bar{\\alpha}_{t})^D}}{(2\\pi)^D\\sqrt{(1-\\alpha_t)^D (1-\\bar{\\alpha}_{t-1})^D}} \\\\[5pt]\n&= \\frac{\\sqrt{(2\\pi)^D}}{(2\\pi)^D} \\frac{\\left((1-\\bar\\alpha_t)^{\\frac{1}{2}}\\right)^D}{\\left(((1-\\alpha_t)(1-\\bar\\alpha_{t-1}))^{\\frac{1}{2}}\\right)^D} \\\\[5pt]\n&= \\frac{\\left((2\\pi)^D\\right)^{\\frac{1}{2}}}{(2\\pi)^D} \\frac{1}{\\dfrac{\\left((1-\\alpha_t)(1-\\bar\\alpha_{t-1})\\right)^{\\frac{D}{2}}}{(1-\\bar\\alpha_t)^{\\frac{D}{2}}}} \\\\[5pt]\n&= \\left(\\frac{(2\\pi)^{\\frac{1}{2}}}{2\\pi}\\right)^D \\frac{1}{\\sqrt{\\dfrac{(1-\\alpha_t)^D(1-\\bar\\alpha_{t-1})^D}{(1-\\bar\\alpha_t)^D}}} \\\\[5pt]\n&= \\left((2\\pi)^{-\\frac{1}{2}}\\right)^D \\frac{1}{\\sqrt{\\left(\\dfrac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)}\\right)^D}} \\\\[5pt]\n&= \\frac{1}{ (\\sqrt{2\\pi})^D \\sqrt{\\left( \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\right)^D}}\n\\end{aligned}\n\\]\n마지막으로 \\(C_3 = \\mathbf{v}^T \\mathbf{v}\\)를 보이기 위해 먼저 \\(\\mathbf{v}^T \\mathbf{v}\\)를 계산합니다.\n\\[\n\\begin{aligned}\n\\mathbf{v}^T \\mathbf{v} &=  \\left(  \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1}) \\mathbf{x}_t^T }{1-\\bar\\alpha_{t}} + \\frac{ \\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\mathbf{x}_0^T}{1-\\bar\\alpha_{t}} \\right) \\left(  \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1}) \\mathbf{x}_t }{1-\\bar\\alpha_{t}} + \\frac{ \\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\mathbf{x}_0}{1-\\bar\\alpha_{t}}  \\right) \\\\[10pt]\n&= \\frac{\\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_t \\mathbf{x}_t + \\frac{\\sqrt{\\alpha_t}\\sqrt{\\bar\\alpha_{t-1}}(1-\\bar\\alpha_{t-1})(1-\\alpha_t)}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_t \\mathbf{x}_0 + \\frac{\\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_0 \\mathbf{x}_t + \\frac{\\bar\\alpha_{t-1}(1-\\alpha_t)^2}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_0 \\mathbf{x}_0 \\\\[10pt]\n&= \\color{#20639B}{\\frac{\\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t}\n+ \\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1}(1-\\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n다음으로 \\(C_3\\)이 방금 전개한 식과 같음을 보이면 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 정규 분포가 됩니다. 이를 위해 \\(C_3\\) 정의로 부터 식을 전개하도록 하겠습니다.\n\\[\n\\begin{aligned}\nC_3 &= \\frac{C_2(\\mathbf{x}_0, \\mathbf{x}_1)}{\\dfrac{1-\\bar\\alpha_t}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}} \\\\[10pt]\n&= \\frac{ \\dfrac{\\mathbf{x}^T_t \\mathbf{x}_t}{1-\\alpha_t} + \\dfrac{\\bar\\alpha_{t-1}\\mathbf{x}^T_0 \\mathbf{x}_0}{1-\\bar\\alpha_{t-1}} - \\dfrac{\\mathbf{x}^T_t \\mathbf{x}_t - 2 \\sqrt{\\bar\\alpha_t} \\mathbf{x}^T_t \\mathbf{x}_0 + \\bar\\alpha_t \\mathbf{x}^T_0 \\mathbf{x}_0 }{1-\\bar\\alpha_t} }{\\dfrac{1-\\bar\\alpha_t}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}} \\\\[10pt]\n&= \\frac{(1-\\bar\\alpha_{t-1})\\mathbf{x}^T_t \\mathbf{x}_t + (1-\\alpha_t)\\bar\\alpha_{t-1} \\mathbf{x}^T_0 \\mathbf{x}_0 - \\dfrac{(\\mathbf{x}^T_t \\mathbf{x}_t - 2 \\sqrt{\\bar\\alpha_t} \\mathbf{x}^T_t \\mathbf{x}_0 + \\bar\\alpha_t \\mathbf{x}^T_0 \\mathbf{x}_0)(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}}{1-\\bar\\alpha_t} \\\\[10pt]\n&= \\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\mathbf{x}^T_t \\mathbf{x}_t + \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\mathbf{x}^T_0 \\mathbf{x}_0 - \\frac{(\\mathbf{x}^T_t \\mathbf{x}_t - 2 \\sqrt{\\bar\\alpha_t} \\mathbf{x}^T_t \\mathbf{x}_0 + \\bar\\alpha_t \\mathbf{x}^T_0 \\mathbf{x}_0)(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\\\[10pt]\n&=  \\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\mathbf{x}^T_t \\mathbf{x}_t + \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\mathbf{x}^T_0 \\mathbf{x}_0  \\\\[5pt]\n& \\quad - \\left(\n    \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\mathbf{x}^T_t \\mathbf{x}_t  \n    - \\frac{2 \\sqrt{\\bar\\alpha_t}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\mathbf{x}^T_t \\mathbf{x}_0 + \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\mathbf{x}^T_0 \\mathbf{x}_0\n\\right) \\\\[10pt]\n&= \\color{#20639B}{\\left[ \\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} - \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\right]}\n\\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t} \\\\[5pt]\n& \\quad + \\color{#3CAEA3}{\\left[ \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} - \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} \\right]}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n이제 \\(\\left[ \\cdot \\right]\\) 부분을 정리합니다.\n먼저 네이비색 부분입니다.\n\\[\n\\color{#20639B}{\\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} - \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}} \\color{black}{= \\frac{(1-\\bar\\alpha_{t-1})(1-\\bar\\alpha_t)-(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}}\n\\]\n분자만 따로 정리를 하면\n\\[\n\\begin{aligned}\n(1-\\bar\\alpha_{t-1})(1-\\bar\\alpha_t)-(1-\\alpha_t)(1-\\bar\\alpha_{t-1}) &= (1-\\bar\\alpha_{t-1})\\left( (1-\\bar\\alpha_t)-(1-\\alpha_t) \\right) \\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) ( 1- \\bar\\alpha_t - 1 + \\alpha_t) \\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) (\\alpha_t - \\bar\\alpha_t) \\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) (\\alpha_t - (\\alpha_t \\cdot \\bar\\alpha_{t-1}))\\\\[5pt]\n&= (1-\\bar\\alpha_{t-1}) \\alpha_t (1 - \\bar\\alpha_{t-1}) \\\\[5pt]\n&= \\alpha_t (1-\\bar\\alpha_{t-1})^2\n\\end{aligned}\n\\]\n분자를 정리된 결과로 바꾸면\n\\[\n\\color{#20639B}{\\frac{(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} - \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{=}\n\\color{#20639B}{\\frac{ \\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}}\n\\]\n이어서 민트색 부분도 정리합니다.\n\\[\n\\color{#3CAEA3}{ \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} - \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} } \\color{black}{\n    = \\frac{(1-\\alpha_t)(1-\\bar\\alpha_t)\\bar\\alpha_{t-1} - \\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\n}\n\\]\n분자만 따로 정리를 하면\n\\[\n\\begin{aligned}\n(1-\\alpha_t)(1-\\bar\\alpha_t) \\bar\\alpha_{t-1} - \\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})\n&= (1 - \\alpha_t) \\left( \\bar\\alpha_{t-1} (1-\\bar\\alpha_t) - \\alpha_t(1 - \\bar\\alpha_{t-1}) \\right) \\\\[5pt]\n&=(1 - \\alpha_t) ( \\bar\\alpha_{t-1} - \\bar\\alpha_t\\bar\\alpha_{t-1} - \\bar\\alpha_t + \\bar\\alpha_t\\bar\\alpha_{t-1} ) \\\\[5pt]\n&= (1 - \\alpha_t) (\\bar\\alpha_{t-1} - \\bar\\alpha_t) \\\\[5pt]\n&= (1 - \\alpha_t) (\\bar\\alpha_{t-1} - (\\alpha_t \\cdot \\bar\\alpha_{t-1}))\\\\[5pt]\n&= (1 - \\alpha_t) \\bar\\alpha_{t-1} (1 - \\alpha_t) \\\\[5pt]\n&= \\bar\\alpha_{t-1} (1 - \\alpha_t)^2\n\\end{aligned}\n\\]\n분자를 정리된 결과로 바꾸면\n\\[\n\\color{#3CAEA3}{ \\frac{(1-\\alpha_t)\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} - \\frac{\\bar\\alpha_t(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2} } \\color{black}{=}\n\\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1} (1 - \\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\]\n계산 결과를 \\(C_3\\)이 전개된 식에 다시 대입하고 최종 결과를 \\(\\mathbf{v}^T \\mathbf{v}\\)와 비교하면\n\\[\n\\begin{aligned}\nC_3 &= \\color{#20639B}{\\frac{ \\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}} \\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t} + \\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1} (1 - \\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\mathbf{v}^T \\mathbf{v} &= \\color{#20639B}{\\frac{\\alpha_t (1-\\bar\\alpha_{t-1})^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_t \\mathbf{x}_t}\n+ \\color{#ED553B}{2 \\left(\\frac{\\sqrt{\\bar\\alpha_{t}}(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{(1-\\bar\\alpha_t)^2}\\right)}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_t}\n+ \\color{#3CAEA3}{\\frac{\\bar\\alpha_{t-1}(1-\\alpha_t)^2}{(1-\\bar\\alpha_t)^2}}\n\\color{black}{\\mathbf{x}^T_0 \\mathbf{x}_0}\n\\end{aligned}\n\\]\n두 식은 일치하고 이로부터 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)는 평균과 분산이 다음과 같은 정규 분포라는 것을 알 수 있습니다.\n\\[\n\\begin{aligned}\n\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) &:= \\frac{\\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)}{1-\\bar\\alpha_t} \\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t = \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t} \\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t \\\\[5pt]\n\\tilde{\\beta}_t &:= \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} \\beta_t\n\\end{aligned} \\tag{7}\n\\]\n따라서\n\\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}) \\tag{6}\n\\]\n가 됩니다.\n타겟이 계산되었으니 다음 단계로 만들어야 하는 한 시간 단계에 대한 리버스 프로세스를 다음처럼 정의 합니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\n\\]\n위 식에서 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\), \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\)가 뉴럴넷의 출력이 되는 것입니다. 이때 분산은 문제를 간단히 하기 위해 \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)=\\sigma^2_t \\mathbf{I}\\)처럼 대각 행렬로 정의합니다. 그리고 \\(\\sigma^2_t\\)는 식(7)에서 유도된 타겟에 의해 \\(\\sigma^2_t = \\tilde\\beta_t = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} \\beta_t\\)로 두거나 더 간단히 그냥 \\(\\sigma^2_t = \\beta_t\\)로 두게 됩니다. 이 두 결과 모두 실험에서 결과에 큰 영향을 주지 않는다고 합니다. 분산을 대각행렬로 간단히해서 다시 쓰면 아래과 같습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma^2_t \\mathbf{I})\n\\]\n손실 함수의 값은 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)와 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\)의 \\(D_{KL}\\)이고 이 두 분포는 모두 정규 분포임을 알고 있으므로 다음처럼 주어지는 정규분포에 대한 \\(D_{KL}\\)을 이용합니다.\n\\[\nD_{KL}(\\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}_x, \\boldsymbol{\\Sigma}_x) \\,\\lVert\\, \\mathcal{N}(\\mathbf{y}; \\boldsymbol{\\mu}_y, \\boldsymbol{\\Sigma}_y)) = \\frac{1}{2}\\left( \\text{tr}\\left(\\boldsymbol{\\Sigma}_y^{-1} \\boldsymbol{\\Sigma}_x \\right) - d + (\\boldsymbol{\\mu}_y - \\boldsymbol{\\mu}_x)^T \\boldsymbol{\\Sigma}^{-1}_y (\\boldsymbol{\\mu}_y - \\boldsymbol{\\mu}_x) + \\log \\left( \\frac{\\det \\boldsymbol{\\Sigma}_y}{\\det \\boldsymbol{\\Sigma}_x} \\right) \\right)\n\\]\n여기서 \\(d\\)는 \\(\\mathbf{x}\\), \\(\\mathbf{y}\\)의 차원입니다.\n이렇게 주어진 정규 분포의 \\(D_{KL}\\)를 식(6)과 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma^2_t \\mathbf{I})\\)를 이용하여 쓰면\n\\[\n\\begin{aligned}\nD_{KL}(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\,\\lVert\\, p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))\n&= D_{KL}(\\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde\\beta_t\\mathbf{I}) \\,\\lVert\\, \\mathcal{N}(\\mathbf{x}_{t-1};\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma^2_t \\mathbf{I})) \\\\[5pt]\n&= \\frac{1}{2} \\left( (\\boldsymbol{\\mu}_\\theta-\\tilde{\\boldsymbol{\\mu}}_t)^T (\\sigma^2_t \\mathbf{I})^{-1} (\\boldsymbol{\\mu}_\\theta-\\tilde{\\boldsymbol{\\mu}}_t)+ \\underbrace{\\text{tr}\\left((\\sigma^2_t \\mathbf{I})^{-1} \\tilde\\beta_t \\mathbf{I} \\right) -d +\\log \\frac{\\det \\sigma^2_t \\mathbf{I}}{\\det \\tilde\\beta\\mathbf{I}}}_{C} \\right) \\\\[5pt]\n&= \\frac{1}{2 \\sigma^2_t} \\lVert  \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) - \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) \\rVert^2_2 + C\n\\end{aligned}\n\\]\n위 식에서 \\(C\\)는 파라미터 \\(\\theta\\)와 무관한 항들입니다. 만약 전술한대로 \\(\\sigma^2_t = \\tilde\\beta_t\\) 또는 \\(\\sigma^2_t = \\tilde\\beta_t = \\beta_t\\)로 두면 \\(C\\)는 사라집니다. 이를 이용하여 식(5)에 \\(L_{t-1}\\)을 써보면 다음과 같게 됩니다.\n\\[\nL_{t-1} = \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\left[ \\frac{1}{2 \\sigma^2_t} \\lVert \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)   \\rVert^2_2\\right] + C \\tag{8}\n\\]\n\\(\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)\\)는 \\(t\\) 시간 단계의 노이즈가 많은 \\(\\mathbf{x}_t\\)로 부터 만들어진 노이즈가 약간 작은 \\(\\mathbf{x}_{t-1}\\)의 분포의 평균이 되고 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\)는 \\(\\mathbf{x}_t\\)와 \\(t\\)를 입력으로 받은 뉴럴넷이 출력한 어떤 평균입니다. 이 두 평균의 차이가 작게 되도록 뉴럴넷을 학습시키면 뉴럴넷이 \\(\\mathbf{x}_t\\)를 입력으로 받고 출력하는 출력값은 \\(\\mathbf{x}_{t-1}\\)의 평균이 되고 이는 노이즈가 약간 줄어들어 있어야 합니다.\n식(8)의 \\(\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)\\)부분을 식(4)를 이용해 좀 더 정리를 하도록 하겠습니다. 식(4)는 \\(\\mathbf{x}_0\\)로 부터 임의의 시간 단계 \\(t\\)에 대한 \\(\\mathbf{x}_t\\)에 대한 분포를 다음처럼 바로 정의 한 식입니다.\n\\[\nq(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, (1- \\bar{\\alpha}_t)\\mathbf{I})\n\\]\n이 식을 이용하면 \\(\\mathbf{x}_t\\)를 다음처럼 표준 정규분포를 이용해 reparameterization 할 수 있습니다.\n\\[\n\\mathbf{x}_t (\\mathbf{x}_0, \\boldsymbol{\\epsilon}) = \\sqrt{\\bar\\alpha_t} \\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon}, \\qquad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\n이로 부터 역으로\n\\[\n\\mathbf{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right)\n\\]\n임을 알 수 있습니다. 이는 추가된 노이즈만 알면 \\(\\mathbf{x}_t\\)에서 \\(\\mathbf{x}_0\\)를 계산할 수 있다는 것을 나타냅니다. 이를 식(8) 대입하면\n\\[\nL_{t-1} - C = \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\tilde{\\boldsymbol{\\mu}}_t \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) ,  \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right)   \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\tag{9}\n\\]\n가 됩니다. 잠재 변수 \\(\\mathbf{x}_t\\)는 \\(\\mathbf{x}_0\\), \\(\\boldsymbol{\\epsilon}\\)의 함수이므로 기댓값을 구하기 위한 적분 변수는 모든 \\(\\mathbf{x}_0\\), \\(\\boldsymbol{\\epsilon}\\)로 바뀌었습니다.\n이제 식(7)로 부터 \\(\\tilde{\\boldsymbol{\\mu}}_t\\)를 정리합니다.\n\\[\n\\begin{aligned}\n\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), \\mathbf{x}_0) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t} \\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) \\\\[10pt]\n&= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t} \\left( \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right) \\right) + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) \\\\[10pt]\n&= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{\\sqrt{\\bar\\alpha_t}(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t\\sqrt{1-\\bar\\alpha_{t}}}{\\sqrt{\\bar\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) \\\\[10pt]\n&=\\left( \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{\\sqrt{\\bar\\alpha_{t-1}}\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)} + \\frac{\\sqrt{\\alpha_t}(1-\\sqrt{\\bar\\alpha_{t-1}})}{1-\\bar\\alpha_t} \\right) \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{\\bar\\alpha_{t-1}}\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\\\[10pt]\n&= \\left( \\frac{\\beta_t}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)} + \\frac{\\alpha_t(1-\\bar\\alpha_{t-1})}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)} \\right) \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})-\\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\\\[10pt]\n&= \\frac{\\beta_t + (\\alpha_t - \\bar\\alpha_t)}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\\\[10pt]\n&= \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\frac{\\beta_t + (\\alpha_t - \\bar\\alpha_t)}{(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{(1-\\bar\\alpha_t)}\\boldsymbol{\\epsilon} \\right) \\\\[10pt]\n&= \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\frac{1-\\alpha_t + (\\alpha_t - \\bar\\alpha_t)}{(1-\\bar\\alpha_t)}\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) \\\\[10pt]\n&= \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right)\n\\end{aligned}\n\\]\n이 결과를 식(9)에 대입하면 식(10)이 얻어집니다.\n\\[\n\\begin{aligned}\nL_{t-1} - C &= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\tilde{\\boldsymbol{\\mu}}_t \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) ,  \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) -  \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon} \\right)   \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\qquad {(9)} \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\qquad\\qquad \\qquad \\qquad  {(10)}\n\\end{aligned}\n\\]\n종합하면 \\(\\boldsymbol{\\mu}_\\theta\\)는 \\(\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right)\\)를 예측하면 되는데 \\(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})\\)는 가우시안 노이즈 \\(\\boldsymbol{\\epsilon}\\)을 사용하여 \\(t\\) 시간 단계에 해당하는 노이즈가 낀 이미지 이고 이는 \\(\\boldsymbol{\\mu}_\\theta\\)를 추정하는 뉴럴넷의 입력으로 주어질 것입니다. 나머지 \\(\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\) 부분도 \\(t\\)가 주어지면 계산되는 항이므로 결국 예측 해야 하는 것은 \\(\\mathbf{x}_t\\)를 만들때 사용한 노이즈 \\(\\boldsymbol{\\epsilon}\\)이라는 것을 알 수 있습니다. 물론 논문에서도 밝히고 있듯이 \\(\\boldsymbol{\\mu}_\\theta\\)는 \\(\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right)\\)를 바로 예측해도 되는데 실험 결과 생성되는 이미지 퀄리티가 떨어진다고 합니다.\n\\[\n\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\tilde{\\boldsymbol{\\mu}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), \\frac{1}{\\sqrt{\\bar\\alpha_t}}(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t) )\\right)\n\\]\n위 식은 \\(\\boldsymbol{\\mu}_\\theta\\)가 타겟 \\(\\tilde{\\boldsymbol{\\mu}}\\)와 같아지기 위해서 \\(\\mathbf{x}_t\\)를 만들기 위해 추가된 노이즈를 \\(\\boldsymbol{\\epsilon}_\\theta\\)으로 잘 예측해야 한다는 것을 이야기 합니다. 예측하는 뉴럴넷 입장에서는 \\(\\mathbf{x}_t\\)만 알 수 있을 뿐 \\(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})\\)에서 \\(\\boldsymbol{\\epsilon}\\)는 알 수 없으므로 추청해야하는 것입니다. 우변항을 이전과 같은 과정으로 계산하여 정리하면\n\\[\n\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\tilde{\\boldsymbol{\\mu}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), \\frac{1}{\\sqrt{\\bar\\alpha_t}}(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\sqrt{1-\\bar\\alpha_t} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t) )\\right) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right) \\tag{11}\n\\]\n를 얻게 됩니다. 식(11) 마지막 표현에서 \\(\\mathbf{x}_t\\)를 만들때 사용된 노이즈 \\(\\boldsymbol{\\epsilon}\\)를 모른다는 것을 명확히 하기 위해 노이즈 낀 이미지를 \\(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon})\\)로 쓰지 않고 \\(\\mathbf{x}_t\\)로 표시 했습니다.\n식(10)에 식(11)을 대입하고 정리하면 식(12)를 얻게 됩니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} &\\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}), t) \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t(\\mathbf{x}_0, \\boldsymbol{\\epsilon}) - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon} \\right) - \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right) \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert  - \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}  + \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)  \\right\\rVert^2_2 \\right] \\\\[10pt]\n\\end{aligned}\n\\]\n벡터 노름에 대한 다음 성질을 이용하여\n\\[\n\\begin{aligned}\n\\lVert c \\mathbf{x} - c \\mathbf{y} \\rVert^2_2 &= \\sum_i (cx_i - cy_i)^2 \\\\\n&= \\sum_i c^2 x_i^2 - 2 c^2 x_i y_i + c^2 y_i^2 \\\\\n&= \\sum_i c^2 y_i^2 - 2 c^2 x_i y_i + c^2 x_i^2 \\\\\n&= c^2 \\sum_i y_i^2 - 2 x_i y_i + x_i^2 \\\\\n&= c^2 \\lVert  \\mathbf{y} - \\mathbf{x} \\rVert^2_2\n\\end{aligned}\n\\]\n식을 계속 정리합니다.\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} & \\left[ \\frac{1}{2 \\sigma^2_t} \\left\\lVert  - \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}  + \\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)  \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\left\\lVert  -\\boldsymbol{\\epsilon}  + \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\left\\lVert   \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t)-\\boldsymbol{\\epsilon}  \\right\\rVert^2_2 \\right] \\qquad \\because \\text{eq.} (4)\\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\left\\lVert   \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t)  \\right\\rVert^2_2 \\right]\n\\end{aligned}\n\\]\n이렇게 \\(t=1\\)에서 \\(t={T-1}\\) 단계에 해당하는 손실을 유도 했습니다. 식(12)를 볼때 한가지 주의해야할 점은 $_(_0 + , t) $에 보이는 \\(\\boldsymbol{\\epsilon}\\)은 타겟인데 이를 예측하는 뉴럴넷 입장에서는 \\(\\boldsymbol{\\epsilon}\\)를 알 수 없고 \\(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}\\)에 의해 노이즈가 낀 \\(\\mathbf{x}_t\\)만 입력으로 받는다는 점입니다. 정리하면 \\(\\mathbf{x}_t\\)와 \\(t\\)를 입력으로 받고 \\(\\boldsymbol{\\epsilon}\\)을 예측하는 것입니다. \\(\\boldsymbol{\\epsilon}\\)에 색을 넣어보면 다음과 같고 빨간색은 타겟, 파란색은 예측을 나타냅니다.\n\\[\nL_{t-1} - C = \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_t}\\cdot\\frac{\\beta_t^2}{\\alpha_t(1-\\bar\\alpha_t)} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}{(}\n    \\underbrace{\n        \\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\color{red}{\\boldsymbol{\\epsilon}}\n    }_{\\mathbf{x}_t}\n    \\color{black}{ , t)}  \\rVert^2_2 \\right] \\tag{12}\n\\]\n\n\n\n꽤 긴 내용으로 \\(t=1 \\sim T-1\\)까지 적용되는 손실 함수에 대해서 알아봤습니다. 마지막으로 \\(L_0\\)에 대해서 알아볼 차례인데 그전에 식(5)의 마지막 항인 \\(- \\log p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)\\)의 의미를 다시 한번 생각해봅시다.\n이미지를 생성하는 마지막 과정에서 \\(\\mathbf{x}_1\\)을 입력으로 받고 \\(\\mathbf{x}_0\\)를 출력으로 내는 모델은 노이즈가 있는 이미지에서 노이즈가 없는 이미지를 만들어내는 모델이므로 디코더라고 할 수 있습니다. 이 디코더에 입력으로 \\(\\mathbf{x}_1\\), 출력으로 \\(\\mathbf{x}_0\\)를 세팅하고 그때 가능도가 최대가 되도록 \\(\\theta\\)를 설정하는 과정이 바로 디코더를 학습시키는 과정이 됩니다. \\(p_\\theta(\\mathbf{x}_0\\mid\\mathbf{x}_1)\\)이 바로 \\(\\mathbf{x}_1\\)을 조건으로 하는 \\(\\mathbf{x}_0\\)의 가능도 함수입니다. 따라서 식(5)에서 마지막 항은 최종적으로 깨끗한 이미지를 생성하는 디코더를 위한 손실 항이 됩니다. 이 손실 항을 식(13)과 같이 정의 합니다.\n\\[\n\\begin{aligned}\np_\\theta(\\mathbf{x}_0 \\mid \\mathbf{x}_1) &= \\prod_{i=1}^D \\int^{\\delta_+(x^i_0)}_{\\delta_-(x^i_0)} \\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 ) dx \\\\[5pt]\n\\delta_+(x) &= \\begin{cases}\n\\infty & \\mbox{if }x=1 \\\\\nx+\\frac{1}{255} & \\mbox{if } x &lt; 1\n\\end{cases} \\quad\n\\delta_-(x) = \\begin{cases}\n-\\infty & \\mbox{if }x = -1 \\\\\nx-\\frac{1}{255} & \\mbox{if } x &gt; -1\n\\end{cases}\n\\end{aligned} \\tag{13}\n\\]\n식(13)은 보기에도 골치가 아파오게 생겼습니다. 그런데 의미를 잘 생각해보면 크게 복잡한 식이 아니란 것을 알 수 있습니다.\n먼저 \\(\\mathbf{x}_0\\)와 \\(\\mathbf{x}_1\\)은 픽셀수가 똑같은 이미지라는 점을 상기합시다. 식(13)에서 상첨자 \\(i\\)는 픽셀의 인덱스를 나타냅니다. 그리고 앞서 시간 단계에 대한 리버스 프로세스를 다음처럼 정규분포로 정의 했습니다.\n\\[\np_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t ) := \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta (\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\right)\n\\]\n노이즈가 제거된 이미지 \\(\\mathbf{x}_{t-1}\\)는 정규분포 하는데 그 분포의 평균과 분산은 노이즈가 조금 더 많은 \\(\\mathbf{x}_t\\)와 시간 단계 \\(t\\)를 입력으로 받은 모델이 예측한 \\(\\boldsymbol{\\mu}_\\theta\\)와 \\(\\boldsymbol{\\Sigma}_\\theta\\)가 된다는 이야기입니다.\n이 정의를 그대로 식(13)에서 다시 사용했는데 두가지 다른 점이 있습니다.\n첫 번째 다른 점은 모든 픽셀이 독립이라는 가정을 한 것입니다. 그래서 \\(\\mathbf{x}_0\\)가 따르는 분포의 평균에 해당하는 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_1, 1)\\)을 각 픽을 별로 \\(\\mu^i_{\\theta}(\\mathbf{x}_1, 1)\\)라고 따로 적고 있습니다. 분산은 예측하지 않을 것이므로 \\(\\sigma^2_1\\)처럼 상수로 나타냈습니다. 이미지에서 픽셀들은 서로 독립이 아닙니다. 특정 픽셀의 색은 인접한 픽셀의 색에 영향을 매우 강하게 받습니다. 그렇기에 픽셀간 종속성을 고려하면 더 뛰어난 디코더가 모델링 되지만 논문에선 향후 작업으로 남겨둔다고 밝히고 있습니다. 이런 독립 가정으로 인해 가능도는 각 픽셀에 대한 확률값의 곱이 됩니다. \\(\\prod\\) 기호가 식에 등장해 모든 확률값을 곱하고 있는 것입니다. 그럼 \\(D\\)는 픽셀 개수가 되고 뒤 적분항은 확률이 됩니다. 확률밀도 함수를 특정 구간에서 적분하므로 확률이 맞습니다.\n두번째 다른 점은 최종 이미지 \\(\\mathbf{x}_0\\)의 각 픽셀 \\(x^i_0\\)는 이산확률변수라는 점입니다. 이 변수들은 \\(0\\) ~ \\(255\\)까지 값을 가지는 이산 확률 변수이고 선형 변환을 통해 \\(-1\\) ~ \\(1\\)로 변환됩니다. 리버스 프로세스 정의에 의해 \\(\\mathbf{x}_0\\)의 \\(i\\)번째 픽셀 \\(x_0^i\\)은 \\(\\mathbf{x}_1\\)로 부터 예측된 평균 \\(\\boldsymbol{\\mu}_\\theta (\\mathbf{x}_1, 1)\\)의 \\(i\\)번째 픽셀인 \\(\\mu^i_{\\theta}(\\mathbf{x}_1, 1)\\)를 평균으로 하는 정규분포를 따르게 됩니다. 그런데 \\(x_0^i\\)는 이산 확률변수기 때문에 이 확률변수의 특정 값에 대한 확률을 구하기 위해서는 \\(\\mathcal{N}(x_0^i; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )\\)를 \\(x_0^i\\) 주변에서 적분하여야 합니다. 이때 적분 구간을 설정해야 하는데 \\(x_0^i\\)를 중심으로 좌우 \\(\\frac{1}{255}\\)로 구간을 설정합니다. 이렇게 설정한 이유는 픽셀이 가질 수 있는 값이 총 256개며 각 값 사이의 구간이 255개 있기 때문입니다. 따라서 적분 구간의 크기는 \\(\\frac{2}{255}\\)가 됩니다. 만약 \\(x_0^i\\)가 경계값인 \\(-1\\)이면 때 적분 구간의 왼쪽값을 \\(-\\infty\\), 오른쪽 값을 \\(x_0^i + \\frac{1}{255}\\)로 설정합니다. 반대쪽 경계값인 \\(1\\)이면 왼쪽 값은 \\(x_0^i - \\frac{1}{255}\\), 오른쪽 값을 \\(\\infty\\)로 설정합니다. 이는 \\(\\mathcal{N}(x_0^i; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )\\)의 전 정의역을 적분구간에 포함시키기 위함입니다.\n이렇게 정의된 가능도 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t )\\)를 이용해 학습된 디코더는 어떤 \\(\\mathbf{x}_0\\)를 만들어내게 되는지 생각해봅시다. 이를 이해하기 아래코드로 식(13)을 시각화 하겠습니다.\n\nfrom scipy.stats import norm\n\nplotly.io.renderers.default = \"notebook\"\n\n\nmu_theta_i = 0.25\nsigma2_i = 0.1\nxmin = -1.3\nxmax = 1.6\nN = norm(loc=mu_theta_i, scale=np.sqrt(sigma2_i))\nx = np.linspace(-1.3, 1.6, 150)\ny = N.pdf(x)\n\n# 0~10을 -1~1로 매핑, 각 값은 25단계로 discrete\nx0s = np.linspace(0, 10, 25) / 10 * 2 - 1\n\n# 적분 구간의 절반\nbin_width_half = 1 / (len(x0s)-1)\n\n\nlayout = go.Layout(\n    # title=r'$\\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )$',\n    title=r'N(mu^i_theta(x; x_1, 1), sigma^2_1)',\n    width=800, height=400,\n    margin=dict(l=0, r=0, b=0, t=25),\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\n\nprobs = []\nfor x0 in x0s:\n    if x0 == -1:\n        int_left = xmin\n        int_right = x0 + bin_width_half\n    elif x0 == 1:\n        int_left = x0 - bin_width_half\n        int_right = xmax\n    else:\n        int_left = x0 - bin_width_half\n        int_right = x0 + bin_width_half\n\n    x_interval = np.linspace(int_left, int_right, 50)\n    y_interval = N.pdf(x_interval)\n    probs.append([x_interval, y_interval, np.trapz(y=y_interval, x=x_interval)])\n\n# 적분 합이 거의 1이 되는지 테스트\n# P = 0.0\n# for i in probs:\n#     P += i[2]\n# print(P)\n\n# Add traces, one for each slider step\nfig.add_trace(go.Scatter(x=x, y=y,\n                    mode='lines',\n                    name='lines',\n                    line_color='indigo'\n                    ))\n\nfor i, prob in enumerate(probs):\n    x_part = prob[0]\n    y_part = prob[1]\n    # https://plotly.com/python/filled-area-plots/\n    fig.add_trace(\n        go.Scatter(\n            x=x_part,\n            y=y_part,\n            fill='tozeroy',\n            mode='lines',\n            line_color='orange',\n            visible=False,\n            name=f\"area:{prob[2]:.3f}\"\n        )\n    )\n\nfig.data[0].visible = True\nfig.data[1].visible = True\n\nsteps = []\nfor i in range(len(probs)):\n    step = dict(\n        method=\"update\",\n        args=[\n            # 일단 모든 적분 영역 안보이게, 가우시안만 보이게\n            {\"visible\": [False] * (len(fig.data))},\n            # {\"visible\": [False] * (len(fig.data)-1) + [True]},\n        ],\n        label=f\"{x0s[i]:.2f}\"\n    )\n    # 해당 i번째 적분 영역만 보이게\n    step[\"args\"][0][\"visible\"][i+1] = True\n    step[\"args\"][0][\"visible\"][0] = True\n    steps.append(step)\n\nsliders = [\n    dict(\n        active=0,\n        currentvalue={\"prefix\": \"x0: \"},\n        pad={\"l\":10, \"t\": 50, \"r\":10, \"b\":10},\n        steps=steps\n    )\n]\n\nfig.update_layout(sliders=sliders)\n\nfig.show()\n\n\n                                                \n\n\n위 그래프는 특정 \\(i\\)번째 픽셀의 평균을 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)=0.25\\)로 가정했을 때 \\(x^i_0\\)값에 따른 적분 구간과 적분값을 보여줍니다. 단 적절한 시각화를 위해 \\(x_0\\)는 0에서 10사이를 24등분해서 25개 값을 가지는 것으로 가정했습니다. 그럼 적분구간의 너비는 \\(\\frac{2}{24}\\)가 되는 상황입니다.\n슬라이드바를 이동하면 \\(x^i_0\\)의 값을 변경할 수 있습니다. 편의상 그래프에서 상첨자 \\(i\\)는 생략했습니다.\n먼저 x0값을 \\(-1\\)로 설정해보세요. 그래프는 아래 그림처럼 보입니다.\n\n적분 구간은 \\(-\\infty\\)에서 \\(-1+\\frac{1}{24}\\)으로 설정됩니다. 하지만 이 구간에서 확률밀도값이 거의 모두 0이기 때문에 적분 구간이 잘 확인되지 않습니다. 그래프 오른쪽에 area값을 보면 적분값 역시 거의 0임을 알 수 있습니다.\n이제 슬라이드를 오른쪽으로 이동시키면 적분 구간이 그려지게 됩니다. x0값을 \\(1\\)로 설정했을 때 그래프는 다음처럼 보이게 됩니다.\n\n설정된 값 \\(1\\)약간 왼쪽(정확히는 \\(1-\\frac{1}{24}\\))에서 부터 \\(\\infty\\)까지 적분되는것을 확인할 수 있고 그 값은 0.013정도가 됩니다.\n마지막으로 x0값을 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)\\)와 같은 값인 0.25로 설정하면 적분 구간은 정규분포의 가운데 위치하게 됩니다.\n\n이때 적분값은 0.105이며 슬라이드를 좌우로 이동해보면 알 수 있겠지만 이 값은 최대 적분값이 됩니다.\n이 실험은 식(13)처럼 가능도를 정의하면 디코더에 의해 생성될 \\(\\mathbf{x}_0\\)의 \\(i\\)번째 픽셀 값이 \\(\\mathbf{x}_1\\)으로 부터 예측된 평균의 \\(i\\)번째 픽셀값 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)\\)과 동일하면 가능도가 최대가 된다는 것을 이야기합니다.\n이 실험을 통해 디코더가 평균 \\(\\mu^i_\\theta(\\mathbf{x}_1, 1)\\)을 그대로 출력하면 가능도는 최대가 된다는 것을 알 수 있습니다!\n여기서 평균은 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1)\\), 다시말해 \\(\\mathbf{x}_1\\)을 입력받고 모델이 예측한 출력입니다. 이런 이유로 논문에서는 다음처럼 이야기하고 있으며\n\nwe display \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1)\\) noiselessly.\n\n이 문장의 의미는 샘플링 알고리즘에서 마지막 단계인 \\(\\mathbf{x}_0\\)를 샘플링할 때 표준 정규분포로 부터 샘플링된 노이즈를 추가하지 않는 다는 뜻입니다. (샘플링 알고리즘을 이야기할 때 알아보겠지만 확률적 다양성을 위해서 예측된 이미지 평균에 정규분포로 부터 샘플링된 노이즈를 더하는 과정을 거치게 됩니다.)\n이렇게 \\(L_0\\)에 대해서 자세히 알아봤습니다. 하지만 구현에서 마지막 단계에만 적용되는 디코더를 따로 모델링하지 않기 때문에 실제로 학습 과정에서 \\(L_0\\)를 사용하지는 않습니다. 그 이유에 대해서는 다음 절에서 자세히 알아보도록 하겠습니다.\n\n\n\n지금까지 정의한 손실함수는 모두 미분 가능하지만 그대로 사용하지 않고 식(14)처럼 간략화시켜 사용하게 됩니다. 식(14)는 식(12)에서 노름 앞부분에 곱해지는 상수를 제거한것과 동일합니다. 최소화를 위해서 상수는 제거해도 상관없기 때문에 식(14)를 제안한것은 타당합니다. 기댓값 기호에 \\(t\\)가 추가된 이유는 식(12)가 \\(t-1\\)에 대한 손실이므로 모든 시간 단계에 대해서 기댓값을 구하기 위해 추가되었습니다. 그런데 이렇게 제시된 식(14)가 앞서 따로 정의한 \\(L_0\\)도 대신할 수 있는지는 따져볼 필요가 있습니다.\n\\[\nL_{\\text{simple}}(\\theta) := \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\left\\lVert \\boldsymbol{\\epsilon}- \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t) \\right\\rVert^2_2 \\right] \\tag{14}\n\\]\n\\(x=1\\), \\(x=-1\\)일 때 무한대까지 적분하는 경우(edge effects)를 무시하면 식(13)의 적분은 다음처럼 근사될 수 있습니다.\n\\[\n\\int^{\\delta_+(x^i_0)}_{\\delta_-(x^i_0)} \\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 ) dx \\approx \\frac{1}{\\sqrt{2 \\pi \\sigma^2_1}} \\exp \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_i, 1))^2}{2 \\sigma_1^2} \\right) \\times \\frac{2}{255}\n\\]\n근사된 적분식은 아래 그림을 통해 직관적으로 이해할 수 있습니다.\n\nmu_theta_i = 0.0\nsigma2_i = 0.1\nN = norm(loc=mu_theta_i, scale=np.sqrt(sigma2_i))\nx = np.linspace(-1.5, 1.5, 150)\ny = N.pdf(x)\n\nlayout = go.Layout(\n    title=r'Approximated integrals',\n    width=800, height=400,\n    margin=dict(l=0, r=0, b=0, t=25),\n)\n\n# Create figure\nfig = go.Figure(layout=layout)\n\nhalf = 0.1\nx_part = np.linspace(0.25-half, 0.25+half, 100)\ny_part = N.pdf(x_part)\nfig.add_trace(\n    go.Scatter(\n        x=x_part,\n        y=y_part,\n        fill='tozeroy',\n        line_color='orange',\n        name=\"Exact Prob.\",\n        showlegend=False,\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=x, y=y,\n        mode='lines',\n        # name=r'$\\mathcal{N}(x; \\mu^i_{\\theta}(\\mathbf{x}_1, 1), \\sigma^2_1 )$',\n        name = r\"N(mu_theta(x; x_1, 1), sigma^2_1)\",\n        line_color='indigo'\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=[0.25, 0.25], y=[0, N.pdf(0.25)],\n        mode='lines',\n        line_color='indigo',\n        line_width=1,\n        line_dash='dash',\n        showlegend=False,\n    ),\n)\n\nfig.add_trace(go.Scatter(\n    x=[0.25],\n    y=[-0.1],\n    mode=\"text\",\n    #name=\"Lines, Markers and Text\",\n    text=[r\"x=0.25\"],\n    textposition=\"top center\",\n    textfont=dict(\n        size=15,\n        color=\"black\"\n    ),\n    showlegend=False,\n))\n\nfig.add_shape(\n    type=\"rect\",\n    xref=\"x\", yref=\"y\",\n    x0=0.25-half, y0=0,\n    x1=0.25+half, y1=N.pdf(0.25),\n    line = dict(\n        color=\"RoyalBlue\",\n        width=2,\n        # dash=\"dash\"\n    )\n)\n\nfig.add_annotation(\n    x=0.35, y=N.pdf(0.25), ax=50,\n    text=\"Approx. Prob.\", align=\"left\",\n    showarrow=True,\n    arrowhead=2, arrowsize=2,\n    arrowcolor=\"RoyalBlue\",\n    font=dict(\n        size=18,\n        color=\"RoyalBlue\"\n    )\n)\n\nfig.add_annotation(\n    x=0.21, y=0.5, ax=-66,\n    text=\"Exact Prob.\", align=\"left\",\n    showarrow=True,\n    arrowhead=2, arrowsize=2,\n    arrowcolor=\"orange\",\n    font=dict(\n        size=18,\n        color=\"orange\"\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\n위 그림은 \\(x^i_0=0.25\\)일 때 적당한 구간에서 정확한 적분값과 위 식으로 근사된 적분값을 나타냅니다. 이 근사치를 이용하여 \\(\\mathbf{x}_1\\)의 로그 가능도를 전개하면 다음과 같습니다.\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x}_1 \\mid \\mathbf{x}_0) &\\approx \\log \\left\\{ \\prod_{i=1}^D \\frac{1}{\\sqrt{2 \\pi \\sigma^2_1}} \\exp \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_i, 1))^2}{2 \\sigma_1^2} \\right) \\times \\frac{2}{255} \\right\\} \\\\[10pt]\n&= \\sum_{i=1}^D \\log \\left\\{ \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2}{2 \\sigma_1^2} \\right) \\times \\frac{2}{255} \\right\\} \\\\[10pt]\n&= \\sum_{i=1}^D \\left\\{  \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2_1}} + \\left( -\\frac{(x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2}{2\\sigma_1^2} \\right) + \\log \\frac{2}{255}  \\right\\} \\\\[10pt]\n&= \\sum_{i=1}^D \\log \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} + \\left( - \\frac{1}{2\\sigma_1^2} \\sum_{i=1}^D (x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2 \\right) + \\sum_{i=1}^D \\log \\frac{2}{255} \\\\[10pt]\n&= -\\frac{1}{2\\sigma_1^2} \\sum_{i=1}^D (x_0^i - \\mu_\\theta^i(\\mathbf{x}_1, 1))^2 + C \\\\\n&= -\\frac{1}{2\\sigma_1^2} \\lVert \\mathbf{x}_0 - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2 + C\n\\end{aligned}\n\\]\n최소화 문제로 바꾸기 위해 마이너스를 곱하고 상수를 무시하면\n\\[\nL_0 = -\\log p_\\theta(\\mathbf{x}_1 \\mid \\mathbf{x}_0) \\approx \\frac{1}{2\\sigma_1^2}\\lVert \\mathbf{x}_0 - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2\n\\]\n가 됩니다. 이를 \\(L_0\\)에 대한 근사값으로 사용할 수 있습니다.\n위 식에서 \\(\\mathbf{x}_0\\)는 식(4)에 reparameterization을 적용하는 과정에 의해\n\\[\n\\mathbf{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha_1}} \\left( \\mathbf{x}_1 -  \\sqrt{1-\\bar\\alpha_1} \\boldsymbol{\\epsilon} \\right)\n\\]\n로 표현됨을 앞서 알아봤고 또한 식(11)에 의해\n\\[\n\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) = \\frac{1}{\\sqrt{\\alpha_1}}\\left( \\mathbf{x}_1 - \\frac{\\beta_1}{\\sqrt{1-\\bar\\alpha_1}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\right)\n\\]\n이므로 이를 \\(L_0\\)에 대입합니다.\n\\[\n\\begin{aligned}\nL_0 &\\approx \\frac{1}{2\\sigma_1^2} \\lVert \\mathbf{x}_0 - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2 \\\\[10pt]\n&= \\frac{1}{2\\sigma_1^2} \\left\\lVert \\frac{1}{\\sqrt{\\bar\\alpha_1}} \\left( \\mathbf{x}_1 -  \\sqrt{1-\\bar\\alpha_1} \\boldsymbol{\\epsilon} \\right) - \\frac{1}{\\sqrt{\\alpha_1}}\\left( \\mathbf{x}_1 - \\frac{\\beta_1}{\\sqrt{1-\\bar\\alpha_1}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\right)  \\right\\rVert^2_2 \\\\[10pt]\n&= \\frac{1}{2\\sigma_1^2} \\left\\lVert - \\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}} \\boldsymbol{\\epsilon} + \\frac{1-\\alpha_1}{\\sqrt{\\alpha_1}\\sqrt{1-\\alpha_1}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\right\\rVert^2_2 \\qquad \\because \\bar\\alpha_1 = \\alpha_1 \\\\[10pt]\n&= \\frac{1}{2\\sigma_1^2} \\cdot \\frac{1-\\alpha_1}{\\alpha_1} \\lVert \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_1, 1) \\rVert^2_2\n\\end{aligned}\n\\]\n얻어진 \\(t=1\\)에 대한 손실은 식(12)에 \\(t=1\\)을 대입하고 상수를 무시한 결과와 일치 합니다.\n\\[\n\\begin{aligned}\nL_{0} &= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_1}\\cdot\\frac{\\beta_1^2}{\\alpha_1(1-\\bar\\alpha_1)} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}(\\mathbf{x}_1 , 1)  \\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_1}\\cdot\\frac{(1-\\alpha_1)^2}{\\alpha_1(1-\\bar\\alpha_1)} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}(\\mathbf{x}_1 , 1)  \\rVert^2_2 \\right] \\\\[10pt]\n&= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\frac{1}{2 \\sigma^2_1}\\cdot\\frac{1-\\alpha_1}{\\alpha_1} \\lVert   \\color{red}{\\boldsymbol{\\epsilon}} \\color{black}{-} \\color{blue}{\\boldsymbol{\\epsilon}_\\theta}\\color{black}(\\mathbf{x}_1 , 1)  \\rVert^2_2 \\right]\n\\end{aligned}\n\\]\n따라서 \\(L_0\\)를 계산하기 위해 적분항의 곱으로 정의된 별도의 손실 함수 식(13)을 따로 계산하지 않고 식(14)를 모든 시간 단계에 대해서 사용할 수 있습니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part1.html#학습-알고리즘",
    "href": "posts/diffusion/ddpm_part1.html#학습-알고리즘",
    "title": "A Gentle Introduction to Diffusion Model: Part 1 - DDPM",
    "section": "",
    "text": "지금까지 내용을 잘 이해했다면 다음 제시된 논문의 알고리즘을 선명하게 이해 할 수 있습니다.\n\n한줄 씩 따져보겠습니다.\n\n1: 학습의 반복을 나타냅니다.\n2: 준비된 학습 이미지중 하나를 선택합니다. 이 이미지가 \\(\\mathbf{x}_0\\)입니다.\n3: 임의의 시간 단계를 선택합니다.\n4: 표준 정규분포로 부터 노이즈를 샘플링합니다.\n5: 2, 3, 4단계에서 선택된 자료를 이용해서 \\(\\mathbf{x}_t\\)를 만들고 모델이 입력하여 출력으로 \\(\\boldsymbol{\\epsilon}_\\theta\\)를 생성합니다. 이를 4단계에서 샘플링한 노이즈와 비교하여 그래디언트를 구하고 경사하강법을 수행합니다.\n6: 이 단계를 손실이 충분히 줄어들 때까지 반복합니다.\n\n알고리즘 1을 수행함에 있어 이미지는 배치로 처리되므로 2, 3, 4단계에서 선택되는 이미지, 시간 단계, 노이즈는 배치수만큼 선택되어 일괄처리 될 것입니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part1.html#샘플링-알고리즘",
    "href": "posts/diffusion/ddpm_part1.html#샘플링-알고리즘",
    "title": "A Gentle Introduction to Diffusion Model: Part 1 - DDPM",
    "section": "",
    "text": "처음 DDPM 모델을 공부할 때 혼란스런 점은 학습된 모델이 깨끗한 이미지를 직접 출력하지 않는다는 점입니다. 학습 알고리즘을 보면 알 수 있듯이 모델이 출력하는 것은 입력 이미지에 포워드 프로세스를 적용할 때 사용한 노이즈입니다. 처음 이를 보면 “쓸데없이 노이즈를 예측해서 뭘 어쩌자는거지?” 라는 생각이 들게 됩니다. 하지만 식(11)에 의해 예측된 노이즈로 부터 이전 시간 단계에 대한 이미지의 평균을 구할 수 있습니다. 이를 이용해 다음처럼 반복적으로 이미지를 샘플링하게 됩니다.\n\n\n1: 모델에 입력될 완전 노이즈 \\(\\mathbf{x}_T\\)를 표준 정규분포로 부터 샘플링합니다.\n2: 시간 단계를 \\(T\\)에서 \\(1\\)까지 거슬러 오르면서 for 루프를 돕니다.\n3: 4단계에서 에측될 노이즈가 살짝 제거된 평균이미지에 추가할 노이즈 \\(\\mathbf{z}\\)를 샘플링합니다. 이 때 \\(t=1\\)인 마지막 시간 단계라면 노이즈를 0으로 둡니다.\n4: 식에 보이는 \\(\\frac{1}{\\sqrt{\\alpha}_t}(\\cdot)\\)부분은 식(11)에 의해 \\(\\mathbf{x}_t\\)로 부터 예측된 \\(\\mathbf{x}_{t-1}\\)의 평균 이미지이며 이 평균에 \\(\\sigma_t\\mathbf{z}\\)만큼의 노이즈를 추가하여 \\(\\mathbf{x}_{t-1}\\)을 만들어냅니다. 식에의해 계산된 평균을 샘플로 취해도 되지만 노이즈를 추가하면 확률적 다양성이 확보되고 최종 이미지 퀄리티도 더 좋아진다는 것이 경험적으로 알려져 있습니다. 단, 여기서 주목해야 하는 부분은 3 단계에 의해 \\(t=1\\)일때, 다시말해 \\(\\mathbf{x}_1\\)으로 부터 최종 이미지 \\(\\mathbf{x}_0\\)를 샘플링 할때는 노이즈를 추가하지 않습니다. 그 이유는 \\(L_0\\)를 설명하면서 이미 알아봤습니다.\n5: for루프를 계속 수행합니다.\n6: 생성된 이미지 \\(\\mathbf{x}_0\\)를 반환합니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part1.html#마무리",
    "href": "posts/diffusion/ddpm_part1.html#마무리",
    "title": "A Gentle Introduction to Diffusion Model: Part 1 - DDPM",
    "section": "",
    "text": "지금까지 DDPM의 이론적인 부분을 가능한 자세히 살펴봤습니다. 식이 많이 등장 하지만 계산만 복잡할 뿐 차근 차근 따라 간다면 누구나 이해할 수 있는 부분이고 다른 글들에서 풀어쓰기 귀찮은 이유로 생략된 부분들을 모두 다뤘습니다. 이 블로그가 존재하는 이유입니다. 이 내용을 바탕으로 실제 모델을 구현하는 내용을 다음 글에서 알아보도록 하겠습니다. 구현은 파이토치, 텐서플로 두가지 모두 제공될 예정이며 먼저 텐서플로 구현으로 다시 찾아오겠습니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-1.html",
    "href": "posts/diffusion/ddpm_part2-1.html",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-1 DDPM Hands-on with TensorFlow",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nimport plotly.io\nimport plotly.graph_objects as go\n\n# colab: coloab\n# jupyter lab: jupyterlab\n# jupyter notebook, quarto blog: notebook\nplotly.io.renderers.default = \"notebook\"\n\n이전 글에서 DDPM에 대한 이론을 자세히 알아 봤습니다. 2-1편에서는 텐서플로를 사용해서 DDPM 모델을 직접 만들어봅니다. DDPM을 구현한 많은 참고 자료들이 인터넷에 많으나 이론에 나오는 수식을 1:1 방식으로 간결하게 코드로 매치 시키는 구현은 찾아보기 힘듭니다. DDPM 공식 구현 코드도 간결하고 읽기 좋게 작성된 코드는 아닙니다. 좋은 품질의 이미지를 생성하기 위해 복잡한 네트워크 구조를 사용하고 다양한 실험 옵션을 반영하기 때문인데 이런 코드는 학습용 코드로는 부적합 합니다.\n이런 이유로 본 글에서는 최소한 납득할 만한 결과를 보여 주면서도 가능한 간단한 구조로 된 모델을 정의해 사용할 것입니다. 이를 통해 쉽고 빠르게 이론과 실제 코드를 연결하여 이해할 수 있도록 구성했습니다. 더 정교한 이미지를 생성하기 위한 복잡한 네트워크의 적용을 독자 여러분의 몫으로 남겨두도록 하겠습니다.\n\n\n[주의] 이 글은 친절한 디퓨전 모델 1편의 후속 글로 1편을 읽지 않고 읽으면 정확히 이해가 안될 수 있습니다.\n이 글에서 사용하는 데이터 셋은 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work에서 사용하는 16x16 크기를 가지는 이미지 스프라이트입니다.\n이 글은 다음 링크를 통해 구글 코랩에서 직접 실행하며 읽을 수 있습니다.\n\n\n\n\n가정 먼저 준비된 데이터 셋 파일을 다운 받습니다.\n\n!gdown 1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\nTo: /content/sprites_1788_16x16.npy\n100% 68.7M/68.7M [00:00&lt;00:00, 118MB/s]\n\n\n데이터 파일이 넘파이 어레이로 구성되어 있으므로 np.load()로 로딩합니다.\n\nsprites = np.load('sprites_1788_16x16.npy')\n\n데이터 셋은 넘파이 어레이이며 데이터 셋의 모양과 각 샘플의 최솟값, 최댓값을 확인해보면 개별 샘플은 모양이 (16,16,3)이고 0에서 255값을 가지는 이미지 어레이라는 것을 알 수 있습니다.\n\nsprites.shape, sprites.min(), sprites.max()\n\n((89400, 16, 16, 3), 0, 255)\n\n\n이미지의 크기와 채널수를 설정합니다.\n\nH = 16\nW = 16\nC = 3\n\n텐서플로는 케라스와 함께 사용할 때 다양한 방식으로 사용할 수 있습니다. 입력과 타겟을 넘파이 어레이 형태로 케라스에 바로 입력할 수도 있고 텐서플로 데이터 셋으로 만들어 입력할 수 도 있습니다. 전자는 데이터의 구조가 간단한 지도학습에 사용하기 알맞고 후자는 데이터에 조작을 가하는 경우 더 유리합니다.\n이 모델에서는 입력되는 원본 이미지에 노이즈를 추가하는 작업을 하고 타겟이 이미지가 아니라 노이즈가 되는 등 데이터를 입력할 때 처리해야 하는 작업이 조금 있습니다. 따라서 텐서플로 데이터 셋을 이용해서 작업하는 편이 더 좋습니다. 또 이렇게 해두는 편이 파이토치로 다시 구현할 때 비슷한 구조로 인해 작업이 더 용이하다는 장점도 있습니다. 파이토치 구현은 다음 글로 이어가겠습니다.\nsprites를 텐서플로 데이터셋으로 변환합니다.\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(sprites)\ntrain_ds\n\n&lt;TensorSliceDataset element_spec=TensorSpec(shape=(16, 16, 3), dtype=tf.uint8, name=None)&gt;\n\n\n만들어진 데이터 셋의 개별 샘플은 모양이 (16,16,3)임을 TensorSpec을 통해 알 수 있습니다. 시험삼아 샘플 여섯 개를 가져와서 그려봅니다.\n\nsamples = train_ds.take(6)\n\n반환된 samples는 (6,16,16,3)인 텐서이며 이 텐서플로 데이터셋을 for 루프에 걸어서 이미지를 받아올 수 있습니다.\n\n# (N,H,W,C)\nXs = np.array([X for i, X in enumerate(samples)])\n\n#                    (H,N,W,C)        (H,N*W,C)\nimages = Xs.transpose(1,0,2,3).reshape(H,-1,C)\n\nfig = plt.figure(figsize=(10,3))\nax = plt.axes()\n\nax.imshow(images)\nplt.show()\n\n\n\n\n샘플의 픽셀이 0에서 255값을 가지기 때문에 이를 -1에서 1로 변환합니다. 데이터 셋에 있는 모든 샘플에 적용할 변환 함수를 만듭니다.\n\n# 데이터셋에 map함수로 각 샘플에 적용될 변환 함수를 지정\ntrain_cast_ds = train_ds.map(\n    lambda image: (tf.cast(image, tf.float32)/255.0) * 2 - 1\n)\n\n변환이 잘 적용되는지 확인하기 위해 다시 여섯 개 샘플을 가져오고 이번에는 그리기 전에 역변환을 해서 그려보겠습니다.\n\nsamples = train_cast_ds.take(6)\n\n\n# (N,H,W,C)\nXs = np.array([X for i, X in enumerate(samples)])\n\n#                    (H,N,W,C)        (H,N*W,C)\nimages = Xs.transpose(1,0,2,3).reshape(H,-1,C)\n\n# -1~1을 0~255로 되돌림\nimgs = ((images + 1) / 2 * 255).astype(np.uint8)\n\nfig = plt.figure(figsize=(10,3))\nax = plt.axes()\n\nax.imshow(imgs)\nplt.show()\n\n\n\n\n정상적으로 그려지는걸 보니 변환과 역변환이 잘 적용되는것 같습니다.\n\n\n식(4)에 의해 \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\)이므로 임의의 시간단계 \\(t\\)에 대한 \\(\\mathbf{x}_t\\)를 바로 만들기 위해 모든 시간 단계에 대해서 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)를 계산합니다.\n\n# beta는 DDPM 원문의 설정을 따르고\nbeta_1 = 1e-4\nbeta_T = 0.02\n# 시간 단계는 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work의 설정을 따름\nT = 500\n\n# beta는 첨자 1부터 T까지 사용하기 위해 제일 앞에 더미 데이터 tf.constant([0.])를 추가하여 만듬\nbeta = tf.concat([tf.constant([0.], dtype=tf.float32), tf.linspace(beta_1, beta_T, T)], axis=0)\nalpha = 1 - beta\n\n# np.exp와 np.cumsum, np.log에 대응하는 TensorFlow 연산을 사용\nalpha_bar = tf.exp(tf.cumsum(tf.math.log(alpha)))\n\nbeta[1], beta[-1], alpha[1], alpha[-1], alpha_bar[1], alpha_bar[-1]\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=1e-04&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.02&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.9999&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.98&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.9999&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0063527143&gt;)\n\n\n계산된 \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)로 \\(\\sqrt{\\bar{\\alpha}_t}\\), \\(\\sqrt{1-\\bar{\\alpha}_t}\\)를 그려보면 시간 단계가 증가할 수 록 \\(\\mathbf{x}_t\\)는 가우시안 노이즈가 되는 것을 알 수 있습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(alpha_bar[1:].numpy(), label=r\"$\\sqrt{\\bar{\\alpha}_t}$\")\nax.plot(tf.sqrt(1-alpha_bar[1:]).numpy(), label=r\"$\\sqrt{1-\\bar{\\alpha}_t}$\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\)로 x로 부터 x_t를 계산하는 함수를 작성합니다. 이 함수는 데이터 셋에 있는 개별 샘플을 x로 입력받아 노이즈가 추가된 x_t를 반환합니다.\n\ndef perturb_x(x, t, eps):\n    # x: (H, W, C), 배치로 처리되지 않음\n\n    return tf.sqrt(alpha_bar[t]) * x + tf.sqrt(1 - alpha_bar[t]) * eps\n\nperturb_x()이용해서 텐서플로 데이터 셋에 적용할 노이즈 적용함수를 작성합니다.\n\ndef apply_perturb(x_0):\n    # sample time step t\n    t = tf.random.uniform(shape=[], minval=1, maxval=T+1, dtype=tf.int32)\n\n    # make x_t\n    eps = tf.random.normal(shape=x_0.shape)\n    x_t = perturb_x(x_0, t, eps)\n\n    # 원본 x_0, 노이즈낀 이미지 x_t와 타겟 eps, 시간단계 t 반환\n    return x_0, x_t, eps, t\n\n이 함수를 데이터 셋에 적용시키면 텐서플로 데이터 셋은 샘플을 리턴하기 직전에 이 함수를 작동시켜 하나의 샘플에 대해서 \\((\\mathbf{x}_0, \\mathbf{x}_t, \\boldsymbol{\\epsilon}, t)\\)를 리턴헤게 됩니다.\n\ntrain_cast_perturb_ds = train_cast_ds.map(apply_perturb)\n\n이제 다시 여섯개 데이터를 가져오고 그림을 그려봅니다.\n\nsamples = train_cast_perturb_ds.take(6)\n\n\nfig, axs = plt.subplots(figsize=(10,5), nrows=3, ncols=6)\n\n# 여기 for문을 돌면서 실제 map()이 적용되고 구체적인 데이터가 반환됨\nfor i, (x_0, x_t, eps, t) in enumerate(samples):\n    x_0 = x_0.numpy()\n    x_t = x_t.numpy()\n    eps = eps.numpy()\n\n    x_0 = ((x_0 - x_0.min()) / (x_0.max() - x_0.min())).clip(0,1)\n    axs[0][i].imshow(x_0)\n    axs[0][i].set_title(f\"t={t.numpy()}\")\n    axs[0][i].set_xticks([])\n    axs[0][i].set_yticks([])\n\n    eps = ((eps - eps.min()) / (eps.max() - eps.min())).clip(0,1)\n    axs[1][i].imshow(eps)\n    axs[1][i].set_xticks([])\n    axs[1][i].set_yticks([])\n\n    x_t = ((x_t - x_t.min()) / (x_t.max() - x_t.min())).clip(0,1)\n    axs[2][i].imshow(x_t)\n    axs[2][i].set_xticks([])\n    axs[2][i].set_yticks([])\n\n    if i == 0:\n        axs[0][i].set_ylabel('x_0')\n        axs[1][i].set_ylabel('eps')\n        axs[2][i].set_ylabel('x_t')\n\nplt.show()\n\n\n\n\n그림을 확인해보면 샘플 중 앞 여섯개에 임의의 시간 단계에 대한 노이즈가 적용되는 것을 알 수 있습니다. 첫 행은 원본 이미지, 둘째 행은 적용될 노이즈, 셋째 행은 노이즈가 적용된 모습을 나타냅니다. 시간 단계가 작을 수록 원본 이미지가 남아있고 200이 넘어가면 거의 알아볼 수 없게 될 것입니다.\n만들어진 데이터 셋에 셔플과 배치사이즈를 적용합니다.\n\nm = 64\n\ntrain_cast_perturb_ds_batch = train_cast_perturb_ds.shuffle(1000).batch(m)\n\n최종적으로 완성된 train_cast_perturb_ds_batch를 for문에 적용하여 한 배치 만큼 데이터를 가져와 봅니다.\n\nfor i, X in enumerate(train_cast_perturb_ds_batch):\n    print(X[0].shape, X[1].shape, X[2].shape, X[3])\n    break\n\n(64, 16, 16, 3) (64, 16, 16, 3) (64, 16, 16, 3) tf.Tensor(\n[ 81 495 196 179 166 147  57 212 364 326 305 173 391 145 435 474  79 294\n 252 377  68 491 234 482 391 437 331  95 210 140 443  42 484 363 183 489\n 327 408 291  96 338 212 474  60  50 143 364 479 266 105 211  53 171 438\n 491 453 223 118 394 398 342 268 349  67], shape=(64,), dtype=int32)\n\n\n순서대로 원본 이미지, 노이즈가 추가된 이미지, 노이즈가 64개 반환되고 마지막으로 각 시간 단계가 1차원 텐서로 반환되는것을 확인할 수 있습니다.\n\n\n\n\n\n정의한 모델을 학습시키기 위해선 알고리즘 1을 따라야 합니다. 케라스에서 제공하는 fit()의 잇점을 그대로 사용하면서 사용자 정의 학습 스탭을 적용시키기 위해 tf.keras.Model을 상속받고 학습 스탭만 재정의 하도록 합니다.\n아래 코드는 Model.fit의 동작 사용자 정의하기에서 제공하는 뼈대코드를 약간 수정하여 재사용한 것입니다.\n\nclass CustomModel(tf.keras.Model):\n    def train_step(self, data):\n        # data는 tf.data.Datset에서 반환하는 배치이므로\n        # x_0, x_t, eps, t로 언팩킹\n        x_0, x_t, eps, t = data\n\n        # 타겟을 노이즈로 설정\n        target = eps\n\n        with tf.GradientTape() as tape:\n            # Forward pass\n            # 시간 단계 숫자 t는 Dense층으로 입력되기 때문에 1차원 벡터 [t]로 변환\n            pred = self( [x_t, tf.reshape(t, (-1,1))], training=True)\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(target, pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(target, pred)\n\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n\n이제 모델을 정의 합니다. 가능하다면 (16,16,3) 크기의 이미지를 (768,) 크기의 벡터로 펼친 다음 완전 연결 네트워크로만 구성하고 싶었지만 실험 결과 이런 MLP 구조로는 의미있는 결과를 만들 수 없었습니다. 따라서 conv 레이어를 가지는 간단한 네트워크를 사용합니다. 원문에는 Unet 구조를 사용한다고 되어있지만 Unet도 복잡하기 때문에 Unet 특징이 약간 들어있는 아래 그림과 같은 네트워크를 사용하겠습니다.\n\n네트워크 구조는 매우 간단하여 그림과 아래 코드를 함께 보면 금방 이해가 갈 것입니다. 네트워크의 특징을 다음으로 요약할 수 있습니다.\n\n이미지에 Conv레이어를 몇번 적용하여 만들어진 특징 맵과 시간 단계에 Dense 레이어를 몇번 적용한 임베딩 벡터를 중간쯤에서 더하여 두 입력을 모두 반영하는 특징 맵을 만듭니다.\n특징 맵을 다시 이미지 모양으로 디코딩할 때 인코딩 과정에서 만들어 둔 특징 맵과 연결시키는 스킵 커넥션 구조를 한번 사용합니다.\n\n\ninput_t = tf.keras.Input(shape=(1), name=\"time_input\")\nt = tf.keras.layers.Dense(32, activation='relu')(input_t)\nt = tf.keras.layers.Dense(64, activation='relu')(t)\nt = tf.keras.layers.Reshape((1,1,64))(t)\n\ninput_image = tf.keras.Input(shape=(H,W,C), name=\"image_input\")\n\n# encoder\nx = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='gelu')(input_image)\nx_32 = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='gelu')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x_32)\nx = tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activation='gelu')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\nx = tf.keras.layers.Add()([x, t])\nx = tf.keras.layers.Conv2D(128, kernel_size=3, padding='same', activation='gelu')(x)\n\n# decoder\nx = tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='gelu', padding='same')(x)\nx = tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='gelu', padding='same')(x)\n# x와 x_32를 ch방향으로 concat\nx = tf.keras.layers.Concatenate(axis=-1)([x, x_32])\noutput = tf.keras.layers.Conv2D(3, kernel_size=3, padding='same', activation='linear')(x)\n\n# 정의된 입력과 출력으로 앞서 정의한 CustomModel을 생성\nmodel = CustomModel(inputs=[input_image, input_t], outputs=output)\n\n모델 서머리를 출력하고 구조를 그려 의도된 형태로 만들어졌는지 확인합니다.\n\nmodel.summary()\n\nModel: \"custom_model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n image_input (InputLayer)       [(None, 16, 16, 3)]  0           []                               \n                                                                                                  \n conv2d (Conv2D)                (None, 16, 16, 32)   896         ['image_input[0][0]']            \n                                                                                                  \n conv2d_1 (Conv2D)              (None, 16, 16, 32)   9248        ['conv2d[0][0]']                 \n                                                                                                  \n time_input (InputLayer)        [(None, 1)]          0           []                               \n                                                                                                  \n max_pooling2d (MaxPooling2D)   (None, 8, 8, 32)     0           ['conv2d_1[0][0]']               \n                                                                                                  \n dense (Dense)                  (None, 32)           64          ['time_input[0][0]']             \n                                                                                                  \n conv2d_2 (Conv2D)              (None, 8, 8, 64)     18496       ['max_pooling2d[0][0]']          \n                                                                                                  \n dense_1 (Dense)                (None, 64)           2112        ['dense[0][0]']                  \n                                                                                                  \n max_pooling2d_1 (MaxPooling2D)  (None, 4, 4, 64)    0           ['conv2d_2[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 1, 1, 64)     0           ['dense_1[0][0]']                \n                                                                                                  \n add (Add)                      (None, 4, 4, 64)     0           ['max_pooling2d_1[0][0]',        \n                                                                  'reshape[0][0]']                \n                                                                                                  \n conv2d_3 (Conv2D)              (None, 4, 4, 128)    73856       ['add[0][0]']                    \n                                                                                                  \n conv2d_transpose (Conv2DTransp  (None, 8, 8, 64)    73792       ['conv2d_3[0][0]']               \n ose)                                                                                             \n                                                                                                  \n conv2d_transpose_1 (Conv2DTran  (None, 16, 16, 32)  18464       ['conv2d_transpose[0][0]']       \n spose)                                                                                           \n                                                                                                  \n concatenate (Concatenate)      (None, 16, 16, 64)   0           ['conv2d_transpose_1[0][0]',     \n                                                                  'conv2d_1[0][0]']               \n                                                                                                  \n conv2d_4 (Conv2D)              (None, 16, 16, 3)    1731        ['concatenate[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 198,659\nTrainable params: 198,659\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\ntf.keras.utils.plot_model(model)\n\n\n\n\n모델을 완성했으면 앞서 만들어둔 데이터 셋을 이용해 포워드 테스트를 해봐야 합니다. 배치 사이즈 64이므로 한 배치를 네트워크에 입력하면 출력으로 (64,16,16,3)이 나와야 합니다.\n\n# forward test\nfor sample in train_cast_perturb_ds_batch.take(1):\n    x_0, x_t, eps, t = sample\n    output = model( [x_t, tf.reshape(t, (-1,1))]  )\n\noutput.shape\n\nTensorShape([64, 16, 16, 3])\n\n\n포워드 테스트가 성공했으므로 이제 학습 시키는 일만 남았습니다.\n\n\n\n\\[\nL_{\\text{simple}}(\\theta) := \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\left\\lVert \\boldsymbol{\\epsilon}- \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t) \\right\\rVert^2_2 \\right] \\tag{14}\n\\]\n위 손실 함수는 모든 샘플과 시간 단계에 대해서 MSE 손실 함수를 사용한다는 것을 의미합니다. 손실 함수를 식(14)처럼 MSE로 설정하고 적당한 옵티마이저를 지정하여 모델을 컴파일 합니다.\n\nloss_func = tf.keras.losses.MeanSquaredError()\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\n\nmodel.compile(optimizer=optimizer, loss=loss_func, metrics=['mae'])\n\n학습을 시작합니다.\n\nepochs = 30\n\nhistory = model.fit(train_cast_perturb_ds_batch, epochs=epochs, shuffle=True)\n\nEpoch 1/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.4195 - mae: 0.4659\nEpoch 2/30\n1397/1397 [==============================] - 15s 11ms/step - loss: 0.2561 - mae: 0.3607\nEpoch 3/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.2113 - mae: 0.3190\nEpoch 4/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1951 - mae: 0.3023\nEpoch 5/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1861 - mae: 0.2936\nEpoch 6/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1789 - mae: 0.2859\nEpoch 7/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1704 - mae: 0.2772\nEpoch 8/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1650 - mae: 0.2711\nEpoch 9/30\n1397/1397 [==============================] - 12s 9ms/step - loss: 0.1584 - mae: 0.2646\nEpoch 10/30\n1397/1397 [==============================] - 11s 8ms/step - loss: 0.1545 - mae: 0.2600\nEpoch 11/30\n1397/1397 [==============================] - 12s 9ms/step - loss: 0.1515 - mae: 0.2563\nEpoch 12/30\n1397/1397 [==============================] - 15s 11ms/step - loss: 0.1475 - mae: 0.2523\nEpoch 13/30\n1397/1397 [==============================] - 16s 12ms/step - loss: 0.1447 - mae: 0.2487\nEpoch 14/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1438 - mae: 0.2471\nEpoch 15/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1417 - mae: 0.2447\nEpoch 16/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1399 - mae: 0.2426\nEpoch 17/30\n1397/1397 [==============================] - 18s 13ms/step - loss: 0.1372 - mae: 0.2399\nEpoch 18/30\n1397/1397 [==============================] - 15s 11ms/step - loss: 0.1358 - mae: 0.2383\nEpoch 19/30\n1397/1397 [==============================] - 16s 12ms/step - loss: 0.1340 - mae: 0.2366\nEpoch 20/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.1326 - mae: 0.2352\nEpoch 21/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1329 - mae: 0.2350\nEpoch 22/30\n1397/1397 [==============================] - 18s 13ms/step - loss: 0.1308 - mae: 0.2330\nEpoch 23/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1304 - mae: 0.2324\nEpoch 24/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1294 - mae: 0.2312\nEpoch 25/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.1268 - mae: 0.2293\nEpoch 26/30\n1397/1397 [==============================] - 16s 12ms/step - loss: 0.1276 - mae: 0.2295\nEpoch 27/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1258 - mae: 0.2277\nEpoch 28/30\n1397/1397 [==============================] - 19s 13ms/step - loss: 0.1264 - mae: 0.2283\nEpoch 29/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1249 - mae: 0.2268\nEpoch 30/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1242 - mae: 0.2263\n\n\n\nplt.plot(history.history['loss'])\nplt.show()\n\n\n\n\n\n\n\n\n학습된 모델을 이용해서 샘플링 알고리즘을 따라 모든 시간 단계에 대한 샘플을 샘플링합니다.\n\n# 샘플링 단계동안 생성된 이미지를 일정 간격마다 저장할 리스트를 준비\ninterval = 20 # 20 시간 단계마다 한장씩 생성 결과 기록\nX = [] # 생성 이미지 저장\nsaved_frame = [] # 이미지를 저장한 시간 단계를 저장\nN = 5 # 모델에 입력할 샘플 개수\n\n# 최초 노이즈 샘플링\nx = tf.random.normal(shape=(N, H, W, C))\n\nfor t in range(T, 0, -1):\n    if t &gt; 1:\n        z = tf.random.normal(shape=(N,H,W,C))\n    else:\n        z = tf.zeros((N,H,W,C))\n\n    eps_theta = model([x, tf.reshape(t, (1,1))])\n    x = (1 / tf.sqrt(alpha[t])) * (x - ((1-alpha[t])/tf.sqrt(1-alpha_bar[t]))*eps_theta) + tf.sqrt(beta[t])*z\n\n    if (T - t) % interval == 0  or t == 1:\n        # 현재 시간 단계로 부터 생성되는 t-1번째 이미지를 저장\n        saved_frame.append(t)\n        x_np = x.numpy()\n\n        x_np = x_np.transpose(1,0,2,3).reshape(H,-1,C)\n        x_np = ((x_np - x_np.min()) / (x_np.max() - x_np.min())).clip(0,1)\n        X.append( x_np*255.0 ) # 0 ~ 1 -&gt; 0 ~ 255\n\nX = np.array(X, dtype=np.uint8)\n\n샘플링 과정동안 수집한 이미지를 애니메이션으로 표현합니다.\nplotly로 애니메이션을 표시하는 예제 코드는 이곳 https://plotly.com/python/animations/과 이곳 https://plotly.com/python/imshow/을 참고 하여 작성하였습니다.\n\nfig = go.Figure(\n    data = [ go.Image(z=X[0]) ],\n    layout = go.Layout(\n        # title=\"Generated image\",\n        autosize = False,\n        width = 800, height = 400,\n        margin = dict(l=0, r=0, b=0, t=30),\n        xaxis = {\"title\": f\"Generated Image: x_{T-1}\"},\n        updatemenus = [\n            dict(\n                type=\"buttons\",\n                buttons=[\n                    # play button\n                    dict(\n                        label=\"Play\", method=\"animate\",\n                        args=[\n                            None,\n                            {\n                                \"frame\": {\"duration\": 50, \"redraw\": True},\n                                \"fromcurrent\": True,\n                                \"transition\": {\"duration\": 50, \"easing\": \"quadratic-in-out\"}\n                            }\n                        ]\n                    ),\n                    # pause button\n                    dict(\n                        label=\"Pause\", method=\"animate\",\n                        args=[\n                            [None],\n                            {\n                                \"frame\": {\"duration\": 0, \"redraw\": False},\n                                \"mode\": \"immediate\",\n                                \"transition\": {\"duration\": 0}\n                            }\n                        ]\n                    )\n                ],\n                direction=\"left\", pad={\"r\": 10, \"t\": 87}, showactive=False,\n                x=0.1, xanchor=\"right\", y=0, yanchor=\"top\"\n            )\n        ], # updatemenus = [\n    ), # layout = go.Layout(\n    frames = [\n        {\n            'data':[go.Image(z=X[t])],\n            'name': t,\n            'layout': {\n                'xaxis': {'title': f\"Generated Image: x_{saved_frame[t]-1}\"}\n            }\n        } for t in range(len(X))\n    ]\n)\n\n################################################################################\n# 슬라이더 처리\nsliders_dict = {\n    \"active\": 0, \"yanchor\": \"top\", \"xanchor\": \"left\",\n    \"currentvalue\": {\n        \"font\": {\"size\": 15}, \"prefix\": \"input time:\",\n        \"visible\": True, \"xanchor\": \"right\"\n    },\n    \"transition\": {\"duration\": 100, \"easing\": \"cubic-in-out\"},\n    \"pad\": {\"b\": 10, \"t\": 50},\n    \"len\": 0.9, \"x\": 0.1, \"y\": 0,\n    \"steps\": []\n}\n\nfor t in range(len(X)):\n    slider_step = {\n        \"label\": f\"{saved_frame[t]}\", \"method\": \"animate\",\n        \"args\": [\n            [t], # frame 이름과 일치해야 연결됨\n            {\n                \"frame\": {\"duration\": 100, \"redraw\": True},\n                \"mode\": \"immediate\",\n                \"transition\": {\"duration\": 100}\n            }\n        ],\n    }\n\n    sliders_dict[\"steps\"].append(slider_step)\n\nfig[\"layout\"][\"sliders\"] = [sliders_dict]\n################################################################################\n\nfig.show()\n\n\n                                                \n\n\n위 그림에서 play를 누르거나 슬라이드 바를 이동 시키면 시간 단계에 따라 노이즈가 제거되는 이미지를 확인할 수 있습니다. 완전히 깨끗하고 정교하게 노이즈가 제거되진 않지만 무작위 노이즈로부터 게임 아이템과 캐릭터 처럼 생긴 물체들이 약 200단계에서부터 서서히 생겨나는것을 확인할 수 있습니다!\n간단한 네트워크로 충분히 납득할만한 결과를 생성할 수 있었습니다. 이제 여러분들이 네트워크를 더 정교하게 다듬어 생생한 캐릭터 이미지를 생성하시기 바랍니다.\n2-2편은 동일한 내용의 파이토치 구현편입니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-1.html#데이터-셋-준비",
    "href": "posts/diffusion/ddpm_part2-1.html#데이터-셋-준비",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-1 DDPM Hands-on with TensorFlow",
    "section": "",
    "text": "가정 먼저 준비된 데이터 셋 파일을 다운 받습니다.\n\n!gdown 1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\nTo: /content/sprites_1788_16x16.npy\n100% 68.7M/68.7M [00:00&lt;00:00, 118MB/s]\n\n\n데이터 파일이 넘파이 어레이로 구성되어 있으므로 np.load()로 로딩합니다.\n\nsprites = np.load('sprites_1788_16x16.npy')\n\n데이터 셋은 넘파이 어레이이며 데이터 셋의 모양과 각 샘플의 최솟값, 최댓값을 확인해보면 개별 샘플은 모양이 (16,16,3)이고 0에서 255값을 가지는 이미지 어레이라는 것을 알 수 있습니다.\n\nsprites.shape, sprites.min(), sprites.max()\n\n((89400, 16, 16, 3), 0, 255)\n\n\n이미지의 크기와 채널수를 설정합니다.\n\nH = 16\nW = 16\nC = 3\n\n텐서플로는 케라스와 함께 사용할 때 다양한 방식으로 사용할 수 있습니다. 입력과 타겟을 넘파이 어레이 형태로 케라스에 바로 입력할 수도 있고 텐서플로 데이터 셋으로 만들어 입력할 수 도 있습니다. 전자는 데이터의 구조가 간단한 지도학습에 사용하기 알맞고 후자는 데이터에 조작을 가하는 경우 더 유리합니다.\n이 모델에서는 입력되는 원본 이미지에 노이즈를 추가하는 작업을 하고 타겟이 이미지가 아니라 노이즈가 되는 등 데이터를 입력할 때 처리해야 하는 작업이 조금 있습니다. 따라서 텐서플로 데이터 셋을 이용해서 작업하는 편이 더 좋습니다. 또 이렇게 해두는 편이 파이토치로 다시 구현할 때 비슷한 구조로 인해 작업이 더 용이하다는 장점도 있습니다. 파이토치 구현은 다음 글로 이어가겠습니다.\nsprites를 텐서플로 데이터셋으로 변환합니다.\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(sprites)\ntrain_ds\n\n&lt;TensorSliceDataset element_spec=TensorSpec(shape=(16, 16, 3), dtype=tf.uint8, name=None)&gt;\n\n\n만들어진 데이터 셋의 개별 샘플은 모양이 (16,16,3)임을 TensorSpec을 통해 알 수 있습니다. 시험삼아 샘플 여섯 개를 가져와서 그려봅니다.\n\nsamples = train_ds.take(6)\n\n반환된 samples는 (6,16,16,3)인 텐서이며 이 텐서플로 데이터셋을 for 루프에 걸어서 이미지를 받아올 수 있습니다.\n\n# (N,H,W,C)\nXs = np.array([X for i, X in enumerate(samples)])\n\n#                    (H,N,W,C)        (H,N*W,C)\nimages = Xs.transpose(1,0,2,3).reshape(H,-1,C)\n\nfig = plt.figure(figsize=(10,3))\nax = plt.axes()\n\nax.imshow(images)\nplt.show()\n\n\n\n\n샘플의 픽셀이 0에서 255값을 가지기 때문에 이를 -1에서 1로 변환합니다. 데이터 셋에 있는 모든 샘플에 적용할 변환 함수를 만듭니다.\n\n# 데이터셋에 map함수로 각 샘플에 적용될 변환 함수를 지정\ntrain_cast_ds = train_ds.map(\n    lambda image: (tf.cast(image, tf.float32)/255.0) * 2 - 1\n)\n\n변환이 잘 적용되는지 확인하기 위해 다시 여섯 개 샘플을 가져오고 이번에는 그리기 전에 역변환을 해서 그려보겠습니다.\n\nsamples = train_cast_ds.take(6)\n\n\n# (N,H,W,C)\nXs = np.array([X for i, X in enumerate(samples)])\n\n#                    (H,N,W,C)        (H,N*W,C)\nimages = Xs.transpose(1,0,2,3).reshape(H,-1,C)\n\n# -1~1을 0~255로 되돌림\nimgs = ((images + 1) / 2 * 255).astype(np.uint8)\n\nfig = plt.figure(figsize=(10,3))\nax = plt.axes()\n\nax.imshow(imgs)\nplt.show()\n\n\n\n\n정상적으로 그려지는걸 보니 변환과 역변환이 잘 적용되는것 같습니다.\n\n\n식(4)에 의해 \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\)이므로 임의의 시간단계 \\(t\\)에 대한 \\(\\mathbf{x}_t\\)를 바로 만들기 위해 모든 시간 단계에 대해서 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)를 계산합니다.\n\n# beta는 DDPM 원문의 설정을 따르고\nbeta_1 = 1e-4\nbeta_T = 0.02\n# 시간 단계는 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work의 설정을 따름\nT = 500\n\n# beta는 첨자 1부터 T까지 사용하기 위해 제일 앞에 더미 데이터 tf.constant([0.])를 추가하여 만듬\nbeta = tf.concat([tf.constant([0.], dtype=tf.float32), tf.linspace(beta_1, beta_T, T)], axis=0)\nalpha = 1 - beta\n\n# np.exp와 np.cumsum, np.log에 대응하는 TensorFlow 연산을 사용\nalpha_bar = tf.exp(tf.cumsum(tf.math.log(alpha)))\n\nbeta[1], beta[-1], alpha[1], alpha[-1], alpha_bar[1], alpha_bar[-1]\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=1e-04&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.02&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.9999&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.98&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.9999&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0063527143&gt;)\n\n\n계산된 \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)로 \\(\\sqrt{\\bar{\\alpha}_t}\\), \\(\\sqrt{1-\\bar{\\alpha}_t}\\)를 그려보면 시간 단계가 증가할 수 록 \\(\\mathbf{x}_t\\)는 가우시안 노이즈가 되는 것을 알 수 있습니다.\n\nfig = plt.figure()\nax = plt.axes()\n\nax.plot(alpha_bar[1:].numpy(), label=r\"$\\sqrt{\\bar{\\alpha}_t}$\")\nax.plot(tf.sqrt(1-alpha_bar[1:]).numpy(), label=r\"$\\sqrt{1-\\bar{\\alpha}_t}$\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t } \\mathbf{x}_0, + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\)로 x로 부터 x_t를 계산하는 함수를 작성합니다. 이 함수는 데이터 셋에 있는 개별 샘플을 x로 입력받아 노이즈가 추가된 x_t를 반환합니다.\n\ndef perturb_x(x, t, eps):\n    # x: (H, W, C), 배치로 처리되지 않음\n\n    return tf.sqrt(alpha_bar[t]) * x + tf.sqrt(1 - alpha_bar[t]) * eps\n\nperturb_x()이용해서 텐서플로 데이터 셋에 적용할 노이즈 적용함수를 작성합니다.\n\ndef apply_perturb(x_0):\n    # sample time step t\n    t = tf.random.uniform(shape=[], minval=1, maxval=T+1, dtype=tf.int32)\n\n    # make x_t\n    eps = tf.random.normal(shape=x_0.shape)\n    x_t = perturb_x(x_0, t, eps)\n\n    # 원본 x_0, 노이즈낀 이미지 x_t와 타겟 eps, 시간단계 t 반환\n    return x_0, x_t, eps, t\n\n이 함수를 데이터 셋에 적용시키면 텐서플로 데이터 셋은 샘플을 리턴하기 직전에 이 함수를 작동시켜 하나의 샘플에 대해서 \\((\\mathbf{x}_0, \\mathbf{x}_t, \\boldsymbol{\\epsilon}, t)\\)를 리턴헤게 됩니다.\n\ntrain_cast_perturb_ds = train_cast_ds.map(apply_perturb)\n\n이제 다시 여섯개 데이터를 가져오고 그림을 그려봅니다.\n\nsamples = train_cast_perturb_ds.take(6)\n\n\nfig, axs = plt.subplots(figsize=(10,5), nrows=3, ncols=6)\n\n# 여기 for문을 돌면서 실제 map()이 적용되고 구체적인 데이터가 반환됨\nfor i, (x_0, x_t, eps, t) in enumerate(samples):\n    x_0 = x_0.numpy()\n    x_t = x_t.numpy()\n    eps = eps.numpy()\n\n    x_0 = ((x_0 - x_0.min()) / (x_0.max() - x_0.min())).clip(0,1)\n    axs[0][i].imshow(x_0)\n    axs[0][i].set_title(f\"t={t.numpy()}\")\n    axs[0][i].set_xticks([])\n    axs[0][i].set_yticks([])\n\n    eps = ((eps - eps.min()) / (eps.max() - eps.min())).clip(0,1)\n    axs[1][i].imshow(eps)\n    axs[1][i].set_xticks([])\n    axs[1][i].set_yticks([])\n\n    x_t = ((x_t - x_t.min()) / (x_t.max() - x_t.min())).clip(0,1)\n    axs[2][i].imshow(x_t)\n    axs[2][i].set_xticks([])\n    axs[2][i].set_yticks([])\n\n    if i == 0:\n        axs[0][i].set_ylabel('x_0')\n        axs[1][i].set_ylabel('eps')\n        axs[2][i].set_ylabel('x_t')\n\nplt.show()\n\n\n\n\n그림을 확인해보면 샘플 중 앞 여섯개에 임의의 시간 단계에 대한 노이즈가 적용되는 것을 알 수 있습니다. 첫 행은 원본 이미지, 둘째 행은 적용될 노이즈, 셋째 행은 노이즈가 적용된 모습을 나타냅니다. 시간 단계가 작을 수록 원본 이미지가 남아있고 200이 넘어가면 거의 알아볼 수 없게 될 것입니다.\n만들어진 데이터 셋에 셔플과 배치사이즈를 적용합니다.\n\nm = 64\n\ntrain_cast_perturb_ds_batch = train_cast_perturb_ds.shuffle(1000).batch(m)\n\n최종적으로 완성된 train_cast_perturb_ds_batch를 for문에 적용하여 한 배치 만큼 데이터를 가져와 봅니다.\n\nfor i, X in enumerate(train_cast_perturb_ds_batch):\n    print(X[0].shape, X[1].shape, X[2].shape, X[3])\n    break\n\n(64, 16, 16, 3) (64, 16, 16, 3) (64, 16, 16, 3) tf.Tensor(\n[ 81 495 196 179 166 147  57 212 364 326 305 173 391 145 435 474  79 294\n 252 377  68 491 234 482 391 437 331  95 210 140 443  42 484 363 183 489\n 327 408 291  96 338 212 474  60  50 143 364 479 266 105 211  53 171 438\n 491 453 223 118 394 398 342 268 349  67], shape=(64,), dtype=int32)\n\n\n순서대로 원본 이미지, 노이즈가 추가된 이미지, 노이즈가 64개 반환되고 마지막으로 각 시간 단계가 1차원 텐서로 반환되는것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-1.html#모델-만들기",
    "href": "posts/diffusion/ddpm_part2-1.html#모델-만들기",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-1 DDPM Hands-on with TensorFlow",
    "section": "",
    "text": "정의한 모델을 학습시키기 위해선 알고리즘 1을 따라야 합니다. 케라스에서 제공하는 fit()의 잇점을 그대로 사용하면서 사용자 정의 학습 스탭을 적용시키기 위해 tf.keras.Model을 상속받고 학습 스탭만 재정의 하도록 합니다.\n아래 코드는 Model.fit의 동작 사용자 정의하기에서 제공하는 뼈대코드를 약간 수정하여 재사용한 것입니다.\n\nclass CustomModel(tf.keras.Model):\n    def train_step(self, data):\n        # data는 tf.data.Datset에서 반환하는 배치이므로\n        # x_0, x_t, eps, t로 언팩킹\n        x_0, x_t, eps, t = data\n\n        # 타겟을 노이즈로 설정\n        target = eps\n\n        with tf.GradientTape() as tape:\n            # Forward pass\n            # 시간 단계 숫자 t는 Dense층으로 입력되기 때문에 1차원 벡터 [t]로 변환\n            pred = self( [x_t, tf.reshape(t, (-1,1))], training=True)\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(target, pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(target, pred)\n\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n\n이제 모델을 정의 합니다. 가능하다면 (16,16,3) 크기의 이미지를 (768,) 크기의 벡터로 펼친 다음 완전 연결 네트워크로만 구성하고 싶었지만 실험 결과 이런 MLP 구조로는 의미있는 결과를 만들 수 없었습니다. 따라서 conv 레이어를 가지는 간단한 네트워크를 사용합니다. 원문에는 Unet 구조를 사용한다고 되어있지만 Unet도 복잡하기 때문에 Unet 특징이 약간 들어있는 아래 그림과 같은 네트워크를 사용하겠습니다.\n\n네트워크 구조는 매우 간단하여 그림과 아래 코드를 함께 보면 금방 이해가 갈 것입니다. 네트워크의 특징을 다음으로 요약할 수 있습니다.\n\n이미지에 Conv레이어를 몇번 적용하여 만들어진 특징 맵과 시간 단계에 Dense 레이어를 몇번 적용한 임베딩 벡터를 중간쯤에서 더하여 두 입력을 모두 반영하는 특징 맵을 만듭니다.\n특징 맵을 다시 이미지 모양으로 디코딩할 때 인코딩 과정에서 만들어 둔 특징 맵과 연결시키는 스킵 커넥션 구조를 한번 사용합니다.\n\n\ninput_t = tf.keras.Input(shape=(1), name=\"time_input\")\nt = tf.keras.layers.Dense(32, activation='relu')(input_t)\nt = tf.keras.layers.Dense(64, activation='relu')(t)\nt = tf.keras.layers.Reshape((1,1,64))(t)\n\ninput_image = tf.keras.Input(shape=(H,W,C), name=\"image_input\")\n\n# encoder\nx = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='gelu')(input_image)\nx_32 = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='gelu')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x_32)\nx = tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activation='gelu')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\nx = tf.keras.layers.Add()([x, t])\nx = tf.keras.layers.Conv2D(128, kernel_size=3, padding='same', activation='gelu')(x)\n\n# decoder\nx = tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='gelu', padding='same')(x)\nx = tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='gelu', padding='same')(x)\n# x와 x_32를 ch방향으로 concat\nx = tf.keras.layers.Concatenate(axis=-1)([x, x_32])\noutput = tf.keras.layers.Conv2D(3, kernel_size=3, padding='same', activation='linear')(x)\n\n# 정의된 입력과 출력으로 앞서 정의한 CustomModel을 생성\nmodel = CustomModel(inputs=[input_image, input_t], outputs=output)\n\n모델 서머리를 출력하고 구조를 그려 의도된 형태로 만들어졌는지 확인합니다.\n\nmodel.summary()\n\nModel: \"custom_model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n image_input (InputLayer)       [(None, 16, 16, 3)]  0           []                               \n                                                                                                  \n conv2d (Conv2D)                (None, 16, 16, 32)   896         ['image_input[0][0]']            \n                                                                                                  \n conv2d_1 (Conv2D)              (None, 16, 16, 32)   9248        ['conv2d[0][0]']                 \n                                                                                                  \n time_input (InputLayer)        [(None, 1)]          0           []                               \n                                                                                                  \n max_pooling2d (MaxPooling2D)   (None, 8, 8, 32)     0           ['conv2d_1[0][0]']               \n                                                                                                  \n dense (Dense)                  (None, 32)           64          ['time_input[0][0]']             \n                                                                                                  \n conv2d_2 (Conv2D)              (None, 8, 8, 64)     18496       ['max_pooling2d[0][0]']          \n                                                                                                  \n dense_1 (Dense)                (None, 64)           2112        ['dense[0][0]']                  \n                                                                                                  \n max_pooling2d_1 (MaxPooling2D)  (None, 4, 4, 64)    0           ['conv2d_2[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 1, 1, 64)     0           ['dense_1[0][0]']                \n                                                                                                  \n add (Add)                      (None, 4, 4, 64)     0           ['max_pooling2d_1[0][0]',        \n                                                                  'reshape[0][0]']                \n                                                                                                  \n conv2d_3 (Conv2D)              (None, 4, 4, 128)    73856       ['add[0][0]']                    \n                                                                                                  \n conv2d_transpose (Conv2DTransp  (None, 8, 8, 64)    73792       ['conv2d_3[0][0]']               \n ose)                                                                                             \n                                                                                                  \n conv2d_transpose_1 (Conv2DTran  (None, 16, 16, 32)  18464       ['conv2d_transpose[0][0]']       \n spose)                                                                                           \n                                                                                                  \n concatenate (Concatenate)      (None, 16, 16, 64)   0           ['conv2d_transpose_1[0][0]',     \n                                                                  'conv2d_1[0][0]']               \n                                                                                                  \n conv2d_4 (Conv2D)              (None, 16, 16, 3)    1731        ['concatenate[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 198,659\nTrainable params: 198,659\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\ntf.keras.utils.plot_model(model)\n\n\n\n\n모델을 완성했으면 앞서 만들어둔 데이터 셋을 이용해 포워드 테스트를 해봐야 합니다. 배치 사이즈 64이므로 한 배치를 네트워크에 입력하면 출력으로 (64,16,16,3)이 나와야 합니다.\n\n# forward test\nfor sample in train_cast_perturb_ds_batch.take(1):\n    x_0, x_t, eps, t = sample\n    output = model( [x_t, tf.reshape(t, (-1,1))]  )\n\noutput.shape\n\nTensorShape([64, 16, 16, 3])\n\n\n포워드 테스트가 성공했으므로 이제 학습 시키는 일만 남았습니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-1.html#학습",
    "href": "posts/diffusion/ddpm_part2-1.html#학습",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-1 DDPM Hands-on with TensorFlow",
    "section": "",
    "text": "\\[\nL_{\\text{simple}}(\\theta) := \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\left\\lVert \\boldsymbol{\\epsilon}- \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t) \\right\\rVert^2_2 \\right] \\tag{14}\n\\]\n위 손실 함수는 모든 샘플과 시간 단계에 대해서 MSE 손실 함수를 사용한다는 것을 의미합니다. 손실 함수를 식(14)처럼 MSE로 설정하고 적당한 옵티마이저를 지정하여 모델을 컴파일 합니다.\n\nloss_func = tf.keras.losses.MeanSquaredError()\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\n\nmodel.compile(optimizer=optimizer, loss=loss_func, metrics=['mae'])\n\n학습을 시작합니다.\n\nepochs = 30\n\nhistory = model.fit(train_cast_perturb_ds_batch, epochs=epochs, shuffle=True)\n\nEpoch 1/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.4195 - mae: 0.4659\nEpoch 2/30\n1397/1397 [==============================] - 15s 11ms/step - loss: 0.2561 - mae: 0.3607\nEpoch 3/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.2113 - mae: 0.3190\nEpoch 4/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1951 - mae: 0.3023\nEpoch 5/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1861 - mae: 0.2936\nEpoch 6/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1789 - mae: 0.2859\nEpoch 7/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1704 - mae: 0.2772\nEpoch 8/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1650 - mae: 0.2711\nEpoch 9/30\n1397/1397 [==============================] - 12s 9ms/step - loss: 0.1584 - mae: 0.2646\nEpoch 10/30\n1397/1397 [==============================] - 11s 8ms/step - loss: 0.1545 - mae: 0.2600\nEpoch 11/30\n1397/1397 [==============================] - 12s 9ms/step - loss: 0.1515 - mae: 0.2563\nEpoch 12/30\n1397/1397 [==============================] - 15s 11ms/step - loss: 0.1475 - mae: 0.2523\nEpoch 13/30\n1397/1397 [==============================] - 16s 12ms/step - loss: 0.1447 - mae: 0.2487\nEpoch 14/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1438 - mae: 0.2471\nEpoch 15/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1417 - mae: 0.2447\nEpoch 16/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1399 - mae: 0.2426\nEpoch 17/30\n1397/1397 [==============================] - 18s 13ms/step - loss: 0.1372 - mae: 0.2399\nEpoch 18/30\n1397/1397 [==============================] - 15s 11ms/step - loss: 0.1358 - mae: 0.2383\nEpoch 19/30\n1397/1397 [==============================] - 16s 12ms/step - loss: 0.1340 - mae: 0.2366\nEpoch 20/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.1326 - mae: 0.2352\nEpoch 21/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1329 - mae: 0.2350\nEpoch 22/30\n1397/1397 [==============================] - 18s 13ms/step - loss: 0.1308 - mae: 0.2330\nEpoch 23/30\n1397/1397 [==============================] - 14s 10ms/step - loss: 0.1304 - mae: 0.2324\nEpoch 24/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1294 - mae: 0.2312\nEpoch 25/30\n1397/1397 [==============================] - 16s 11ms/step - loss: 0.1268 - mae: 0.2293\nEpoch 26/30\n1397/1397 [==============================] - 16s 12ms/step - loss: 0.1276 - mae: 0.2295\nEpoch 27/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1258 - mae: 0.2277\nEpoch 28/30\n1397/1397 [==============================] - 19s 13ms/step - loss: 0.1264 - mae: 0.2283\nEpoch 29/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1249 - mae: 0.2268\nEpoch 30/30\n1397/1397 [==============================] - 17s 12ms/step - loss: 0.1242 - mae: 0.2263\n\n\n\nplt.plot(history.history['loss'])\nplt.show()"
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-1.html#샘플링",
    "href": "posts/diffusion/ddpm_part2-1.html#샘플링",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-1 DDPM Hands-on with TensorFlow",
    "section": "",
    "text": "학습된 모델을 이용해서 샘플링 알고리즘을 따라 모든 시간 단계에 대한 샘플을 샘플링합니다.\n\n# 샘플링 단계동안 생성된 이미지를 일정 간격마다 저장할 리스트를 준비\ninterval = 20 # 20 시간 단계마다 한장씩 생성 결과 기록\nX = [] # 생성 이미지 저장\nsaved_frame = [] # 이미지를 저장한 시간 단계를 저장\nN = 5 # 모델에 입력할 샘플 개수\n\n# 최초 노이즈 샘플링\nx = tf.random.normal(shape=(N, H, W, C))\n\nfor t in range(T, 0, -1):\n    if t &gt; 1:\n        z = tf.random.normal(shape=(N,H,W,C))\n    else:\n        z = tf.zeros((N,H,W,C))\n\n    eps_theta = model([x, tf.reshape(t, (1,1))])\n    x = (1 / tf.sqrt(alpha[t])) * (x - ((1-alpha[t])/tf.sqrt(1-alpha_bar[t]))*eps_theta) + tf.sqrt(beta[t])*z\n\n    if (T - t) % interval == 0  or t == 1:\n        # 현재 시간 단계로 부터 생성되는 t-1번째 이미지를 저장\n        saved_frame.append(t)\n        x_np = x.numpy()\n\n        x_np = x_np.transpose(1,0,2,3).reshape(H,-1,C)\n        x_np = ((x_np - x_np.min()) / (x_np.max() - x_np.min())).clip(0,1)\n        X.append( x_np*255.0 ) # 0 ~ 1 -&gt; 0 ~ 255\n\nX = np.array(X, dtype=np.uint8)\n\n샘플링 과정동안 수집한 이미지를 애니메이션으로 표현합니다.\nplotly로 애니메이션을 표시하는 예제 코드는 이곳 https://plotly.com/python/animations/과 이곳 https://plotly.com/python/imshow/을 참고 하여 작성하였습니다.\n\nfig = go.Figure(\n    data = [ go.Image(z=X[0]) ],\n    layout = go.Layout(\n        # title=\"Generated image\",\n        autosize = False,\n        width = 800, height = 400,\n        margin = dict(l=0, r=0, b=0, t=30),\n        xaxis = {\"title\": f\"Generated Image: x_{T-1}\"},\n        updatemenus = [\n            dict(\n                type=\"buttons\",\n                buttons=[\n                    # play button\n                    dict(\n                        label=\"Play\", method=\"animate\",\n                        args=[\n                            None,\n                            {\n                                \"frame\": {\"duration\": 50, \"redraw\": True},\n                                \"fromcurrent\": True,\n                                \"transition\": {\"duration\": 50, \"easing\": \"quadratic-in-out\"}\n                            }\n                        ]\n                    ),\n                    # pause button\n                    dict(\n                        label=\"Pause\", method=\"animate\",\n                        args=[\n                            [None],\n                            {\n                                \"frame\": {\"duration\": 0, \"redraw\": False},\n                                \"mode\": \"immediate\",\n                                \"transition\": {\"duration\": 0}\n                            }\n                        ]\n                    )\n                ],\n                direction=\"left\", pad={\"r\": 10, \"t\": 87}, showactive=False,\n                x=0.1, xanchor=\"right\", y=0, yanchor=\"top\"\n            )\n        ], # updatemenus = [\n    ), # layout = go.Layout(\n    frames = [\n        {\n            'data':[go.Image(z=X[t])],\n            'name': t,\n            'layout': {\n                'xaxis': {'title': f\"Generated Image: x_{saved_frame[t]-1}\"}\n            }\n        } for t in range(len(X))\n    ]\n)\n\n################################################################################\n# 슬라이더 처리\nsliders_dict = {\n    \"active\": 0, \"yanchor\": \"top\", \"xanchor\": \"left\",\n    \"currentvalue\": {\n        \"font\": {\"size\": 15}, \"prefix\": \"input time:\",\n        \"visible\": True, \"xanchor\": \"right\"\n    },\n    \"transition\": {\"duration\": 100, \"easing\": \"cubic-in-out\"},\n    \"pad\": {\"b\": 10, \"t\": 50},\n    \"len\": 0.9, \"x\": 0.1, \"y\": 0,\n    \"steps\": []\n}\n\nfor t in range(len(X)):\n    slider_step = {\n        \"label\": f\"{saved_frame[t]}\", \"method\": \"animate\",\n        \"args\": [\n            [t], # frame 이름과 일치해야 연결됨\n            {\n                \"frame\": {\"duration\": 100, \"redraw\": True},\n                \"mode\": \"immediate\",\n                \"transition\": {\"duration\": 100}\n            }\n        ],\n    }\n\n    sliders_dict[\"steps\"].append(slider_step)\n\nfig[\"layout\"][\"sliders\"] = [sliders_dict]\n################################################################################\n\nfig.show()\n\n\n                                                \n\n\n위 그림에서 play를 누르거나 슬라이드 바를 이동 시키면 시간 단계에 따라 노이즈가 제거되는 이미지를 확인할 수 있습니다. 완전히 깨끗하고 정교하게 노이즈가 제거되진 않지만 무작위 노이즈로부터 게임 아이템과 캐릭터 처럼 생긴 물체들이 약 200단계에서부터 서서히 생겨나는것을 확인할 수 있습니다!\n간단한 네트워크로 충분히 납득할만한 결과를 생성할 수 있었습니다. 이제 여러분들이 네트워크를 더 정교하게 다듬어 생생한 캐릭터 이미지를 생성하시기 바랍니다.\n2-2편은 동일한 내용의 파이토치 구현편입니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-2.html",
    "href": "posts/diffusion/ddpm_part2-2.html",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-2 DDPM Hands-on with Pytorch",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom tqdm import tqdm\n\nimport plotly.io\nimport plotly.graph_objects as go\n\n# colab: coloab\n# jupyter lab: jupyterlab\n# jupyter notebook, quarto blog: notebook\nplotly.io.renderers.default = \"notebook\"\n\n이전 글에서 DDPM에 대한 이론을 자세히 알아 봤습니다. 2-1편에 이어 2-2편에서는 파이토치를 사용해서 DDPM 모델을 직접 만들어봅니다. DDPM을 구현한 많은 참고 자료들이 인터넷에 많으나 이론에 나오는 수식을 1:1 방식으로 간결하게 코드로 매치 시키는 구현은 찾아보기 힘듭니다. DDPM 공식 구현 코드도 간결하고 읽기 좋게 작성된 코드는 아닙니다. 좋은 품질의 이미지를 생성하기 위해 복잡한 네트워크 구조를 사용하고 다양한 실험 옵션을 반영하기 때문인데 이런 코드는 학습용 코드로는 부적합 합니다.\n이런 이유로 본 글에서는 최소한 납득할 만한 결과를 보여 주면서도 가능한 간단한 구조로 된 모델을 정의해 사용할 것입니다. 이를 통해 쉽고 빠르게 이론과 실제 코드를 연결하여 이해할 수 있도록 구성했습니다. 더 정교한 이미지를 생성하기 위한 복잡한 네트워크의 적용을 독자 여러분의 몫으로 남겨두도록 하겠습니다.\n\n\n[주의] 이 글은 친절한 디퓨전 모델 1편의 후속 글로 1편을 읽지 않고 읽으면 정확히 이해가 안될 수 있습니다.\n이 글에서 사용하는 데이터 셋은 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work에서 사용하는 16x16 크기를 가지는 이미지 스프라이트입니다.\n이 글은 다음 링크를 통해 구글 코랩에서 직접 실행하며 읽을 수 있습니다.\n\n\n\n# 사용 디바이스 세팅\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\n\n\n가정 먼저 준비된 데이터 셋 파일을 다운 받습니다.\n\n!gdown 1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\n\n데이터 파일이 넘파이 어레이로 구성되어 있으므로 np.load()로 로딩합니다.\n\nsprites = np.load('sprites_1788_16x16.npy')\n\n데이터 셋은 넘파이 어레이이며 데이터 셋의 모양과 각 샘플의 최솟값, 최댓값을 확인해보면 개별 샘플은 모양이 (16,16,3)이고 0에서 255값을 가지는 이미지 어레이라는 것을 알 수 있습니다.\n\nsprites.shape, sprites.min(), sprites.max()\n\n((89400, 16, 16, 3), 0, 255)\n\n\n이미지의 크기와 채널수를 설정합니다.\n\nH = 16\nW = 16\nC = 3\n\n\n\n이미지를 한장씩 반환하는 데이터셋 클래스 MyDataset을 정의 합니다. 데이터셋 내부에서 이미지에 대한 일련의 transform을 적용하고 나서 노이즈 추가을 진행하기 위해 식(4)에서 사용하는 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)를 클래스 변수로 계산해 둡니다.\n\nclass MyDataset(Dataset):\n    # beta는 DDPM 원문의 설정을 따르고\n    beta_1 = 1e-4\n    beta_T = 0.02\n    # 시간 단계는 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work의 설정을 따름\n    T = 500\n\n    # beta는 첨자 1부터 T까지 사용하기 위해 제일 앞에 더미 데이터 tf.constant([0.])를 추가하여 만듬\n    beta = torch.cat([ torch.tensor([0]), torch.linspace(beta_1, beta_T, T)], axis=0)\n    alpha = 1 - beta\n    alpha_bar = torch.exp(torch.cumsum(torch.log(alpha), axis=0))\n\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, i):\n        x_0 = self.data[i]\n\n        # normalize -1~1로 만들기\n        if self.transform:\n            x_0 = self.transform(x_0)\n\n        # noise 추가\n        t = np.random.randint(1, MyDataset.T+1)\n        eps = torch.randn_like(x_0)\n        x_t = torch.sqrt(MyDataset.alpha_bar[t]) * x_0 + torch.sqrt(1 - MyDataset.alpha_bar[t]) * eps\n\n        return x_0, x_t, eps, t\n\n이미지 픽셀값을 -1~1사이로 변환하는 transform을 구성하고 데이터 셋을 생성합니다. 계속해서 생성된 데이터 셋을 이용하는 데이터 로더도 준비합니다. 배치사이즈는 텐서플로 구현과 같게 64로 지정합니다.\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),                # from [0,255] to range [0.0, 1.0]\n    transforms.Normalize((0.5,), (0.5,))  # range [-1,1]\n])\n\ntrain_ds = MyDataset(sprites, transform)\n\nm = 64\n\ntrain_loader = DataLoader(train_ds, batch_size=m, shuffle=True)\ntrain_loader_iter = iter(train_loader)\n\n데이터 로더로 부터 미니배치를 받아와 그림을 그려봅니다.\n\nsamples = next(train_loader_iter)\n\nx_0s = samples[0][:6].numpy()\nx_ts = samples[1][:6].numpy()\nepss = samples[2][:6].numpy()\nts =  samples[3][:6].numpy()\n\n\nfig, axs = plt.subplots(figsize=(10,5), nrows=3, ncols=6)\n\ni = 0\nfor (x_0, x_t, eps, t) in zip(x_0s, x_ts, epss, ts):\n    x_0 = x_0.transpose(1,2,0)\n    x_0 = ((x_0 - x_0.min()) / (x_0.max() - x_0.min())).clip(0,1)\n    axs[0][i].imshow(x_0)\n    axs[0][i].set_title(f\"t={t}\")\n    axs[0][i].set_xticks([])\n    axs[0][i].set_yticks([])\n\n    eps = eps.transpose(1,2,0)\n    eps = ((eps - eps.min()) / (eps.max() - eps.min())).clip(0,1)\n    axs[1][i].imshow(eps)\n    axs[1][i].set_xticks([])\n    axs[1][i].set_yticks([])\n\n    x_t = x_t.transpose(1,2,0)\n    x_t = ((x_t - x_t.min()) / (x_t.max() - x_t.min())).clip(0,1)\n    axs[2][i].imshow(x_t)\n    axs[2][i].set_xticks([])\n    axs[2][i].set_yticks([])\n\n    if i == 0:\n        axs[0][i].set_ylabel('x_0')\n        axs[1][i].set_ylabel('eps')\n        axs[2][i].set_ylabel('x_t')\n\n    i += 1\n\nplt.show()\n\n\n\n\n그림을 확인해보면 샘플 중 앞 여섯개에 임의의 시간 단계에 대한 노이즈가 적용되는 것을 알 수 있습니다. 첫 행은 원본 이미지, 둘째 행은 적용될 노이즈, 셋째 행은 노이즈가 적용된 모습을 나타냅니다. 시간 단계가 작을 수록 원본 이미지가 남아있고 200이 넘어가면 거의 알아볼 수 없게 될 것입니다.\n만들어진 데이터 셋에 셔플과 배치사이즈를 적용합니다.\n\n\n\n\n이제 모델을 정의 합니다. 가능하다면 (3,16,16) 크기의 이미지를 (768,) 크기의 벡터로 펼친 다음 완전 연결 네트워크로만 구성하고 싶었지만 실험 결과 이런 MLP 구조로는 의미있는 결과를 만들 수 없었습니다. 따라서 conv 레이어를 가지는 간단한 네트워크를 사용합니다. 원문에는 Unet 구조를 사용한다고 되어있지만 Unet도 복잡하기 때문에 Unet 특징이 약간 들어있는 아래 그림과 같은 네트워크를 사용하겠습니다.\n\n네트워크 구조는 매우 간단하여 그림과 아래 코드를 함께 보면 금방 이해가 갈 것입니다. 네트워크의 특징을 다음으로 요약할 수 있습니다.\n\n이미지에 Conv레이어를 몇번 적용하여 만들어진 특징 맵과 시간 단계에 Dense 레이어를 몇번 적용한 임베딩 벡터를 중간쯤에서 더하여 두 입력을 모두 반영하는 특징 맵을 만듭니다.\n특징 맵을 다시 이미지 모양으로 디코딩할 때 인코딩 과정에서 만들어 둔 특징 맵과 연결시키는 스킵 커넥션 구조를 한번 사용합니다.\n\n\nclass DDPM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.emb_1 = torch.nn.Linear(in_features=1, out_features=32)\n        self.emb_2 = torch.nn.Linear(in_features=32, out_features=64)\n\n        self.down_conv1_32 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        self.down_conv2_32 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n        self.down_conv3_64 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.down_conv4_128 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n\n        self.up_conv1_64 = torch.nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1)\n        self.up_conv2_32 = torch.nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1)\n        self.up_conv3_32 = torch.nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1)\n\n        self.relu = torch.nn.ReLU()\n        self.gelu = torch.nn.GELU()\n\n    def forward(self, x, t):\n        # x: (N, C, H, W)\n        # t: (N,1)\n        batch_size = t.shape[0]\n\n        # time embedding\n        t = self.relu( self.emb_1(t) ) # (N, 32)\n        t = self.relu( self.emb_2(t) ) # (N, 64)\n        t = t.reshape(batch_size, -1, 1, 1) # (N, 64, 1, 1)\n\n        # image down conv\n        x = self.gelu( self.down_conv1_32(x) )    # (N, 32, 16, 16)\n        x_32 = self.gelu( self.down_conv2_32(x) ) # (N, 32, 16, 16)\n        size_32 = x_32.shape\n        x = torch.nn.functional.max_pool2d(x_32, (2,2)) # (N, 32, 8, 8)\n        x = self.gelu( self.down_conv3_64(x) ) # (N, 64, 8, 8)\n        size_64 = x.shape\n        x = torch.nn.functional.max_pool2d(x, (2,2)) # (N, 64, 4, 4)\n\n        x = x + t # (N, 64, 4, 4) + (N, 64, 1, 1) = (N, 64, 4, 4)\n        x = self.gelu( self.down_conv4_128(x) ) # (N, 128, 4, 4)\n\n        # image up conv\n        x = self.gelu( self.up_conv1_64(x, output_size=size_64) ) # (N, 64, 8, 8)\n        x = self.gelu( self.up_conv2_32(x, output_size=size_32) ) # (N, 32, 16, 16)\n        x = torch.concat([x, x_32], axis=1) # (N, 64, 16, 16)\n        out = self.up_conv3_32(x) # (N, 3, 16, 16)\n\n        return out\n\n모델을 만들고 사용 다바이스로 모델을 이동시킵니다.\n\nmodel = DDPM()\nmodel.to(device)\n\nDDPM(\n  (emb_1): Linear(in_features=1, out_features=32, bias=True)\n  (emb_2): Linear(in_features=32, out_features=64, bias=True)\n  (down_conv1_32): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (down_conv2_32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (down_conv3_64): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (down_conv4_128): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (up_conv1_64): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (up_conv2_32): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (up_conv3_32): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu): ReLU()\n  (gelu): GELU(approximate='none')\n)\n\n\n모델을 완성했으면 앞서 만들어둔 데이터 셋을 이용해 포워드 테스트를 해봐야 합니다. 배치 사이즈 64이므로 한 배치를 네트워크에 입력하면 출력으로 (64,3,16,16)이 나와야 합니다.\n\noutput = model(samples[0].to(device), samples[3].reshape(-1,1).float().to(device))\n\nprint(output.shape)\n\ntorch.Size([64, 3, 16, 16])\n\n\n포워드 테스트가 성공했으므로 이제 학습 시키는 일만 남았습니다.\n\n\n\n\\[\nL_{\\text{simple}}(\\theta) := \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\left\\lVert \\boldsymbol{\\epsilon}- \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t) \\right\\rVert^2_2 \\right] \\tag{14}\n\\]\n위 손실 함수는 모든 샘플과 시간 단계에 대해서 MSE 손실 함수를 사용한다는 것을 의미합니다. 손실 함수를 식(14)처럼 MSE로 설정하고 적당한 옵티마이저를 생성합니다.\n\nloss_func = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n다음은 학습 루프를 구현할 차례입니다.\n\n위 그림에 나타난 학습 알고리즘을 직접 구현합니다. 학습 루프에서 mae 매트릭도 함께 계산해서 화면에 출력해 줍니다.\n\nepochs = 30\nlosses = []\n\nfor e in range(epochs):\n    epoch_loss = 0.0\n    epoch_mae = 0.0\n\n    for i, data in enumerate(tqdm(train_loader)):\n        x_0, x_t, eps, t = data\n        x_t = x_t.to(device)\n        eps = eps.to(device)\n        t = t.to(device)\n\n        optimizer.zero_grad()\n        eps_theta = model(x_t, t.reshape(-1,1).float())\n        loss = loss_func(eps_theta, eps)\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            epoch_loss += loss.item()\n            epoch_mae += torch.nn.functional.l1_loss(eps_theta, eps)\n\n    epoch_loss /= len(train_loader)\n    epoch_mae /= len(train_loader)\n\n    print(f\"Epoch: {e+1:2d}: loss:{epoch_loss:.4f}, mae:{epoch_mae:.4f}\")\n    losses.append(epoch_loss)\n\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 193.73it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.67it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 199.00it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 204.76it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.65it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.85it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.54it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 207.12it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.49it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.28it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.37it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 206.47it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 204.66it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.35it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.87it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 201.97it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.26it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 199.27it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 206.42it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 199.97it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.68it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 198.86it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 199.91it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 203.76it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.81it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 207.95it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.44it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 207.08it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 204.23it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.92it/s]\n\n\nEpoch:  1: loss:0.3821, mae:0.4512\nEpoch:  2: loss:0.2485, mae:0.3536\nEpoch:  3: loss:0.2037, mae:0.3103\nEpoch:  4: loss:0.1852, mae:0.2918\nEpoch:  5: loss:0.1719, mae:0.2790\nEpoch:  6: loss:0.1661, mae:0.2724\nEpoch:  7: loss:0.1602, mae:0.2661\nEpoch:  8: loss:0.1550, mae:0.2602\nEpoch:  9: loss:0.1530, mae:0.2575\nEpoch: 10: loss:0.1504, mae:0.2545\nEpoch: 11: loss:0.1468, mae:0.2506\nEpoch: 12: loss:0.1451, mae:0.2487\nEpoch: 13: loss:0.1425, mae:0.2459\nEpoch: 14: loss:0.1414, mae:0.2445\nEpoch: 15: loss:0.1385, mae:0.2418\nEpoch: 16: loss:0.1372, mae:0.2402\nEpoch: 17: loss:0.1361, mae:0.2391\nEpoch: 18: loss:0.1359, mae:0.2385\nEpoch: 19: loss:0.1341, mae:0.2365\nEpoch: 20: loss:0.1331, mae:0.2355\nEpoch: 21: loss:0.1327, mae:0.2348\nEpoch: 22: loss:0.1312, mae:0.2336\nEpoch: 23: loss:0.1290, mae:0.2316\nEpoch: 24: loss:0.1299, mae:0.2319\nEpoch: 25: loss:0.1289, mae:0.2310\nEpoch: 26: loss:0.1277, mae:0.2299\nEpoch: 27: loss:0.1275, mae:0.2298\nEpoch: 28: loss:0.1266, mae:0.2286\nEpoch: 29: loss:0.1272, mae:0.2290\nEpoch: 30: loss:0.1250, mae:0.2271\n\n\n\nplt.plot(losses)\nplt.show()\n\n\n\n\n\n\n\n\n학습된 모델을 이용해서 샘플링 알고리즘을 따라 모든 시간 단계에 대한 샘플을 샘플링합니다.\n샘플링 과정에서 사용할 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)를 클래스 외부 변수로 빼둡니다.\n\nalpha = MyDataset.alpha.to(device)\nalpha_bar = MyDataset.alpha_bar.to(device)\nbeta = MyDataset.beta.to(device)\nT = MyDataset.T\n\n\n# 샘플링 단계동안 생성된 이미지를 일정 간격마다 저장할 리스트를 준비\ninterval = 20 # 20 시간 단계마다 한장씩 생성 결과 기록\nX = [] # 생성 이미지 저장\nsaved_frame = [] # 이미지를 저장한 시간 단계를 저장\nN = 5 # 모델에 입력할 샘플 개수\n\n\n# 최초 노이즈 샘플링\nx = torch.randn(size=(N, C, H, W)).to(device)\n\nfor t in range(T, 0, -1):\n    if t &gt; 1:\n        z = torch.randn(size=(N,C,H,W)).to(device)\n    else:\n        z = torch.zeros((N,C,H,W)).to(device)\n\n    t_torch = torch.tensor([[t]]*N, dtype=torch.float32).to(device)\n    eps_theta = model(x, t_torch)\n    x = (1 / torch.sqrt(alpha[t])) * \\\n        (x - ((1-alpha[t])/torch.sqrt(1-alpha_bar[t]))*eps_theta) + torch.sqrt(beta[t])*z\n\n    if (T - t) % interval == 0  or t == 1:\n        # 현재 시간 단계로 부터 생성되는 t-1번째 이미지를 저장\n        saved_frame.append(t)\n        x_np = x.detach().cpu().numpy()\n\n        # (N,C,H,W)-&gt;(H,N,W,C)\n        x_np = x_np.transpose(2,0,3,1).reshape(H,-1,C)\n        x_np = ((x_np - x_np.min()) / (x_np.max() - x_np.min())).clip(0,1)\n        X.append( x_np*255.0 ) # 0 ~ 1 -&gt; 0 ~ 255\n\nX = np.array(X, dtype=np.uint8)\n\n샘플링 과정동안 수집한 이미지를 애니메이션으로 표현합니다.\nplotly로 애니메이션을 표시하는 예제 코드는 이곳 https://plotly.com/python/animations/과 이곳 https://plotly.com/python/imshow/을 참고 하여 작성하였습니다.\n\nfig = go.Figure(\n    data = [ go.Image(z=X[0]) ],\n    layout = go.Layout(\n        # title=\"Generated image\",\n        autosize = False,\n        width = 800, height = 400,\n        margin = dict(l=0, r=0, b=0, t=30),\n        xaxis = {\"title\": f\"Generated Image: x_{T-1}\"},\n        updatemenus = [\n            dict(\n                type=\"buttons\",\n                buttons=[\n                    # play button\n                    dict(\n                        label=\"Play\", method=\"animate\",\n                        args=[\n                            None,\n                            {\n                                \"frame\": {\"duration\": 50, \"redraw\": True},\n                                \"fromcurrent\": True,\n                                \"transition\": {\"duration\": 50, \"easing\": \"quadratic-in-out\"}\n                            }\n                        ]\n                    ),\n                    # pause button\n                    dict(\n                        label=\"Pause\", method=\"animate\",\n                        args=[\n                            [None],\n                            {\n                                \"frame\": {\"duration\": 0, \"redraw\": False},\n                                \"mode\": \"immediate\",\n                                \"transition\": {\"duration\": 0}\n                            }\n                        ]\n                    )\n                ],\n                direction=\"left\", pad={\"r\": 10, \"t\": 87}, showactive=False,\n                x=0.1, xanchor=\"right\", y=0, yanchor=\"top\"\n            )\n        ], # updatemenus = [\n    ), # layout = go.Layout(\n    frames = [\n        {\n            'data':[go.Image(z=X[t])],\n            'name': t,\n            'layout': {\n                'xaxis': {'title': f\"Generated Image: x_{saved_frame[t]-1}\"}\n            }\n        } for t in range(len(X))\n    ]\n)\n\n################################################################################\n# 슬라이더 처리\nsliders_dict = {\n    \"active\": 0, \"yanchor\": \"top\", \"xanchor\": \"left\",\n    \"currentvalue\": {\n        \"font\": {\"size\": 15}, \"prefix\": \"input time:\",\n        \"visible\": True, \"xanchor\": \"right\"\n    },\n    \"transition\": {\"duration\": 100, \"easing\": \"cubic-in-out\"},\n    \"pad\": {\"b\": 10, \"t\": 50},\n    \"len\": 0.9, \"x\": 0.1, \"y\": 0,\n    \"steps\": []\n}\n\nfor t in range(len(X)):\n    slider_step = {\n        \"label\": f\"{saved_frame[t]}\", \"method\": \"animate\",\n        \"args\": [\n            [t], # frame 이름과 일치해야 연결됨\n            {\n                \"frame\": {\"duration\": 100, \"redraw\": True},\n                \"mode\": \"immediate\",\n                \"transition\": {\"duration\": 100}\n            }\n        ],\n    }\n\n    sliders_dict[\"steps\"].append(slider_step)\n\nfig[\"layout\"][\"sliders\"] = [sliders_dict]\n################################################################################\n\nfig.show()\n\n\n                                                \n\n\n위 그림에서 play를 누르거나 슬라이드 바를 이동 시키면 시간 단계에 따라 노이즈가 제거되는 이미지를 확인할 수 있습니다. 완전히 깨끗하고 정교하게 노이즈가 제거되진 않지만 무작위 노이즈로부터 게임 아이템과 캐릭터 처럼 생긴 물체들이 약 200단계에서부터 서서히 생겨나는것을 확인할 수 있습니다!\n간단한 네트워크로 충분히 납득할만한 결과를 생성할 수 있었습니다. 이제 여러분들이 네트워크를 더 정교하게 다듬어 생생한 캐릭터 이미지를 생성하시기 바랍니다.\n\n\n\n디퓨전 모델은 생성되는 이미지가 다양해야하는 문제와 이미지의 품질 문제를 잘 해결하지만 이미지를 생성하는데 시간이 오래 걸린다는 단점이 있습니다. 지난 두 편의 글을 통해 DDPM을 잘 이해 했다면 이제 샘플링 속도를 빠르게 하는 방법에 대해서 알아볼 차례입니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-2.html#데이터-셋-준비",
    "href": "posts/diffusion/ddpm_part2-2.html#데이터-셋-준비",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-2 DDPM Hands-on with Pytorch",
    "section": "",
    "text": "가정 먼저 준비된 데이터 셋 파일을 다운 받습니다.\n\n!gdown 1gADYmo2UXlr24dUUNaqyPF2LZXk1HhrJ\n\n데이터 파일이 넘파이 어레이로 구성되어 있으므로 np.load()로 로딩합니다.\n\nsprites = np.load('sprites_1788_16x16.npy')\n\n데이터 셋은 넘파이 어레이이며 데이터 셋의 모양과 각 샘플의 최솟값, 최댓값을 확인해보면 개별 샘플은 모양이 (16,16,3)이고 0에서 255값을 가지는 이미지 어레이라는 것을 알 수 있습니다.\n\nsprites.shape, sprites.min(), sprites.max()\n\n((89400, 16, 16, 3), 0, 255)\n\n\n이미지의 크기와 채널수를 설정합니다.\n\nH = 16\nW = 16\nC = 3\n\n\n\n이미지를 한장씩 반환하는 데이터셋 클래스 MyDataset을 정의 합니다. 데이터셋 내부에서 이미지에 대한 일련의 transform을 적용하고 나서 노이즈 추가을 진행하기 위해 식(4)에서 사용하는 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)를 클래스 변수로 계산해 둡니다.\n\nclass MyDataset(Dataset):\n    # beta는 DDPM 원문의 설정을 따르고\n    beta_1 = 1e-4\n    beta_T = 0.02\n    # 시간 단계는 deeplearning.ai에서 제공하는 숏코스 How Diffusion Models Work의 설정을 따름\n    T = 500\n\n    # beta는 첨자 1부터 T까지 사용하기 위해 제일 앞에 더미 데이터 tf.constant([0.])를 추가하여 만듬\n    beta = torch.cat([ torch.tensor([0]), torch.linspace(beta_1, beta_T, T)], axis=0)\n    alpha = 1 - beta\n    alpha_bar = torch.exp(torch.cumsum(torch.log(alpha), axis=0))\n\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, i):\n        x_0 = self.data[i]\n\n        # normalize -1~1로 만들기\n        if self.transform:\n            x_0 = self.transform(x_0)\n\n        # noise 추가\n        t = np.random.randint(1, MyDataset.T+1)\n        eps = torch.randn_like(x_0)\n        x_t = torch.sqrt(MyDataset.alpha_bar[t]) * x_0 + torch.sqrt(1 - MyDataset.alpha_bar[t]) * eps\n\n        return x_0, x_t, eps, t\n\n이미지 픽셀값을 -1~1사이로 변환하는 transform을 구성하고 데이터 셋을 생성합니다. 계속해서 생성된 데이터 셋을 이용하는 데이터 로더도 준비합니다. 배치사이즈는 텐서플로 구현과 같게 64로 지정합니다.\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),                # from [0,255] to range [0.0, 1.0]\n    transforms.Normalize((0.5,), (0.5,))  # range [-1,1]\n])\n\ntrain_ds = MyDataset(sprites, transform)\n\nm = 64\n\ntrain_loader = DataLoader(train_ds, batch_size=m, shuffle=True)\ntrain_loader_iter = iter(train_loader)\n\n데이터 로더로 부터 미니배치를 받아와 그림을 그려봅니다.\n\nsamples = next(train_loader_iter)\n\nx_0s = samples[0][:6].numpy()\nx_ts = samples[1][:6].numpy()\nepss = samples[2][:6].numpy()\nts =  samples[3][:6].numpy()\n\n\nfig, axs = plt.subplots(figsize=(10,5), nrows=3, ncols=6)\n\ni = 0\nfor (x_0, x_t, eps, t) in zip(x_0s, x_ts, epss, ts):\n    x_0 = x_0.transpose(1,2,0)\n    x_0 = ((x_0 - x_0.min()) / (x_0.max() - x_0.min())).clip(0,1)\n    axs[0][i].imshow(x_0)\n    axs[0][i].set_title(f\"t={t}\")\n    axs[0][i].set_xticks([])\n    axs[0][i].set_yticks([])\n\n    eps = eps.transpose(1,2,0)\n    eps = ((eps - eps.min()) / (eps.max() - eps.min())).clip(0,1)\n    axs[1][i].imshow(eps)\n    axs[1][i].set_xticks([])\n    axs[1][i].set_yticks([])\n\n    x_t = x_t.transpose(1,2,0)\n    x_t = ((x_t - x_t.min()) / (x_t.max() - x_t.min())).clip(0,1)\n    axs[2][i].imshow(x_t)\n    axs[2][i].set_xticks([])\n    axs[2][i].set_yticks([])\n\n    if i == 0:\n        axs[0][i].set_ylabel('x_0')\n        axs[1][i].set_ylabel('eps')\n        axs[2][i].set_ylabel('x_t')\n\n    i += 1\n\nplt.show()\n\n\n\n\n그림을 확인해보면 샘플 중 앞 여섯개에 임의의 시간 단계에 대한 노이즈가 적용되는 것을 알 수 있습니다. 첫 행은 원본 이미지, 둘째 행은 적용될 노이즈, 셋째 행은 노이즈가 적용된 모습을 나타냅니다. 시간 단계가 작을 수록 원본 이미지가 남아있고 200이 넘어가면 거의 알아볼 수 없게 될 것입니다.\n만들어진 데이터 셋에 셔플과 배치사이즈를 적용합니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-2.html#모델-만들기",
    "href": "posts/diffusion/ddpm_part2-2.html#모델-만들기",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-2 DDPM Hands-on with Pytorch",
    "section": "",
    "text": "이제 모델을 정의 합니다. 가능하다면 (3,16,16) 크기의 이미지를 (768,) 크기의 벡터로 펼친 다음 완전 연결 네트워크로만 구성하고 싶었지만 실험 결과 이런 MLP 구조로는 의미있는 결과를 만들 수 없었습니다. 따라서 conv 레이어를 가지는 간단한 네트워크를 사용합니다. 원문에는 Unet 구조를 사용한다고 되어있지만 Unet도 복잡하기 때문에 Unet 특징이 약간 들어있는 아래 그림과 같은 네트워크를 사용하겠습니다.\n\n네트워크 구조는 매우 간단하여 그림과 아래 코드를 함께 보면 금방 이해가 갈 것입니다. 네트워크의 특징을 다음으로 요약할 수 있습니다.\n\n이미지에 Conv레이어를 몇번 적용하여 만들어진 특징 맵과 시간 단계에 Dense 레이어를 몇번 적용한 임베딩 벡터를 중간쯤에서 더하여 두 입력을 모두 반영하는 특징 맵을 만듭니다.\n특징 맵을 다시 이미지 모양으로 디코딩할 때 인코딩 과정에서 만들어 둔 특징 맵과 연결시키는 스킵 커넥션 구조를 한번 사용합니다.\n\n\nclass DDPM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.emb_1 = torch.nn.Linear(in_features=1, out_features=32)\n        self.emb_2 = torch.nn.Linear(in_features=32, out_features=64)\n\n        self.down_conv1_32 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        self.down_conv2_32 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n        self.down_conv3_64 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.down_conv4_128 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n\n        self.up_conv1_64 = torch.nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1)\n        self.up_conv2_32 = torch.nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1)\n        self.up_conv3_32 = torch.nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1)\n\n        self.relu = torch.nn.ReLU()\n        self.gelu = torch.nn.GELU()\n\n    def forward(self, x, t):\n        # x: (N, C, H, W)\n        # t: (N,1)\n        batch_size = t.shape[0]\n\n        # time embedding\n        t = self.relu( self.emb_1(t) ) # (N, 32)\n        t = self.relu( self.emb_2(t) ) # (N, 64)\n        t = t.reshape(batch_size, -1, 1, 1) # (N, 64, 1, 1)\n\n        # image down conv\n        x = self.gelu( self.down_conv1_32(x) )    # (N, 32, 16, 16)\n        x_32 = self.gelu( self.down_conv2_32(x) ) # (N, 32, 16, 16)\n        size_32 = x_32.shape\n        x = torch.nn.functional.max_pool2d(x_32, (2,2)) # (N, 32, 8, 8)\n        x = self.gelu( self.down_conv3_64(x) ) # (N, 64, 8, 8)\n        size_64 = x.shape\n        x = torch.nn.functional.max_pool2d(x, (2,2)) # (N, 64, 4, 4)\n\n        x = x + t # (N, 64, 4, 4) + (N, 64, 1, 1) = (N, 64, 4, 4)\n        x = self.gelu( self.down_conv4_128(x) ) # (N, 128, 4, 4)\n\n        # image up conv\n        x = self.gelu( self.up_conv1_64(x, output_size=size_64) ) # (N, 64, 8, 8)\n        x = self.gelu( self.up_conv2_32(x, output_size=size_32) ) # (N, 32, 16, 16)\n        x = torch.concat([x, x_32], axis=1) # (N, 64, 16, 16)\n        out = self.up_conv3_32(x) # (N, 3, 16, 16)\n\n        return out\n\n모델을 만들고 사용 다바이스로 모델을 이동시킵니다.\n\nmodel = DDPM()\nmodel.to(device)\n\nDDPM(\n  (emb_1): Linear(in_features=1, out_features=32, bias=True)\n  (emb_2): Linear(in_features=32, out_features=64, bias=True)\n  (down_conv1_32): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (down_conv2_32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (down_conv3_64): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (down_conv4_128): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (up_conv1_64): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (up_conv2_32): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (up_conv3_32): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu): ReLU()\n  (gelu): GELU(approximate='none')\n)\n\n\n모델을 완성했으면 앞서 만들어둔 데이터 셋을 이용해 포워드 테스트를 해봐야 합니다. 배치 사이즈 64이므로 한 배치를 네트워크에 입력하면 출력으로 (64,3,16,16)이 나와야 합니다.\n\noutput = model(samples[0].to(device), samples[3].reshape(-1,1).float().to(device))\n\nprint(output.shape)\n\ntorch.Size([64, 3, 16, 16])\n\n\n포워드 테스트가 성공했으므로 이제 학습 시키는 일만 남았습니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-2.html#학습",
    "href": "posts/diffusion/ddpm_part2-2.html#학습",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-2 DDPM Hands-on with Pytorch",
    "section": "",
    "text": "\\[\nL_{\\text{simple}}(\\theta) := \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\left\\lVert \\boldsymbol{\\epsilon}- \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\boldsymbol{\\epsilon}, t) \\right\\rVert^2_2 \\right] \\tag{14}\n\\]\n위 손실 함수는 모든 샘플과 시간 단계에 대해서 MSE 손실 함수를 사용한다는 것을 의미합니다. 손실 함수를 식(14)처럼 MSE로 설정하고 적당한 옵티마이저를 생성합니다.\n\nloss_func = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n다음은 학습 루프를 구현할 차례입니다.\n\n위 그림에 나타난 학습 알고리즘을 직접 구현합니다. 학습 루프에서 mae 매트릭도 함께 계산해서 화면에 출력해 줍니다.\n\nepochs = 30\nlosses = []\n\nfor e in range(epochs):\n    epoch_loss = 0.0\n    epoch_mae = 0.0\n\n    for i, data in enumerate(tqdm(train_loader)):\n        x_0, x_t, eps, t = data\n        x_t = x_t.to(device)\n        eps = eps.to(device)\n        t = t.to(device)\n\n        optimizer.zero_grad()\n        eps_theta = model(x_t, t.reshape(-1,1).float())\n        loss = loss_func(eps_theta, eps)\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            epoch_loss += loss.item()\n            epoch_mae += torch.nn.functional.l1_loss(eps_theta, eps)\n\n    epoch_loss /= len(train_loader)\n    epoch_mae /= len(train_loader)\n\n    print(f\"Epoch: {e+1:2d}: loss:{epoch_loss:.4f}, mae:{epoch_mae:.4f}\")\n    losses.append(epoch_loss)\n\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 193.73it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.67it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 199.00it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 204.76it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.65it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.85it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.54it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 207.12it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.49it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.28it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.37it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 206.47it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 204.66it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.35it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.87it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 201.97it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.26it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 199.27it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 206.42it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 199.97it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.68it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:07&lt;00:00, 198.86it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 199.91it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 203.76it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 208.81it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 207.95it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 205.44it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 207.08it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 204.23it/s]\n100%|██████████████████████████████████████████████████████████████████████████| 1397/1397 [00:06&lt;00:00, 202.92it/s]\n\n\nEpoch:  1: loss:0.3821, mae:0.4512\nEpoch:  2: loss:0.2485, mae:0.3536\nEpoch:  3: loss:0.2037, mae:0.3103\nEpoch:  4: loss:0.1852, mae:0.2918\nEpoch:  5: loss:0.1719, mae:0.2790\nEpoch:  6: loss:0.1661, mae:0.2724\nEpoch:  7: loss:0.1602, mae:0.2661\nEpoch:  8: loss:0.1550, mae:0.2602\nEpoch:  9: loss:0.1530, mae:0.2575\nEpoch: 10: loss:0.1504, mae:0.2545\nEpoch: 11: loss:0.1468, mae:0.2506\nEpoch: 12: loss:0.1451, mae:0.2487\nEpoch: 13: loss:0.1425, mae:0.2459\nEpoch: 14: loss:0.1414, mae:0.2445\nEpoch: 15: loss:0.1385, mae:0.2418\nEpoch: 16: loss:0.1372, mae:0.2402\nEpoch: 17: loss:0.1361, mae:0.2391\nEpoch: 18: loss:0.1359, mae:0.2385\nEpoch: 19: loss:0.1341, mae:0.2365\nEpoch: 20: loss:0.1331, mae:0.2355\nEpoch: 21: loss:0.1327, mae:0.2348\nEpoch: 22: loss:0.1312, mae:0.2336\nEpoch: 23: loss:0.1290, mae:0.2316\nEpoch: 24: loss:0.1299, mae:0.2319\nEpoch: 25: loss:0.1289, mae:0.2310\nEpoch: 26: loss:0.1277, mae:0.2299\nEpoch: 27: loss:0.1275, mae:0.2298\nEpoch: 28: loss:0.1266, mae:0.2286\nEpoch: 29: loss:0.1272, mae:0.2290\nEpoch: 30: loss:0.1250, mae:0.2271\n\n\n\nplt.plot(losses)\nplt.show()"
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-2.html#샘플링",
    "href": "posts/diffusion/ddpm_part2-2.html#샘플링",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-2 DDPM Hands-on with Pytorch",
    "section": "",
    "text": "학습된 모델을 이용해서 샘플링 알고리즘을 따라 모든 시간 단계에 대한 샘플을 샘플링합니다.\n샘플링 과정에서 사용할 \\(\\beta_t\\), \\(\\alpha_t\\), \\(\\bar\\alpha_t\\)를 클래스 외부 변수로 빼둡니다.\n\nalpha = MyDataset.alpha.to(device)\nalpha_bar = MyDataset.alpha_bar.to(device)\nbeta = MyDataset.beta.to(device)\nT = MyDataset.T\n\n\n# 샘플링 단계동안 생성된 이미지를 일정 간격마다 저장할 리스트를 준비\ninterval = 20 # 20 시간 단계마다 한장씩 생성 결과 기록\nX = [] # 생성 이미지 저장\nsaved_frame = [] # 이미지를 저장한 시간 단계를 저장\nN = 5 # 모델에 입력할 샘플 개수\n\n\n# 최초 노이즈 샘플링\nx = torch.randn(size=(N, C, H, W)).to(device)\n\nfor t in range(T, 0, -1):\n    if t &gt; 1:\n        z = torch.randn(size=(N,C,H,W)).to(device)\n    else:\n        z = torch.zeros((N,C,H,W)).to(device)\n\n    t_torch = torch.tensor([[t]]*N, dtype=torch.float32).to(device)\n    eps_theta = model(x, t_torch)\n    x = (1 / torch.sqrt(alpha[t])) * \\\n        (x - ((1-alpha[t])/torch.sqrt(1-alpha_bar[t]))*eps_theta) + torch.sqrt(beta[t])*z\n\n    if (T - t) % interval == 0  or t == 1:\n        # 현재 시간 단계로 부터 생성되는 t-1번째 이미지를 저장\n        saved_frame.append(t)\n        x_np = x.detach().cpu().numpy()\n\n        # (N,C,H,W)-&gt;(H,N,W,C)\n        x_np = x_np.transpose(2,0,3,1).reshape(H,-1,C)\n        x_np = ((x_np - x_np.min()) / (x_np.max() - x_np.min())).clip(0,1)\n        X.append( x_np*255.0 ) # 0 ~ 1 -&gt; 0 ~ 255\n\nX = np.array(X, dtype=np.uint8)\n\n샘플링 과정동안 수집한 이미지를 애니메이션으로 표현합니다.\nplotly로 애니메이션을 표시하는 예제 코드는 이곳 https://plotly.com/python/animations/과 이곳 https://plotly.com/python/imshow/을 참고 하여 작성하였습니다.\n\nfig = go.Figure(\n    data = [ go.Image(z=X[0]) ],\n    layout = go.Layout(\n        # title=\"Generated image\",\n        autosize = False,\n        width = 800, height = 400,\n        margin = dict(l=0, r=0, b=0, t=30),\n        xaxis = {\"title\": f\"Generated Image: x_{T-1}\"},\n        updatemenus = [\n            dict(\n                type=\"buttons\",\n                buttons=[\n                    # play button\n                    dict(\n                        label=\"Play\", method=\"animate\",\n                        args=[\n                            None,\n                            {\n                                \"frame\": {\"duration\": 50, \"redraw\": True},\n                                \"fromcurrent\": True,\n                                \"transition\": {\"duration\": 50, \"easing\": \"quadratic-in-out\"}\n                            }\n                        ]\n                    ),\n                    # pause button\n                    dict(\n                        label=\"Pause\", method=\"animate\",\n                        args=[\n                            [None],\n                            {\n                                \"frame\": {\"duration\": 0, \"redraw\": False},\n                                \"mode\": \"immediate\",\n                                \"transition\": {\"duration\": 0}\n                            }\n                        ]\n                    )\n                ],\n                direction=\"left\", pad={\"r\": 10, \"t\": 87}, showactive=False,\n                x=0.1, xanchor=\"right\", y=0, yanchor=\"top\"\n            )\n        ], # updatemenus = [\n    ), # layout = go.Layout(\n    frames = [\n        {\n            'data':[go.Image(z=X[t])],\n            'name': t,\n            'layout': {\n                'xaxis': {'title': f\"Generated Image: x_{saved_frame[t]-1}\"}\n            }\n        } for t in range(len(X))\n    ]\n)\n\n################################################################################\n# 슬라이더 처리\nsliders_dict = {\n    \"active\": 0, \"yanchor\": \"top\", \"xanchor\": \"left\",\n    \"currentvalue\": {\n        \"font\": {\"size\": 15}, \"prefix\": \"input time:\",\n        \"visible\": True, \"xanchor\": \"right\"\n    },\n    \"transition\": {\"duration\": 100, \"easing\": \"cubic-in-out\"},\n    \"pad\": {\"b\": 10, \"t\": 50},\n    \"len\": 0.9, \"x\": 0.1, \"y\": 0,\n    \"steps\": []\n}\n\nfor t in range(len(X)):\n    slider_step = {\n        \"label\": f\"{saved_frame[t]}\", \"method\": \"animate\",\n        \"args\": [\n            [t], # frame 이름과 일치해야 연결됨\n            {\n                \"frame\": {\"duration\": 100, \"redraw\": True},\n                \"mode\": \"immediate\",\n                \"transition\": {\"duration\": 100}\n            }\n        ],\n    }\n\n    sliders_dict[\"steps\"].append(slider_step)\n\nfig[\"layout\"][\"sliders\"] = [sliders_dict]\n################################################################################\n\nfig.show()\n\n\n                                                \n\n\n위 그림에서 play를 누르거나 슬라이드 바를 이동 시키면 시간 단계에 따라 노이즈가 제거되는 이미지를 확인할 수 있습니다. 완전히 깨끗하고 정교하게 노이즈가 제거되진 않지만 무작위 노이즈로부터 게임 아이템과 캐릭터 처럼 생긴 물체들이 약 200단계에서부터 서서히 생겨나는것을 확인할 수 있습니다!\n간단한 네트워크로 충분히 납득할만한 결과를 생성할 수 있었습니다. 이제 여러분들이 네트워크를 더 정교하게 다듬어 생생한 캐릭터 이미지를 생성하시기 바랍니다."
  },
  {
    "objectID": "posts/diffusion/ddpm_part2-2.html#다음편-예고",
    "href": "posts/diffusion/ddpm_part2-2.html#다음편-예고",
    "title": "A Gentle Introduction to Diffusion Model: Part 2-2 DDPM Hands-on with Pytorch",
    "section": "",
    "text": "디퓨전 모델은 생성되는 이미지가 다양해야하는 문제와 이미지의 품질 문제를 잘 해결하지만 이미지를 생성하는데 시간이 오래 걸린다는 단점이 있습니다. 지난 두 편의 글을 통해 DDPM을 잘 이해 했다면 이제 샘플링 속도를 빠르게 하는 방법에 대해서 알아볼 차례입니다."
  }
]